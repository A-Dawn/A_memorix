#!/usr/bin/env python3
"""
çŸ¥è¯†åº“è‡ªåŠ¨å¯¼å…¥è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«æ plugins/A_memorix/data/raw ä¸‹çš„ .txt æ–‡ä»¶
2. æ£€æŸ¥ data/import_manifest.json ç¡®è®¤æ˜¯å¦å·²å¯¼å…¥
3. è°ƒç”¨ LLM å¤„ç†æœªå¯¼å…¥æ–‡ä»¶ç”Ÿæˆ JSON
4. å°†ç”Ÿæˆçš„æ•°æ®ç›´æ¥å­˜å…¥ VectorStore/GraphStore/MetadataStore
5. æ›´æ–° manifest

ç”¨æ³•ï¼šæ— éœ€å‚æ•°ï¼Œç›´æ¥è¿è¡Œ
"""

import sys
import os
import json
import asyncio
import time
import random
import hashlib
import tomlkit
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# è·¯å¾„è®¾ç½®
current_dir = Path(__file__).resolve().parent
# å‡è®¾è„šæœ¬åœ¨ plugins/A_memorix/scripts
plugin_root = current_dir.parent
project_root = plugin_root.parent.parent
sys.path.insert(0, str(project_root))

# æ•°æ®ç›®å½•
DATA_DIR = plugin_root / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
MANIFEST_PATH = DATA_DIR / "import_manifest.json"

try:
    print(f"Project root: {project_root}")
    print(f"Sys path: {sys.path[:3]}...")
    
    import src
    print("âœ… src imported")
    
    import plugins
    print("âœ… plugins imported")
    
    import plugins.A_memorix
    print("âœ… plugins.A_memorix imported")
    
    from src.common.logger import get_logger
    from src.plugin_system.apis import llm_api
    from src.config.config import global_config, model_config
    
    # å¯¼å…¥æ ¸å¿ƒç»„ä»¶
    from plugins.A_memorix.core import (
        VectorStore,
        GraphStore,
        MetadataStore,
        create_embedding_api_adapter,
        PersonalizedPageRank,
        KnowledgeType,
    )
    from plugins.A_memorix.core.storage import (
        QuantizationType, 
        SparseMatrixFormat,
        detect_knowledge_type
    )
    
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ¨¡å—: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

logger = get_logger("A_Memorix.AutoImport")

class AutoImporter:
    def __init__(self, force: bool = False, clear_manifest: bool = False, target_type: str = "auto"):
        self.vector_store: Optional[VectorStore] = None
        self.graph_store: Optional[GraphStore] = None
        self.metadata_store: Optional[MetadataStore] = None
        self.embedding_manager = None
        self.plugin_config = {}
        self.manifest = {}
        self.force = force
        self.clear_manifest = clear_manifest
        self.target_type = target_type

    async def initialize(self):
        """åˆå§‹åŒ–é…ç½®å’Œå­˜å‚¨"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–...")
        
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
        RAW_DIR.mkdir(parents=True, exist_ok=True)
        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
        
        # 2. åŠ è½½ Manifest
        if self.clear_manifest:
            logger.info("ğŸ§¹ æ¸…ç† Manifest (--clear-manifest activated)")
            self.manifest = {}
            self._save_manifest()
        elif MANIFEST_PATH.exists():
            try:
                with open(MANIFEST_PATH, "r", encoding="utf-8") as f:
                    self.manifest = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½ Mainfest å¤±è´¥: {e}")
                self.manifest = {}
        
        # 3. åŠ è½½æ’ä»¶é…ç½®
        config_path = plugin_root / "config.toml"
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.plugin_config = tomlkit.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ’ä»¶é…ç½®å¤±è´¥: {e}")
            return False

        # 4. åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
        try:
            await self._init_stores()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
            
        return True

    async def _init_stores(self):
        """åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶ (å‚è€ƒ A_MemorixPlugin)"""
        # åµŒå…¥API
        self.embedding_manager = create_embedding_api_adapter(
            batch_size=self.plugin_config.get("embedding", {}).get("batch_size", 32),
            default_dimension=self.plugin_config.get("embedding", {}).get("dimension", 384),
            model_name=self.plugin_config.get("embedding", {}).get("model_name", "auto"),
        )
        
        # æ£€æµ‹ç»´åº¦
        try:
            dim = await self.embedding_manager._detect_dimension()
        except:
            dim = self.embedding_manager.default_dimension
            
        # å‘é‡å­˜å‚¨
        q_type_str = self.plugin_config.get("embedding", {}).get("quantization_type", "int8")
        q_map = {"float32": QuantizationType.FLOAT32, "int8": QuantizationType.INT8, "pq": QuantizationType.PQ}
        
        self.vector_store = VectorStore(
            dimension=dim,
            quantization_type=q_map.get(q_type_str, QuantizationType.INT8),
            data_dir=DATA_DIR / "vectors"
        )
        
        # å›¾å­˜å‚¨
        m_fmt_str = self.plugin_config.get("graph", {}).get("sparse_matrix_format", "csr")
        m_map = {"csr": SparseMatrixFormat.CSR, "csc": SparseMatrixFormat.CSC}
        
        self.graph_store = GraphStore(
            matrix_format=m_map.get(m_fmt_str, SparseMatrixFormat.CSR),
            data_dir=DATA_DIR / "graph"
        )
        
        # å…ƒæ•°æ®å­˜å‚¨
        self.metadata_store = MetadataStore(data_dir=DATA_DIR / "metadata")
        self.metadata_store.connect()
        
        # åŠ è½½æ•°æ®
        if self.vector_store.has_data():
            self.vector_store.load()
        if self.graph_store.has_data():
            self.graph_store.load()

    def load_file(self, file_path: Path) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    def get_file_hash(self, content: str) -> str:
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    async def process_and_import(self):
        """ä¸»å¤„ç†å¾ªç¯"""
        if not await self.initialize():
            return

        files = list(RAW_DIR.glob("*.txt"))
        logger.info(f"æ‰«æåˆ° {len(files)} ä¸ªæ–‡ä»¶ in {RAW_DIR}")

        processed_count = 0
        
        for file_path in files:
            filename = file_path.name
            content = self.load_file(file_path)
            file_hash = self.get_file_hash(content)
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not self.force and filename in self.manifest:
                record = self.manifest[filename]
                if record.get("hash") == file_hash and record.get("imported"):
                    logger.info(f"è·³è¿‡å·²å¯¼å…¥æ–‡ä»¶: {filename}")
                    continue
            
            if self.force:
                logger.info(f"å¼ºåˆ¶é‡æ–°å¯¼å…¥: {filename}")
            
            logger.info(f"=== å¼€å§‹å¤„ç†: {filename} ===")
            
            # 1. LLM å¤„ç†ç”Ÿæˆ JSON
            json_data = await self._process_text_to_json(content, filename)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            json_path = PROCESSED_DIR / f"{file_path.stem}.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
                
            # 2. å¯¼å…¥åˆ°æ•°æ®åº“
            await self._import_to_db(json_data)
            
            # 3. æ›´æ–° Manifest
            self.manifest[filename] = {
                "hash": file_hash,
                "timestamp": time.time(),
                "imported": True
            }
            self._save_manifest()
            
            logger.info(f"âœ… æ–‡ä»¶ {filename} å¤„ç†å¹¶å¯¼å…¥å®Œæˆ")
            processed_count += 1
            
            # ä¿å­˜æ•°æ®åº“çŠ¶æ€
            self.vector_store.save()
            self.graph_store.save()

        if processed_count == 0:
            logger.info("æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†")
        else:
            logger.info(f"æœ¬æ¬¡å…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

    async def _select_model(self) -> Any:
        """ç²¾ç¡®é€‰æ‹©æœ€é€‚åˆçŸ¥è¯†æŠ½å–çš„æ¨¡å‹ (ä»…é™æ˜ç¡®é…ç½®å’Œä»»åŠ¡åŒ¹é…)"""
        models = llm_api.get_available_models()
        if not models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹é…ç½®")

        # 1. ä¼˜å…ˆçº§æœ€é«˜ï¼šæ’ä»¶é…ç½®å¼ºåˆ¶æŒ‡å®š
        config_model = self.plugin_config.get("advanced", {}).get("extraction_model", "auto")
        if config_model != "auto" and config_model in models:
            logger.info(f"  ä½¿ç”¨æ’ä»¶é…ç½®æŒ‡å®šçš„æ¨¡å‹: {config_model}")
            return models[config_model]

        # 2. ä¼˜å…ˆçº§ç¬¬äºŒï¼šä¸»ç¨‹åºä»»åŠ¡é…ç½®åŒ¹é… (lpmm_entity_extract)
        try:
            from src.config.config import model_config as host_model_config
            task_configs = getattr(host_model_config, "model_task_config", {})
            
            # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸¤ç§ç›¸å…³çš„ä»»åŠ¡é…ç½®
            for task_key in ["lpmm_entity_extract", "lpmm_rdf_build"]:
                if task_key in task_configs:
                    task_models = task_configs[task_key].get("model_list", [])
                    for m in task_models:
                        if m in models:
                            logger.info(f"  é€šè¿‡ä¸»ç¨‹åºä»»åŠ¡é…ç½® [{task_key}] åŒ¹é…åˆ°æ¨¡å‹: {m}")
                            return models[m]
        except Exception as e:
            logger.debug(f"è¯»å–ä¸»ç¨‹åºä»»åŠ¡é…ç½®å¤±è´¥: {e}")

        # 3. å…œåº•ç­–ç•¥ï¼šå¦‚æœä»¥ä¸Šå‡æœªåŒ¹é…ï¼ŒæŠ›å‡ºé”™è¯¯å¼•å¯¼ç”¨æˆ·é…ç½®
        logger.error("âŒ æœªèƒ½åœ¨ä¸»ç¨‹åºé…ç½®ä¸­æ‰¾åˆ°åˆé€‚çš„ [lpmm_entity_extract] ä»»åŠ¡æ¨¡å‹")
        logger.warning("è¯·åœ¨ model_config.toml çš„ [model_task_config.lpmm_entity_extract] ä¸­æŒ‡å®šæ¨¡å‹ï¼Œ")
        logger.warning("æˆ–è€…åœ¨æ’ä»¶ config.toml çš„ [advanced] ä¸­è®¾ç½® extraction_model")
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å›é¦–ä¸ªå¯ç”¨æ¨¡å‹ä½†ç»™å‡ºå¼ºçƒˆè­¦å‘Š
        first_model = list(models.keys())[0]
        logger.warning(f"ç”±äºæœªåŒ¹é…åˆ°ä¸“ç”¨æ¨¡å‹ï¼Œè¢«è¿«ä½¿ç”¨é¦–ä¸ªå¯ç”¨æ¨¡å‹: {first_model}")
        return models[first_model]

    async def _process_text_to_json(self, text: str, filename: str) -> Dict:
        """è°ƒç”¨ LLM å¤„ç†æ–‡æœ¬"""
        chunks = self._split_text(text)
        logger.info(f"  åˆ†å—æ•°é‡: {len(chunks)}")
        
        all_data = {"paragraphs": [], "entities": [], "relations": []}
        
        # æ™ºèƒ½é€‰æ‹©æ¨¡å‹é…ç½®
        model_config = await self._select_model()
        
        for i, chunk in enumerate(chunks):
            # æ·»åŠ æ®µè½
            all_data["paragraphs"].append({"content": chunk, "source": filename})
            
            # æå–ä¿¡æ¯
            result = await self._extract_info(chunk, model_config)
            
            if result.get("entities"):
                all_data["entities"].extend(result["entities"])
            if result.get("relations"):
                all_data["relations"].extend(result["relations"])
                
            logger.info(f"  å·²å¤„ç†å— {i+1}/{len(chunks)}")
            await asyncio.sleep(0.5)
            
        # å»é‡
        all_data["entities"] = list(set(all_data["entities"]))
        return all_data

    async def _extract_info(self, chunk: str, model_config: Any) -> Dict:
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„å®ä½“ï¼ˆEntitiesï¼‰å’Œå…³ç³»ï¼ˆRelationsï¼‰ã€‚
ä»…æå–å…³é”®ä¿¡æ¯ã€‚
JSONæ ¼å¼: {{ "entities": ["e1"], "relations": [{{"subject": "s", "predicate": "p", "object": "o"}}] }}
æ–‡æœ¬:
{chunk[:2000]}
"""
        success, response, _, _ = await llm_api.generate_with_model(
            prompt=prompt,
            model_config=model_config,
            request_type="Script.ProcessKnowledge"
        )
        if success:
            try:
                # ç®€å•æ¸…ç†
                txt = response.strip()
                if "```" in txt:
                    txt = txt.split("```json")[-1].split("```")[0].strip()
                    if txt.startswith("json"): txt = txt[4:].strip()
                return json.loads(txt)
            except:
                pass
        return {}

    def _split_text(self, text: str, size=800) -> List[str]:
        # ç®€å•æŒ‰è¡Œåˆ†å—
        lines = text.split("\n")
        chunks = []
        cur = ""
        for line in lines:
            if len(cur) + len(line) > size:
                chunks.append(cur)
                cur = line + "\n"
            else:
                cur += line + "\n"
        if cur: chunks.append(cur)
        return chunks

    async def _import_to_db(self, data: Dict):
        """å°†JSONæ•°æ®å¯¼å…¥å­˜å‚¨"""
        # 1. å¯¼å…¥æ®µè½
        for item in data.get("paragraphs", []):
            content = item["content"] if isinstance(item, dict) else item
            
            # å…ƒæ•°æ®åˆ¤å®š
            if self.target_type and self.target_type != "auto":
                from plugins.A_memorix.core.storage import get_knowledge_type_from_string
                k_type = get_knowledge_type_from_string(self.target_type) or detect_knowledge_type(content)
            else:
                k_type = detect_knowledge_type(content)
                
            h_val = self.metadata_store.add_paragraph(content, "auto_import", k_type.value)
            
            # å‘é‡
            emb = await self.embedding_manager.encode(content)
            self.vector_store.add(emb.reshape(1, -1), [h_val])
            
        # 2. å¯¼å…¥å®ä½“
        entities = data.get("entities", [])
        if entities:
            self.graph_store.add_nodes(entities)
            
        # 3. å¯¼å…¥å…³ç³»
        for rel in data.get("relations", []):
            s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
            if s and p and o:
                # è¿™é‡Œçš„add_edgesä¼šè‡ªåŠ¨add_nodesï¼Œä½†ä¸ºäº†å®‰å…¨å…ˆä¿è¯nodeså­˜åœ¨
                self.graph_store.add_nodes([s, o])
                
                # æ·»åŠ åˆ°å›¾
                self.graph_store.add_edges([(s, o)])
                
                # æ·»åŠ åˆ°å…ƒæ•°æ®ï¼ˆå¦‚æœéœ€è¦å…³ç³»å…ƒæ•°æ®æ”¯æŒï¼‰
                self.metadata_store.add_relation(s, p, o)

    def _save_manifest(self):
        with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
            json.dump(self.manifest, f, ensure_ascii=False, indent=2)

async def main():
    quotes = [
        "è¨˜æ†¶ã®ä¸­ã«å±…ãŸ,æ¸©ã‚‚ã‚ŠãŒå´ã«ã„ã¦",
        "å¹¸ç¦ã®åˆ‡ã‚Œç«¯ã‚’,ç¹‹ã„ã§ã„ãŸã€‚é¡˜ã£ã¦ã„ãŸ"
    ]
    logger.info(random.choice(quotes))  # Runtime Easter Egg
    
    parser = argparse.ArgumentParser(description="A_Memorix çŸ¥è¯†åº“è‡ªåŠ¨å¯¼å…¥å·¥å…·")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°å¯¼å…¥æ‰€æœ‰æ–‡ä»¶ï¼Œå¿½ç•¥å·²å¯¼å…¥è®°å½•")
    parser.add_argument("--clear-manifest", action="store_true", help="å¤„ç†å‰æ¸…ç©ºå¯¼å…¥å†å²è®°å½•")
    parser.add_argument("--type", "-t", choices=["structured", "narrative", "factual", "mixed", "auto"], default="auto", help="å¼ºåˆ¶æŒ‡å®šæ‰€æœ‰å¯¼å…¥æ–‡ä»¶çš„çŸ¥è¯†ç±»å‹")
    args = parser.parse_args()

    if not global_config or not model_config:
        logger.error("å…¨å±€é…ç½®æœªåŠ è½½")
        return
        
    importer = AutoImporter(force=args.force, clear_manifest=args.clear_manifest, target_type=args.type)
    await importer.process_and_import()

if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
