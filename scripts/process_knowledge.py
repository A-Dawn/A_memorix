#!/usr/bin/env python3
"""
çŸ¥è¯†åº“è‡ªåŠ¨å¯¼å…¥è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«æ plugins/A_memorix/data/raw ä¸‹çš„ .txt æ–‡ä»¶
2. æ£€æŸ¥ data/import_manifest.json ç¡®è®¤æ˜¯å¦å·²å¯¼å…¥
3. è°ƒç”¨ LLM å¤„ç†æœªå¯¼å…¥æ–‡ä»¶ç”Ÿæˆ JSON
4. å°†ç”Ÿæˆçš„æ•°æ®ç›´æ¥å­˜å…¥ VectorStore/GraphStore/MetadataStore
5. æ›´æ–° manifest

ç”¨æ³•ï¼šæ— éœ€å‚æ•°ï¼Œç›´æ¥è¿è¡Œ
"""

import sys
import os
import json
import asyncio
import time
import random
import hashlib
import tomlkit
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# è·¯å¾„è®¾ç½®
current_dir = Path(__file__).resolve().parent
# å‡è®¾è„šæœ¬åœ¨ plugins/A_memorix/scripts
plugin_root = current_dir.parent
project_root = plugin_root.parent.parent
sys.path.insert(0, str(project_root))

# æ•°æ®ç›®å½•
DATA_DIR = plugin_root / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
MANIFEST_PATH = DATA_DIR / "import_manifest.json"

try:
    print(f"Project root: {project_root}")
    print(f"Sys path: {sys.path[:3]}...")
    
    import src
    print("âœ… src imported")
    
    import plugins
    print("âœ… plugins imported")
    
    # åŠ¨æ€è®¡ç®—æ’ä»¶åç§° (å‡è®¾è„šæœ¬ä½äº plugins/<plugin_name>/scripts/)
    script_path = Path(__file__).resolve()
    plugin_dir = script_path.parent.parent
    plugin_name = plugin_dir.name
    
    import importlib
    
    # ç¡®ä¿ plugins åŒ…å·²åŠ è½½
    try:
        if f"plugins.{plugin_name}" not in sys.modules:
            importlib.import_module(f"plugins.{plugin_name}")
        print(f"âœ… plugins.{plugin_name} imported")
    except ImportError as e:
        print(f"âš ï¸ Could not import plugins.{plugin_name}: {e}")

    from src.common.logger import get_logger
    from src.plugin_system.apis import llm_api
    from src.config.config import global_config, model_config
    
    # åŠ¨æ€å¯¼å…¥æ ¸å¿ƒç»„ä»¶
    core_module = importlib.import_module(f"plugins.{plugin_name}.core")
    VectorStore = core_module.VectorStore
    GraphStore = core_module.GraphStore
    MetadataStore = core_module.MetadataStore
    create_embedding_api_adapter = core_module.create_embedding_api_adapter
    PersonalizedPageRank = core_module.PersonalizedPageRank
    KnowledgeType = core_module.KnowledgeType

    storage_module = importlib.import_module(f"plugins.{plugin_name}.core.storage")
    QuantizationType = storage_module.QuantizationType
    SparseMatrixFormat = storage_module.SparseMatrixFormat
    detect_knowledge_type = storage_module.detect_knowledge_type
    
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ¨¡å—: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

logger = get_logger("A_Memorix.AutoImport")

class AutoImporter:
    def __init__(self, force: bool = False, clear_manifest: bool = False, target_type: str = "auto", concurrency: int = 5):
        self.vector_store: Optional[VectorStore] = None
        self.graph_store: Optional[GraphStore] = None
        self.metadata_store: Optional[MetadataStore] = None
        self.embedding_manager = None
        self.plugin_config = {}
        self.manifest = {}
        self.force = force
        self.clear_manifest = clear_manifest
        self.target_type = target_type
        
        # å¹¶å‘æ§åˆ¶
        self.concurrency_limit = concurrency
        self.semaphore = None
        self.storage_lock = None

    async def initialize(self):
        """åˆå§‹åŒ–é…ç½®å’Œå­˜å‚¨"""
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–... (å¹¶å‘æ•°: {self.concurrency_limit})")
        
        # åˆå§‹åŒ–å¹¶å‘åŸè¯­
        self.semaphore = asyncio.Semaphore(self.concurrency_limit)
        self.storage_lock = asyncio.Lock()
        
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
        RAW_DIR.mkdir(parents=True, exist_ok=True)
        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
        
        # 2. åŠ è½½ Manifest
        if self.clear_manifest:
            logger.info("ğŸ§¹ æ¸…ç† Manifest (--clear-manifest activated)")
            self.manifest = {}
            self._save_manifest()
        elif MANIFEST_PATH.exists():
            try:
                with open(MANIFEST_PATH, "r", encoding="utf-8") as f:
                    self.manifest = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½ Mainfest å¤±è´¥: {e}")
                self.manifest = {}
        
        # 3. åŠ è½½æ’ä»¶é…ç½®
        config_path = plugin_root / "config.toml"
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.plugin_config = tomlkit.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ’ä»¶é…ç½®å¤±è´¥: {e}")
            return False

        # 4. åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
        try:
            await self._init_stores()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å­˜å‚¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
            
        return True

    async def _init_stores(self):
        """åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶ (å‚è€ƒ A_MemorixPlugin)"""
        # åµŒå…¥API
        self.embedding_manager = create_embedding_api_adapter(
            batch_size=self.plugin_config.get("embedding", {}).get("batch_size", 32),
            default_dimension=self.plugin_config.get("embedding", {}).get("dimension", 384),
            model_name=self.plugin_config.get("embedding", {}).get("model_name", "auto"),
        )
        
        # æ£€æµ‹ç»´åº¦
        try:
            dim = await self.embedding_manager._detect_dimension()
        except:
            dim = self.embedding_manager.default_dimension
            
        # å‘é‡å­˜å‚¨
        q_type_str = self.plugin_config.get("embedding", {}).get("quantization_type", "int8")
        q_map = {"float32": QuantizationType.FLOAT32, "int8": QuantizationType.INT8, "pq": QuantizationType.PQ}
        
        self.vector_store = VectorStore(
            dimension=dim,
            quantization_type=q_map.get(q_type_str, QuantizationType.INT8),
            data_dir=DATA_DIR / "vectors"
        )
        
        # å›¾å­˜å‚¨
        m_fmt_str = self.plugin_config.get("graph", {}).get("sparse_matrix_format", "csr")
        m_map = {"csr": SparseMatrixFormat.CSR, "csc": SparseMatrixFormat.CSC}
        
        self.graph_store = GraphStore(
            matrix_format=m_map.get(m_fmt_str, SparseMatrixFormat.CSR),
            data_dir=DATA_DIR / "graph"
        )
        
        # å…ƒæ•°æ®å­˜å‚¨
        self.metadata_store = MetadataStore(data_dir=DATA_DIR / "metadata")
        self.metadata_store.connect()
        
        # åŠ è½½æ•°æ®
        if self.vector_store.has_data():
            self.vector_store.load()
        if self.graph_store.has_data():
            self.graph_store.load()

    def load_file(self, file_path: Path) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    def get_file_hash(self, content: str) -> str:
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    # ... (store initialization remains same) ...

    async def process_and_import(self):
        """ä¸»å¤„ç†å¾ªç¯ (å¹¶è¡Œç‰ˆ)"""
        if not await self.initialize():
            return

        files = list(RAW_DIR.glob("*.txt"))
        logger.info(f"æ‰«æåˆ° {len(files)} ä¸ªæ–‡ä»¶ in {RAW_DIR}")

        if not files:
            logger.info("æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†")
            return

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for file_path in files:
            task = asyncio.create_task(self._process_single_file(file_path))
            tasks.append(task)
            
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = 0
        for res in results:
            if res is True:
                success_count += 1
            elif isinstance(res, Exception):
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {res}")
        
        logger.info(f"æœ¬æ¬¡ä¸»å¤„ç†å®Œæˆï¼Œå…±æˆåŠŸå¤„ç† {success_count}/{len(files)} ä¸ªæ–‡ä»¶")
        
        # æœ€åå†ä¸€æ¬¡ä¿å­˜ç¡®ä¿å®‰å…¨
        if self.vector_store: self.vector_store.save()
        if self.graph_store: self.graph_store.save()

    async def _process_single_file(self, file_path: Path) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æµç¨‹ (å—ä¿¡å·é‡æ§åˆ¶)"""
        filename = file_path.name
        
        # 1. è·å–ä¿¡å·é‡ (é™åˆ¶å¹¶å‘ LLM è°ƒç”¨)
        async with self.semaphore:
            try:
                content = self.load_file(file_path)
                file_hash = self.get_file_hash(content)
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç† (å¿«é€Ÿæ£€æŸ¥ï¼Œæ— éœ€é”)
                if not self.force and filename in self.manifest:
                    record = self.manifest[filename]
                    if record.get("hash") == file_hash and record.get("imported"):
                        logger.info(f"è·³è¿‡å·²å¯¼å…¥æ–‡ä»¶: {filename}")
                        return False
                
                if self.force:
                    logger.info(f"å¼ºåˆ¶é‡æ–°å¯¼å…¥: {filename}")
                
                logger.info(f">>> å¼€å§‹å¤„ç†: {filename}")
                
                # 2. LLM å¤„ç†ç”Ÿæˆ JSON (è€—æ—¶æ“ä½œï¼Œå¹¶å‘æ‰§è¡Œ)
                # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½ä¼šæœ‰å¤§é‡çš„ LLM è¯·æ±‚
                json_data = await self._process_text_to_json(content, filename)
                
                # HACK: å°†æ–‡ä»¶å†…å®¹åµŒå…¥åˆ° json_data ä¸­ï¼Œä»¥ä¾¿ _import_to_db ä½¿ç”¨ (å¦‚æœéœ€è¦)
                # å®é™…ä¸Š _import_to_db ä¸»è¦ç”¨ content å­˜æ®µè½ï¼Œjson_data["paragraphs"] é‡Œå·²ç»æœ‰äº†
                
                # ä¿å­˜ä¸­é—´ç»“æœ (IOæ“ä½œï¼Œç›¸å¯¹å¿«ï¼Œæš‚ä¸é”)
                json_path = PROCESSED_DIR / f"{file_path.stem}.json"
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

                # 3. å¯¼å…¥åˆ°æ•°æ®åº“ (å†™æ“ä½œï¼Œå¿…é¡»åŠ é”ä¸²è¡ŒåŒ–)
                async with self.storage_lock:
                    logger.info(f"ğŸ”’ æ­£åœ¨å†™å…¥æ•°æ®åº“: {filename}")
                    try:
                        await self._import_to_db(json_data)

                        # æ›´æ–° Manifest
                        self.manifest[filename] = {
                            "hash": file_hash,
                            "timestamp": time.time(),
                            "imported": True
                        }
                        self._save_manifest()
                        
                        # æ¯æ¬¡æˆåŠŸå¤„ç†åä¿å­˜ä¸€æ¬¡ï¼Œé¿å…å´©æºƒä¸¢å¤±å…¨éƒ¨
                        # è€ƒè™‘åˆ°æ€§èƒ½ï¼Œå¯ä»¥æ”¹ä¸ºæ¯Nä¸ªä¿å­˜ä¸€æ¬¡ï¼Œæˆ–è€…å°±ä¿æŒè¿™æ ·å®‰å…¨æ€§é«˜
                        self.vector_store.save()
                        self.graph_store.save()
                        
                        logger.info(f"âœ… æ–‡ä»¶ {filename} å¤„ç†å¹¶å¯¼å…¥å®Œæˆ")
                        return True

                    except Exception as e:
                        logger.error(f"âŒ å¯¼å…¥æ•°æ®åº“å¤±è´¥ {filename}: {e}")
                        import traceback
                        traceback.print_exc()

                        self.manifest[filename] = {
                            "hash": file_hash,
                            "timestamp": time.time(),
                            "imported": False,
                            "error": str(e)
                        }
                        self._save_manifest()
                        # å³ä½¿å¤±è´¥ä¹Ÿç®—å¤„ç†ç»“æŸ
                        return False

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                return False

    async def _select_model(self) -> Any:
        """ç²¾ç¡®é€‰æ‹©æœ€é€‚åˆçŸ¥è¯†æŠ½å–çš„æ¨¡å‹ (è¿”å› TaskConfig)"""
        models = llm_api.get_available_models()
        if not models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹é…ç½®")

        # 1. ä¼˜å…ˆçº§æœ€é«˜ï¼šæ’ä»¶é…ç½®å¼ºåˆ¶æŒ‡å®šï¼ˆæ”¯æŒä»»åŠ¡åç§°ï¼‰
        config_model = self.plugin_config.get("advanced", {}).get("extraction_model", "auto")

        # å¦‚æœæŒ‡å®šäº†ä»»åŠ¡åç§°ï¼ˆå¦‚ "lpmm_entity_extract"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        if config_model != "auto" and config_model in models:
            logger.info(f"  ä½¿ç”¨æ’ä»¶é…ç½®æŒ‡å®šçš„ä»»åŠ¡: {config_model}")
            return models[config_model]

        # 2. ä¼˜å…ˆçº§ç¬¬äºŒï¼šé»˜è®¤ä½¿ç”¨ lpmm_entity_extract ä»»åŠ¡
        for task_key in ["lpmm_entity_extract", "lpmm_rdf_build", "embedding"]:
            if task_key in models:
                logger.info(f"  ä½¿ç”¨ä¸»ç¨‹åºä»»åŠ¡é…ç½®: {task_key}")
                task_cfg = models[task_key]
                logger.info(f"    æ¨¡å‹åˆ—è¡¨: {task_cfg.model_list}")
                return models[task_key]

        # 3. å…œåº•ç­–ç•¥ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ä»»åŠ¡
        first_task = list(models.keys())[0]
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å®ä½“æŠ½å–ä¸“ç”¨ä»»åŠ¡ï¼Œä½¿ç”¨ä»»åŠ¡: {first_task}")
        return models[first_task]

    async def _process_text_to_json(self, text: str, filename: str) -> Dict:
        """è°ƒç”¨ LLM å¤„ç†æ–‡æœ¬"""
        chunks = self._split_text(text)
        logger.info(f"  åˆ†å—æ•°é‡: {len(chunks)}")
        
        all_data = {"paragraphs": [], "entities": [], "relations": []}
        
        # æ™ºèƒ½é€‰æ‹©æ¨¡å‹é…ç½®
        model_config = await self._select_model()
        
        for i, chunk in enumerate(chunks):
            # æå–ä¿¡æ¯
            result = await self._extract_info(chunk, model_config)
            
            # è®°å½•æ®µè½åŠå…¶å…³è”çš„çŸ¥è¯†
            paragraph_item = {
                "content": chunk,
                "source": filename,
                "entities": result.get("entities", []),
                "relations": result.get("relations", [])
            }
            all_data["paragraphs"].append(paragraph_item)
            
            # åŒæ—¶ä¹Ÿç»´æŠ¤å¹³é“ºçš„å®ä½“åˆ—è¡¨ä»¥ä¾¿å»é‡
            if result.get("entities"):
                all_data["entities"].extend(result["entities"])
                
            logger.info(f"  å·²å¤„ç†å— {i+1}/{len(chunks)}")
            await asyncio.sleep(0.2)

        # å»é‡å®ä½“ï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œå­—å…¸æ ¼å¼ï¼‰
        def dedupe_entities(entities):
            seen = set()
            unique = []
            for e in entities:
                key = e if isinstance(e, str) else json.dumps(e, sort_keys=True, ensure_ascii=False)
                if key not in seen:
                    seen.add(key)
                    unique.append(e)
            return unique

        all_data["entities"] = dedupe_entities(all_data["entities"])
        return all_data

    async def _extract_info(self, chunk: str, model_config: Any) -> Dict:
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„å®ä½“ï¼ˆEntitiesï¼‰å’Œå…³ç³»ï¼ˆRelationsï¼‰ã€‚
ä»…æå–å…³é”®æˆ–é‡è¦çš„ä¿¡æ¯ã€‚å®ä½“åç§°åº”å°½å¯èƒ½å®Œæ•´ã€‚
ä¸è¦ä½¿ç”¨ e1, e2 ç­‰å ä½ç¬¦ä½œä¸ºå®ä½“åï¼Œç›´æ¥ä½¿ç”¨å®ä½“çš„å®é™…åç§°ã€‚

JSONæ ¼å¼ç¤ºä¾‹:
{{
  "entities": ["æ¢…éœ²å¯", "å›¾å›¾"],
  "relations": [
    {{"subject": "æ¢…éœ²å¯", "predicate": "ä¼™ä¼´", "object": "å›¾å›¾"}}
  ]
}}

æ–‡æœ¬å†…å®¹:
{chunk[:2000]}
"""
        success, response, _, _ = await llm_api.generate_with_model(
            prompt=prompt,
            model_config=model_config,
            request_type="Script.ProcessKnowledge"
        )
        if success:
            try:
                # ç®€å•æ¸…ç†
                txt = response.strip()
                if "```" in txt:
                    txt = txt.split("```json")[-1].split("```")[0].strip()
                    if txt.startswith("json"): txt = txt[4:].strip()
                return json.loads(txt)
            except:
                pass
        return {}

    def _split_text(self, text: str, size=800) -> List[str]:
        # ç®€å•æŒ‰è¡Œåˆ†å—
        lines = text.split("\n")
        chunks = []
        cur = ""
        for line in lines:
            if len(cur) + len(line) > size:
                chunks.append(cur)
                cur = line + "\n"
            else:
                cur += line + "\n"
        if cur: chunks.append(cur)
        return chunks

    async def _add_entity_with_vector(self, name: str, source_paragraph: Optional[str] = None) -> str:
        """æ·»åŠ å®ä½“å¹¶åœ¨å‘é‡åº“ä¸­ç”Ÿæˆç´¢å¼•"""
        # 1. å­˜å…¥å…ƒæ•°æ®å’Œå›¾å­˜å‚¨
        hash_value = self.metadata_store.add_entity(name, source_paragraph=source_paragraph)
        self.graph_store.add_nodes([name])

        # 2. ç”Ÿæˆå‘é‡å¹¶å­˜å…¥å‘é‡åº“
        try:
            emb = await self.embedding_manager.encode(name)
            try:
                self.vector_store.add(emb.reshape(1, -1), [hash_value])
            except ValueError:
                # å¿½ç•¥å·²å­˜åœ¨çš„ID
                pass
        except Exception as e:
            logger.warning(f"  [Error] Failed to vectorize entity {name}: {e}")

        return hash_value

    async def _import_to_db(self, data: Dict):
        """å°†JSONæ•°æ®å¯¼å…¥å­˜å‚¨"""
        # ä½¿ç”¨æ‰¹é‡æ›´æ–°æ¨¡å¼ä¼˜åŒ–å›¾å­˜å‚¨æ€§èƒ½ (é¿å… CSR è­¦å‘Š)
        # æ³¨æ„: batch_update æ˜¯åŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¸å½±å“ async await
        with self.graph_store.batch_update():
            # 1. æŒ‰æ®µè½å¯¼å…¥åŠå…¶å…³è”çš„çŸ¥è¯†
            for item in data.get("paragraphs", []):
                content = item["content"] if isinstance(item, dict) else item
                source = item.get("source", "script") if isinstance(item, dict) else "script"
                
                # å…ƒæ•°æ®åˆ¤å®š
                if self.target_type and self.target_type != "auto":
                    # åŠ¨æ€å¯¼å…¥ get_knowledge_type_from_string
                    plugin_name = self.plugin_config.get("plugin", {}).get("name", "A_memorix") # Fallback name from config or path
                    # Better to reuse the plugin_name calculated at module level, but we are in a method. 
                    # Let's re-calculate or assume module level variable is available if we made it global, 
                    # but here we can just use relative import logic since we know the structure or importlib again.
                    # Actually, `storage_module` from global scope is not easily accessible here unless passed.
                    
                    # Re-calculate cleanly
                    script_path = Path(__file__).resolve()
                    plugin_name = script_path.parent.parent.name
                    storage_mod = importlib.import_module(f"plugins.{plugin_name}.core.storage")
                    get_knowledge_type_from_string = storage_mod.get_knowledge_type_from_string

                    k_type = get_knowledge_type_from_string(self.target_type) or detect_knowledge_type(content)
                else:
                    k_type = detect_knowledge_type(content)
                    
                h_val = self.metadata_store.add_paragraph(content, source, k_type.value)
                
                # å‘é‡
                emb = await self.embedding_manager.encode(content)
                self.vector_store.add(emb.reshape(1, -1), [h_val])
                
                # å¯¼å…¥è¯¥æ®µè½å…³è”çš„å®ä½“ (ç¡®ä¿å­˜åœ¨)
                para_entities = item.get("entities", []) if isinstance(item, dict) else []
                for entity in para_entities:
                    await self._add_entity_with_vector(entity, source_paragraph=h_val)
                    
                # å¯¼å…¥è¯¥æ®µè½å…³è”çš„å…³ç³» (å…³é”®ï¼šä¼ å…¥ h_val)
                para_relations = item.get("relations", []) if isinstance(item, dict) else []
                for rel in para_relations:
                    s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
                    if s and p and o:
                        await self._add_entity_with_vector(s, source_paragraph=h_val)
                        await self._add_entity_with_vector(o, source_paragraph=h_val)
                        
                        self.graph_store.add_edges([(s, o)])
                        # ä¼ å…¥ source_paragraph å“ˆå¸Œ
                        self.metadata_store.add_relation(s, p, o, source_paragraph=h_val)

    def _save_manifest(self):
        with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
            json.dump(self.manifest, f, ensure_ascii=False, indent=2)

async def main():
    quotes = [
        "è¨˜æ†¶ã®ä¸­ã«å±…ãŸ,æ¸©ã‚‚ã‚ŠãŒå´ã«ã„ã¦",
        "å¹¸ç¦ã®åˆ‡ã‚Œç«¯ã‚’,ç¹‹ã„ã§ã„ãŸã€‚é¡˜ã£ã¦ã„ãŸ"
    ]
    logger.info(random.choice(quotes))  # Runtime Easter Egg
    
    parser = argparse.ArgumentParser(description="A_Memorix çŸ¥è¯†åº“è‡ªåŠ¨å¯¼å…¥å·¥å…·")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°å¯¼å…¥æ‰€æœ‰æ–‡ä»¶ï¼Œå¿½ç•¥å·²å¯¼å…¥è®°å½•")
    parser.add_argument("--clear-manifest", action="store_true", help="å¤„ç†å‰æ¸…ç©ºå¯¼å…¥å†å²è®°å½•")
    parser.add_argument("--type", "-t", choices=["structured", "narrative", "factual", "mixed", "auto"], default="auto", help="å¼ºåˆ¶æŒ‡å®šæ‰€æœ‰å¯¼å…¥æ–‡ä»¶çš„çŸ¥è¯†ç±»å‹")
    parser.add_argument("--concurrency", "-c", type=int, default=5, help="LLM å¹¶å‘è¯·æ±‚æ•°é‡é™åˆ¶ (é»˜è®¤: 5)")
    args = parser.parse_args()

    if not global_config or not model_config:
        logger.error("å…¨å±€é…ç½®æœªåŠ è½½")
        return
        
    importer = AutoImporter(
        force=args.force, 
        clear_manifest=args.clear_manifest, 
        target_type=args.type,
        concurrency=args.concurrency
    )
    await importer.process_and_import()

if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
