"""
èŠå¤©æ€»ç»“ä¸çŸ¥è¯†å¯¼å…¥å·¥å…·

è¯¥æ¨¡å—è´Ÿè´£ä»èŠå¤©è®°å½•ä¸­æå–ä¿¡æ¯ï¼Œç”Ÿæˆæ€»ç»“ï¼Œå¹¶å°†æ€»ç»“å†…å®¹åŠæå–çš„å®ä½“/å…³ç³»
å¯¼å…¥åˆ° A_memorix çš„å­˜å‚¨ç»„ä»¶ä¸­ã€‚
"""

import time
import json
import re
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.apis import llm_api, message_api
from src.chat.utils.prompt_builder import global_prompt_manager, Prompt
from src.config.config import global_config

from ..storage import (
    VectorStore,
    GraphStore,
    MetadataStore,
    KnowledgeType,
    get_knowledge_type_from_string
)
from ..embedding import EmbeddingAPIAdapter

logger = get_logger("A_Memorix.SummaryImporter")

# é»˜è®¤æ€»ç»“æç¤ºè¯æ¨¡ç‰ˆ
SUMMARY_PROMPT_TEMPLATE = """
ä½ æ˜¯ {bot_name}ã€‚{personality_context}
ç°åœ¨ä½ éœ€è¦å¯¹ä»¥ä¸‹ä¸€æ®µèŠå¤©è®°å½•è¿›è¡Œæ€»ç»“ï¼Œå¹¶æå–å…¶ä¸­çš„é‡è¦çŸ¥è¯†ã€‚

èŠå¤©è®°å½•å†…å®¹ï¼š
{chat_history}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. **ç”Ÿæˆæ€»ç»“**ï¼šä»¥ç¬¬ä¸‰äººç§°æˆ–æœºå™¨äººçš„è§†è§’ï¼Œç®€æ´æ˜äº†åœ°æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€å‘ç”Ÿçš„äº‹ä»¶æˆ–è®¨è®ºçš„ä¸»é¢˜ã€‚
2. **æå–å®ä½“ä¸å…³ç³»**ï¼šè¯†åˆ«å¹¶æå–å¯¹è¯ä¸­æåˆ°çš„é‡è¦å®ä½“ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

è¯·ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "summary": "æ€»ç»“æ–‡æœ¬å†…å®¹",
  "entities": ["å¼ ä¸‰", "æå››"],
  "relations": [
    {{"subject": "å¼ ä¸‰", "predicate": "è®¤è¯†", "object": "æå››"}}
  ]
}}

æ³¨æ„ï¼šæ€»ç»“åº”å…·æœ‰å™äº‹æ€§ï¼Œèƒ½å¤Ÿä½œä¸ºé•¿ç¨‹è®°å¿†çš„ä¸€éƒ¨åˆ†ã€‚ç›´æ¥ä½¿ç”¨å®ä½“çš„å®é™…åç§°ï¼Œä¸è¦ä½¿ç”¨ e1/e2 ç­‰ä»£å·ã€‚
"""

class SummaryImporter:
    """æ€»ç»“å¹¶å¯¼å…¥çŸ¥è¯†çš„å·¥å…·ç±»"""

    def __init__(
        self,
        vector_store: VectorStore,
        graph_store: GraphStore,
        metadata_store: MetadataStore,
        embedding_manager: EmbeddingAPIAdapter,
        plugin_config: dict
    ):
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.metadata_store = metadata_store
        self.embedding_manager = embedding_manager
        self.plugin_config = plugin_config

    async def import_from_stream(
        self,
        stream_id: str,
        context_length: Optional[int] = None,
        include_personality: Optional[bool] = None
    ) -> Tuple[bool, str]:
        """
        ä»æŒ‡å®šçš„èŠå¤©æµä¸­æå–è®°å½•å¹¶æ‰§è¡Œæ€»ç»“å¯¼å…¥

        Args:
            stream_id: èŠå¤©æµ ID
            context_length: æ€»ç»“çš„å†å²æ¶ˆæ¯æ¡æ•°
            include_personality: æ˜¯å¦åŒ…å«äººè®¾

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            # 1. è·å–é…ç½®
            if context_length is None:
                context_length = self.plugin_config.get("summarization", {}).get("context_length", 50)
            
            if include_personality is None:
                include_personality = self.plugin_config.get("summarization", {}).get("include_personality", True)

            # 2. è·å–å†å²æ¶ˆæ¯
            # è·å–å½“å‰æ—¶é—´ä¹‹å‰çš„æ¶ˆæ¯
            now = time.time()
            messages = message_api.get_messages_before_time_in_chat(
                chat_id=stream_id,
                timestamp=now,
                limit=context_length
            )

            if not messages:
                return False, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„èŠå¤©è®°å½•è¿›è¡Œæ€»ç»“"

            # è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
            chat_history_text = message_api.build_readable_messages_to_str(messages)
            
            # 3. å‡†å¤‡æç¤ºè¯å†…å®¹
            bot_name = global_config.bot.nickname or "æœºå™¨äºº"
            personality_context = ""
            if include_personality:
                personality = getattr(global_config.bot, "personality", "")
                if personality:
                    personality_context = f"ä½ çš„æ€§æ ¼è®¾å®šæ˜¯ï¼š{personality}"

            # 4. è°ƒç”¨ LLM
            prompt = SUMMARY_PROMPT_TEMPLATE.format(
                bot_name=bot_name,
                personality_context=personality_context,
                chat_history=chat_history_text
            )

            model_name = self.plugin_config.get("summarization", {}).get("model_name", "auto")
            
            # è·å–å¯ç”¨æ¨¡å‹å¹¶åŒ¹é…
            available_models = llm_api.get_available_models()
            model_config_to_use = None
            if model_name in available_models:
                model_config_to_use = available_models[model_name]
            elif "balanced" in available_models:
                model_config_to_use = available_models["balanced"]
            elif available_models:
                model_config_to_use = list(available_models.values())[0]

            logger.info(f"æ­£åœ¨ä¸ºæµ {stream_id} æ‰§è¡Œæ€»ç»“ï¼Œæ¶ˆæ¯æ¡æ•°: {len(messages)}")

            success, response, _, _ = await llm_api.generate_with_model(
                prompt=prompt,
                model_config=model_config_to_use,
                request_type="A_Memorix.ChatSummarization"
            )

            if not success or not response:
                return False, "LLM ç”Ÿæˆæ€»ç»“å¤±è´¥"

            # 5. è§£æç»“æœ
            data = self._parse_llm_response(response)
            if not data or "summary" not in data:
                return False, "è§£æ LLM å“åº”å¤±è´¥æˆ–æ€»ç»“ä¸ºç©º"

            summary_text = data["summary"]
            entities = data.get("entities", [])
            relations = data.get("relations", [])

            # 6. æ‰§è¡Œå¯¼å…¥
            await self._execute_import(summary_text, entities, relations, stream_id)

            # 7. æŒä¹…åŒ–
            self.vector_store.save()
            self.graph_store.save()

            result_msg = (
                f"âœ… æ€»ç»“å¯¼å…¥æˆåŠŸ\n"
                f"ğŸ“ æ€»ç»“é•¿åº¦: {len(summary_text)}\n"
                f"ğŸ“Œ æå–å®ä½“: {len(entities)}\n"
                f"ğŸ”— æå–å…³ç³»: {len(relations)}"
            )
            return True, result_msg

        except Exception as e:
            logger.error(f"æ€»ç»“å¯¼å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
            return False, f"é”™è¯¯: {str(e)}"

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM è¿”å›çš„ JSON"""
        try:
            # å°è¯•æŸ¥æ‰¾ JSON
            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return {}
        except Exception as e:
            logger.warning(f"è§£ææ€»ç»“ JSON å¤±è´¥: {e}")
            return {}

    async def _execute_import(
        self,
        summary: str,
        entities: List[str],
        relations: List[Dict[str, str]],
        stream_id: str
    ):
        """å°†æ•°æ®å†™å…¥å­˜å‚¨"""
        # è·å–é»˜è®¤çŸ¥è¯†ç±»å‹
        type_str = self.plugin_config.get("summarization", {}).get("default_knowledge_type", "narrative")
        knowledge_type = get_knowledge_type_from_string(type_str) or KnowledgeType.NARRATIVE

        # å¯¼å…¥æ€»ç»“æ–‡æœ¬
        hash_value = self.metadata_store.add_paragraph(
            content=summary,
            source=f"chat_summary:{stream_id}",
            knowledge_type=knowledge_type.value
        )

        embedding = await self.embedding_manager.encode(summary)
        self.vector_store.add(
            vectors=embedding.reshape(1, -1),
            ids=[hash_value]
        )

        # å¯¼å…¥å®ä½“
        if entities:
            self.graph_store.add_nodes(entities)

        # å¯¼å…¥å…³ç³»
        for rel in relations:
            s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
            if all([s, p, o]):
                # å†™å…¥å…ƒæ•°æ®
                rel_hash = self.metadata_store.add_relation(
                    subject=s,
                    predicate=p,
                    obj=o,
                    confidence=1.0,
                    source_paragraph=summary
                )
                # å†™å…¥å›¾æ•°æ®åº“
                self.graph_store.add_edges([(s, o)])
                
        logger.info(f"æ€»ç»“å¯¼å…¥å®Œæˆ: hash={hash_value[:8]}")
