"""
èŠå¤©æ€»ç»“ä¸çŸ¥è¯†å¯¼å…¥å·¥å…·

è¯¥æ¨¡å—è´Ÿè´£ä»èŠå¤©è®°å½•ä¸­æå–ä¿¡æ¯ï¼Œç”Ÿæˆæ€»ç»“ï¼Œå¹¶å°†æ€»ç»“å†…å®¹åŠæå–çš„å®ä½“/å…³ç³»
å¯¼å…¥åˆ° A_memorix çš„å­˜å‚¨ç»„ä»¶ä¸­ã€‚
"""

import time
import json
import re
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.apis import llm_api, message_api
from src.chat.utils.prompt_builder import global_prompt_manager, Prompt
from src.config.config import global_config, model_config as host_model_config
from src.config.api_ada_configs import TaskConfig

from ..storage import (
    VectorStore,
    GraphStore,
    MetadataStore,
    KnowledgeType,
    get_knowledge_type_from_string
)
from ..embedding import EmbeddingAPIAdapter

logger = get_logger("A_Memorix.SummaryImporter")

# é»˜è®¤æ€»ç»“æç¤ºè¯æ¨¡ç‰ˆ
SUMMARY_PROMPT_TEMPLATE = """
ä½ æ˜¯ {bot_name}ã€‚{personality_context}
ç°åœ¨ä½ éœ€è¦å¯¹ä»¥ä¸‹ä¸€æ®µèŠå¤©è®°å½•è¿›è¡Œæ€»ç»“ï¼Œå¹¶æå–å…¶ä¸­çš„é‡è¦çŸ¥è¯†ã€‚

èŠå¤©è®°å½•å†…å®¹ï¼š
{chat_history}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. **ç”Ÿæˆæ€»ç»“**ï¼šä»¥ç¬¬ä¸‰äººç§°æˆ–æœºå™¨äººçš„è§†è§’ï¼Œç®€æ´æ˜äº†åœ°æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€å‘ç”Ÿçš„äº‹ä»¶æˆ–è®¨è®ºçš„ä¸»é¢˜ã€‚
2. **æå–å®ä½“ä¸å…³ç³»**ï¼šè¯†åˆ«å¹¶æå–å¯¹è¯ä¸­æåˆ°çš„é‡è¦å®ä½“ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

è¯·ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "summary": "æ€»ç»“æ–‡æœ¬å†…å®¹",
  "entities": ["å¼ ä¸‰", "æå››"],
  "relations": [
    {{"subject": "å¼ ä¸‰", "predicate": "è®¤è¯†", "object": "æå››"}}
  ]
}}

æ³¨æ„ï¼šæ€»ç»“åº”å…·æœ‰å™äº‹æ€§ï¼Œèƒ½å¤Ÿä½œä¸ºé•¿ç¨‹è®°å¿†çš„ä¸€éƒ¨åˆ†ã€‚ç›´æ¥ä½¿ç”¨å®ä½“çš„å®é™…åç§°ï¼Œä¸è¦ä½¿ç”¨ e1/e2 ç­‰ä»£å·ã€‚
"""

class SummaryImporter:
    """æ€»ç»“å¹¶å¯¼å…¥çŸ¥è¯†çš„å·¥å…·ç±»"""

    def __init__(
        self,
        vector_store: VectorStore,
        graph_store: GraphStore,
        metadata_store: MetadataStore,
        embedding_manager: EmbeddingAPIAdapter,
        plugin_config: dict
    ):
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.metadata_store = metadata_store
        self.embedding_manager = embedding_manager
        self.plugin_config = plugin_config

    def _normalize_summary_model_selectors(self, raw_value: Any) -> List[str]:
        """æ ‡å‡†åŒ– summarization.model_name é…ç½®ï¼Œå…¼å®¹å­—ç¬¦ä¸²å’Œå­—ç¬¦ä¸²æ•°ç»„ã€‚"""
        if raw_value is None:
            return ["auto"]
        if isinstance(raw_value, str):
            v = raw_value.strip()
            if not v:
                return ["auto"]
            # å…¼å®¹å†™æ³•: "utils:model1","utils:model2",replyer
            if "," in v:
                parts = [part.strip().strip("'\"") for part in v.split(",")]
                selectors = [part for part in parts if part]
                return selectors or ["auto"]
            return [v]
        if isinstance(raw_value, list):
            selectors = [str(x).strip() for x in raw_value if str(x).strip()]
            return selectors or ["auto"]
        v = str(raw_value).strip()
        return [v] if v else ["auto"]

    def _pick_default_summary_task(self, available_tasks: Dict[str, TaskConfig]) -> Tuple[Optional[str], Optional[TaskConfig]]:
        """
        é€‰æ‹©æ€»ç»“é»˜è®¤ä»»åŠ¡ï¼Œé¿å…é”™è¯¯è½åˆ° embedding ä»»åŠ¡ã€‚
        ä¼˜å…ˆçº§ï¼šreplyer > utils > planner > tool_use > å…¶ä»–é embeddingã€‚
        """
        preferred = ("replyer", "utils", "planner", "tool_use")
        for name in preferred:
            cfg = available_tasks.get(name)
            if cfg and cfg.model_list:
                return name, cfg

        for name, cfg in available_tasks.items():
            if name != "embedding" and cfg.model_list:
                return name, cfg

        for name, cfg in available_tasks.items():
            if cfg.model_list:
                return name, cfg

        return None, None

    def _resolve_summary_model_config(self) -> Optional[TaskConfig]:
        """
        è§£æ summarization.model_name ä¸º TaskConfigã€‚
        æ”¯æŒï¼š
        - "auto"
        - "replyer"ï¼ˆä»»åŠ¡åï¼‰
        - "some-model-name"ï¼ˆå…·ä½“æ¨¡å‹åï¼‰
        - ["utils:model1", "utils:model2", "replyer"]ï¼ˆæ•°ç»„æ··åˆè¯­æ³•ï¼‰
        """
        available_tasks = llm_api.get_available_models()
        if not available_tasks:
            return None

        raw_cfg = self.plugin_config.get("summarization", {}).get("model_name", "auto")
        selectors = self._normalize_summary_model_selectors(raw_cfg)
        default_task_name, default_task_cfg = self._pick_default_summary_task(available_tasks)

        selected_models: List[str] = []
        base_cfg: Optional[TaskConfig] = None
        model_dict = getattr(host_model_config, "models_dict", {})

        def _append_models(models: List[str]):
            for model_name in models:
                if model_name and model_name not in selected_models:
                    selected_models.append(model_name)

        for raw_selector in selectors:
            selector = raw_selector.strip()
            if not selector:
                continue

            if selector.lower() == "auto":
                if default_task_cfg:
                    _append_models(default_task_cfg.model_list)
                    if base_cfg is None:
                        base_cfg = default_task_cfg
                continue

            if ":" in selector:
                task_name, model_name = selector.split(":", 1)
                task_name = task_name.strip()
                model_name = model_name.strip()
                task_cfg = available_tasks.get(task_name)
                if not task_cfg:
                    logger.warning(f"æ€»ç»“æ¨¡å‹é€‰æ‹©å™¨ '{selector}' çš„ä»»åŠ¡ '{task_name}' ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡")
                    continue

                if base_cfg is None:
                    base_cfg = task_cfg

                if not model_name or model_name.lower() == "auto":
                    _append_models(task_cfg.model_list)
                    continue

                if model_name in model_dict or model_name in task_cfg.model_list:
                    _append_models([model_name])
                else:
                    logger.warning(f"æ€»ç»“æ¨¡å‹é€‰æ‹©å™¨ '{selector}' çš„æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡")
                continue

            task_cfg = available_tasks.get(selector)
            if task_cfg:
                _append_models(task_cfg.model_list)
                if base_cfg is None:
                    base_cfg = task_cfg
                continue

            if selector in model_dict:
                _append_models([selector])
                continue

            logger.warning(f"æ€»ç»“æ¨¡å‹é€‰æ‹©å™¨ '{selector}' æ— æ³•è¯†åˆ«ï¼Œå·²è·³è¿‡")

        if not selected_models:
            if default_task_cfg:
                _append_models(default_task_cfg.model_list)
                if base_cfg is None:
                    base_cfg = default_task_cfg
            else:
                first_cfg = next(iter(available_tasks.values()))
                _append_models(first_cfg.model_list)
                if base_cfg is None:
                    base_cfg = first_cfg

        if not selected_models:
            return None

        template_cfg = base_cfg or default_task_cfg or next(iter(available_tasks.values()))
        return TaskConfig(
            model_list=selected_models,
            max_tokens=template_cfg.max_tokens,
            temperature=template_cfg.temperature,
            slow_threshold=template_cfg.slow_threshold,
            selection_strategy=template_cfg.selection_strategy,
        )

    async def import_from_stream(
        self,
        stream_id: str,
        context_length: Optional[int] = None,
        include_personality: Optional[bool] = None
    ) -> Tuple[bool, str]:
        """
        ä»æŒ‡å®šçš„èŠå¤©æµä¸­æå–è®°å½•å¹¶æ‰§è¡Œæ€»ç»“å¯¼å…¥

        Args:
            stream_id: èŠå¤©æµ ID
            context_length: æ€»ç»“çš„å†å²æ¶ˆæ¯æ¡æ•°
            include_personality: æ˜¯å¦åŒ…å«äººè®¾

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            # 1. è·å–é…ç½®
            if context_length is None:
                context_length = self.plugin_config.get("summarization", {}).get("context_length", 50)
            
            if include_personality is None:
                include_personality = self.plugin_config.get("summarization", {}).get("include_personality", True)

            # 2. è·å–å†å²æ¶ˆæ¯
            # è·å–å½“å‰æ—¶é—´ä¹‹å‰çš„æ¶ˆæ¯
            now = time.time()
            messages = message_api.get_messages_before_time_in_chat(
                chat_id=stream_id,
                timestamp=now,
                limit=context_length
            )

            if not messages:
                return False, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„èŠå¤©è®°å½•è¿›è¡Œæ€»ç»“"

            # è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
            chat_history_text = message_api.build_readable_messages_to_str(messages)
            
            # 3. å‡†å¤‡æç¤ºè¯å†…å®¹
            bot_name = global_config.bot.nickname or "æœºå™¨äºº"
            personality_context = ""
            if include_personality:
                personality = getattr(global_config.bot, "personality", "")
                if personality:
                    personality_context = f"ä½ çš„æ€§æ ¼è®¾å®šæ˜¯ï¼š{personality}"

            # 4. è°ƒç”¨ LLM
            prompt = SUMMARY_PROMPT_TEMPLATE.format(
                bot_name=bot_name,
                personality_context=personality_context,
                chat_history=chat_history_text
            )

            model_config_to_use = self._resolve_summary_model_config()
            if model_config_to_use is None:
                return False, "æœªæ‰¾åˆ°å¯ç”¨çš„æ€»ç»“æ¨¡å‹é…ç½®"

            logger.info(f"æ­£åœ¨ä¸ºæµ {stream_id} æ‰§è¡Œæ€»ç»“ï¼Œæ¶ˆæ¯æ¡æ•°: {len(messages)}")
            logger.info(f"æ€»ç»“æ¨¡å‹å€™é€‰åˆ—è¡¨: {model_config_to_use.model_list}")

            success, response, _, _ = await llm_api.generate_with_model(
                prompt=prompt,
                model_config=model_config_to_use,
                request_type="A_Memorix.ChatSummarization"
            )

            if not success or not response:
                return False, "LLM ç”Ÿæˆæ€»ç»“å¤±è´¥"

            # 5. è§£æç»“æœ
            data = self._parse_llm_response(response)
            if not data or "summary" not in data:
                return False, "è§£æ LLM å“åº”å¤±è´¥æˆ–æ€»ç»“ä¸ºç©º"

            summary_text = data["summary"]
            entities = data.get("entities", [])
            relations = data.get("relations", [])

            # 6. æ‰§è¡Œå¯¼å…¥
            await self._execute_import(summary_text, entities, relations, stream_id)

            # 7. æŒä¹…åŒ–
            self.vector_store.save()
            self.graph_store.save()

            result_msg = (
                f"âœ… æ€»ç»“å¯¼å…¥æˆåŠŸ\n"
                f"ğŸ“ æ€»ç»“é•¿åº¦: {len(summary_text)}\n"
                f"ğŸ“Œ æå–å®ä½“: {len(entities)}\n"
                f"ğŸ”— æå–å…³ç³»: {len(relations)}"
            )
            return True, result_msg

        except Exception as e:
            logger.error(f"æ€»ç»“å¯¼å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
            return False, f"é”™è¯¯: {str(e)}"

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM è¿”å›çš„ JSON"""
        try:
            # å°è¯•æŸ¥æ‰¾ JSON
            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return {}
        except Exception as e:
            logger.warning(f"è§£ææ€»ç»“ JSON å¤±è´¥: {e}")
            return {}

    async def _execute_import(
        self,
        summary: str,
        entities: List[str],
        relations: List[Dict[str, str]],
        stream_id: str
    ):
        """å°†æ•°æ®å†™å…¥å­˜å‚¨"""
        # è·å–é»˜è®¤çŸ¥è¯†ç±»å‹
        type_str = self.plugin_config.get("summarization", {}).get("default_knowledge_type", "narrative")
        knowledge_type = get_knowledge_type_from_string(type_str) or KnowledgeType.NARRATIVE

        # å¯¼å…¥æ€»ç»“æ–‡æœ¬
        hash_value = self.metadata_store.add_paragraph(
            content=summary,
            source=f"chat_summary:{stream_id}",
            knowledge_type=knowledge_type.value
        )

        embedding = await self.embedding_manager.encode(summary)
        self.vector_store.add(
            vectors=embedding.reshape(1, -1),
            ids=[hash_value]
        )

        # å¯¼å…¥å®ä½“
        if entities:
            self.graph_store.add_nodes(entities)

        # å¯¼å…¥å…³ç³»
        for rel in relations:
            s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
            if all([s, p, o]):
                # å†™å…¥å…ƒæ•°æ®
                rel_hash = self.metadata_store.add_relation(
                    subject=s,
                    predicate=p,
                    obj=o,
                    confidence=1.0,
                    source_paragraph=summary
                )
                # å†™å…¥å›¾æ•°æ®åº“
                self.graph_store.add_edges([(s, o)])
                
        logger.info(f"æ€»ç»“å¯¼å…¥å®Œæˆ: hash={hash_value[:8]}")
