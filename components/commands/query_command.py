"""
æŸ¥è¯¢çŸ¥è¯†Commandç»„ä»¶

æä¾›çŸ¥è¯†åº“æŸ¥è¯¢åŠŸèƒ½ï¼Œæ”¯æŒæ®µè½å’Œå…³ç³»æŸ¥è¯¢ã€‚
"""

import time
import re
from typing import Tuple, Optional, List, Dict, Any
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.base.base_command import BaseCommand
from src.chat.message_receive.message import MessageRecv

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from ...core import (
    DualPathRetriever,
    RetrievalStrategy,
    DualPathRetrieverConfig,
    TemporalQueryOptions,
    DynamicThresholdFilter,
    ThresholdMethod,
    ThresholdConfig,
    SparseBM25Config,
    FusionConfig,
)
from ...core.utils.time_parser import parse_query_time_range

logger = get_logger("A_Memorix.QueryCommand")


class QueryCommand(BaseCommand):
    """æŸ¥è¯¢çŸ¥è¯†Command

    åŠŸèƒ½ï¼š
    - åŒè·¯æ£€ç´¢æŸ¥è¯¢
    - å®ä½“æŸ¥è¯¢
    - å…³ç³»æŸ¥è¯¢
    - ç»Ÿè®¡ä¿¡æ¯æŸ¥è¯¢
    """

    # CommandåŸºæœ¬ä¿¡æ¯
    command_name = "query"
    command_description = "æŸ¥è¯¢çŸ¥è¯†åº“ï¼Œæ”¯æŒæ£€ç´¢ã€å®ä½“ã€å…³ç³»å’Œç»Ÿè®¡ä¿¡æ¯"
    command_pattern = r"^\/query(?:\s+(?P<mode>\w+))?(?:\s+(?P<content>.+))?$"

    def __init__(self, message: MessageRecv, plugin_config: Optional[dict] = None):
        """åˆå§‹åŒ–æŸ¥è¯¢Command"""
        super().__init__(message, plugin_config)

        logger.info(f"QueryCommand åˆå§‹åŒ–å¼€å§‹")
        logger.info(f"  plugin_config keys: {list(self.plugin_config.keys()) if self.plugin_config else 'None'}")

        # è·å–å­˜å‚¨å®ä¾‹ (ä¼˜å…ˆä»é…ç½®è·å–ï¼Œå…œåº•ä»æ’ä»¶å®ä¾‹è·å–)
        self.vector_store = self.plugin_config.get("vector_store")
        self.graph_store = self.plugin_config.get("graph_store")
        self.metadata_store = self.plugin_config.get("metadata_store")
        self.embedding_manager = self.plugin_config.get("embedding_manager")
        self.sparse_index = self.plugin_config.get("sparse_index")

        logger.info(f"  ä» plugin_config è·å–: vector_store={self.vector_store is not None}, "
                   f"graph_store={self.graph_store is not None}, "
                   f"metadata_store={self.metadata_store is not None}, "
                   f"embedding_manager={self.embedding_manager is not None}")

        # å…œåº•é€»è¾‘ï¼šå¦‚æœé…ç½®ä¸­æ²¡æœ‰å­˜å‚¨å®ä¾‹ï¼Œå°è¯•ç›´æ¥ä»æ’ä»¶ç³»ç»Ÿè·å–
        # ä½¿ç”¨ is not None æ£€æŸ¥ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½å¸ƒå°”å€¼ä¸º False
        if not all([
            self.vector_store is not None,
            self.graph_store is not None,
            self.metadata_store is not None,
            self.embedding_manager is not None
        ]):
            logger.warning(f"  é…ç½®ä¸å®Œæ•´ï¼Œå°è¯•ä»æ’ä»¶å®ä¾‹è·å–...")
            try:
                from ...plugin import A_MemorixPlugin
                instances = A_MemorixPlugin.get_storage_instances()
                logger.info(f"  get_storage_instances() è¿”å›: {list(instances.keys()) if instances else 'empty dict'}")
                
                if instances:
                    self.vector_store = self.vector_store or instances.get("vector_store")
                    self.graph_store = self.graph_store or instances.get("graph_store")
                    self.metadata_store = self.metadata_store or instances.get("metadata_store")
                    self.embedding_manager = self.embedding_manager or instances.get("embedding_manager")
                    self.sparse_index = self.sparse_index or instances.get("sparse_index")
                    
                    logger.info(f"  å…œåº•å: vector_store={self.vector_store is not None}, "
                               f"graph_store={self.graph_store is not None}, "
                               f"metadata_store={self.metadata_store is not None}, "
                               f"embedding_manager={self.embedding_manager is not None}")
                else:
                    logger.error(f"  get_storage_instances() è¿”å›ç©ºå­—å…¸ï¼")
            except Exception as e:
                logger.error(f"  å…œåº•é€»è¾‘å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.retriever: Optional[DualPathRetriever] = None
        self.threshold_filter: Optional[DynamicThresholdFilter] = None

        # è®¾ç½®æ—¥å¿—å‰ç¼€
        if self.message and self.message.chat_stream:
            self.log_prefix = f"[QueryCommand-{self.message.chat_stream.stream_id}]"
        else:
            self.log_prefix = "[QueryCommand]"

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

    @property
    def debug_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è°ƒè¯•æ¨¡å¼"""
        advanced = self.plugin_config.get("advanced", {})
        if isinstance(advanced, dict):
            return advanced.get("debug", False)
        return self.plugin_config.get("debug", False)

    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æ£€ç´¢å’Œè¿‡æ»¤ç»„ä»¶"""
        try:
            # æ£€æŸ¥å­˜å‚¨æ˜¯å¦å¯ç”¨ (ä½¿ç”¨ is not None è€Œéå¸ƒå°”å€¼ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½ä¸º False)
            if not all([
                self.vector_store is not None,
                self.graph_store is not None,
                self.metadata_store is not None,
                self.embedding_manager is not None
            ]):
                logger.warning(f"{self.log_prefix} å­˜å‚¨ç»„ä»¶æœªå®Œå…¨åˆå§‹åŒ–")
                return

            # åˆ›å»ºæ£€ç´¢å™¨é…ç½®
            sparse_cfg_raw = self.get_config("retrieval.sparse", {}) or {}
            if not isinstance(sparse_cfg_raw, dict):
                sparse_cfg_raw = {}
            fusion_cfg_raw = self.get_config("retrieval.fusion", {}) or {}
            if not isinstance(fusion_cfg_raw, dict):
                fusion_cfg_raw = {}
            try:
                sparse_cfg = SparseBM25Config(**sparse_cfg_raw)
            except Exception as e:
                logger.warning(f"{self.log_prefix} sparse é…ç½®éæ³•ï¼Œå›é€€é»˜è®¤: {e}")
                sparse_cfg = SparseBM25Config()
            try:
                fusion_cfg = FusionConfig(**fusion_cfg_raw)
            except Exception as e:
                logger.warning(f"{self.log_prefix} fusion é…ç½®éæ³•ï¼Œå›é€€é»˜è®¤: {e}")
                fusion_cfg = FusionConfig()
            config = DualPathRetrieverConfig(
                top_k_paragraphs=self.get_config("retrieval.top_k_paragraphs", 20),
                top_k_relations=self.get_config("retrieval.top_k_relations", 10),
                top_k_final=self.get_config("retrieval.top_k_final", 10),
                alpha=self.get_config("retrieval.alpha", 0.5),
                enable_ppr=self.get_config("retrieval.enable_ppr", True),
                ppr_alpha=self.get_config("retrieval.ppr_alpha", 0.85),
                ppr_concurrency_limit=self.get_config("retrieval.ppr_concurrency_limit", 4),
                enable_parallel=self.get_config("retrieval.enable_parallel", True),
                retrieval_strategy=RetrievalStrategy.DUAL_PATH,
                debug=self.debug_enabled,
                sparse=sparse_cfg,
                fusion=fusion_cfg,
            )

            # åˆ›å»ºæ£€ç´¢å™¨
            self.retriever = DualPathRetriever(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                metadata_store=self.metadata_store,
                embedding_manager=self.embedding_manager,
                sparse_index=self.sparse_index,
                config=config,
            )

            # åˆ›å»ºé˜ˆå€¼è¿‡æ»¤å™¨
            threshold_config = ThresholdConfig(
                method=ThresholdMethod.ADAPTIVE,
                min_threshold=self.get_config("threshold.min_threshold", 0.3),
                max_threshold=self.get_config("threshold.max_threshold", 0.95),
                percentile=self.get_config("threshold.percentile", 75.0),
                std_multiplier=self.get_config("threshold.std_multiplier", 1.5),
                min_results=self.get_config("threshold.min_results", 3),
                enable_auto_adjust=self.get_config("threshold.enable_auto_adjust", True),
            )

            self.threshold_filter = DynamicThresholdFilter(threshold_config)

            logger.info(f"{self.log_prefix} æŸ¥è¯¢ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"{self.log_prefix} ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

    async def execute(self) -> Tuple[bool, Optional[str], int]:
        """æ‰§è¡ŒæŸ¥è¯¢å‘½ä»¤

        Returns:
            Tuple[bool, Optional[str], int]: (æ˜¯å¦æˆåŠŸ, å›å¤æ¶ˆæ¯, æ‹¦æˆªçº§åˆ«)
        """
        # æ£€æŸ¥ç»„ä»¶æ˜¯å¦åˆå§‹åŒ–
        if not self.retriever:
            error_msg = "âŒ æŸ¥è¯¢ç»„ä»¶æœªåˆå§‹åŒ–"
            return False, error_msg, 0

        # è·å–åŒ¹é…çš„å‚æ•°
        mode = self.matched_groups.get("mode", "search")
        content = self.matched_groups.get("content", "")

        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œæ˜¾ç¤ºå¸®åŠ©
        if not content and mode not in ["stats", "help"]:
            help_msg = self._get_help_message()
            return True, help_msg, 0

        logger.info(f"{self.log_prefix} æ‰§è¡ŒæŸ¥è¯¢: mode={mode}, content='{content}'")

        try:
            # æ ¹æ®æ¨¡å¼æ‰§è¡ŒæŸ¥è¯¢
            if mode == "search" or mode == "s":
                success, result = await self._query_search(content)
            elif mode == "time" or mode == "t":
                success, result = await self._query_time(content)
            elif mode == "entity" or mode == "e":
                success, result = await self._query_entity(content)
            elif mode == "relation" or mode == "r":
                success, result = await self._query_relation(content)
            elif mode == "stats":
                success, result = self._query_stats()
            elif mode == "help":
                success, result = True, self._get_help_message()
            else:
                success, result = False, f"âŒ æœªçŸ¥çš„æŸ¥è¯¢æ¨¡å¼: {mode}"

            return success, result, 0

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
            logger.error(f"{self.log_prefix} {error_msg}")
            return False, error_msg, 0

    async def _query_search(self, query: str) -> Tuple[bool, str]:
        """æ‰§è¡Œæ£€ç´¢æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        start_time = time.time()

        # æ‰§è¡Œæ£€ç´¢ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
        results = await self.retriever.retrieve(query, top_k=10)

        if self.debug_enabled:
            logger.info(f"{self.log_prefix} [DEBUG] åŸå§‹æ£€ç´¢ç»“æœæ•°é‡: {len(results)}")
            for i, r in enumerate(results):
                logger.info(f"{self.log_prefix} [DEBUG] Result {i}: type={r.result_type}, score={r.score:.4f}, hash={r.hash_value}")

        # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        if self.threshold_filter:
            results = self.threshold_filter.filter(results)
            if self.debug_enabled:
                logger.info(f"{self.log_prefix} [DEBUG] è¿‡æ»¤åç»“æœæ•°é‡: {len(results)}")

        elapsed = time.time() - start_time

        # æ ¼å¼åŒ–ç»“æœ
        if not results:
            return True, f"ğŸ” æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼ˆè€—æ—¶: {elapsed*1000:.1f}msï¼‰"

        # æŒ‰ç±»å‹åˆ†ç»„
        paragraphs = [r for r in results if r.result_type == "paragraph"]
        relations = [r for r in results if r.result_type == "relation"]

        # æ„å»ºå“åº”
        lines = [
            f"ğŸ” æ£€ç´¢ç»“æœï¼ˆæŸ¥è¯¢: '{query}'ï¼Œè€—æ—¶: {elapsed*1000:.1f}msï¼‰",
            "",
        ]

        if paragraphs:
            lines.append("ğŸ“„ åŒ¹é…çš„æ®µè½ï¼š")
            for i, result in enumerate(paragraphs[:5], 1):
                score_pct = result.score * 100
                content = result.content[:80] + "..." if len(result.content) > 80 else result.content
                lines.append(f"  {i}. [{score_pct:.1f}%] {content}")
            lines.append("")

        if relations:
            lines.append("ğŸ”— åŒ¹é…çš„å…³ç³»ï¼š")
            for i, result in enumerate(relations[:5], 1):
                score_pct = result.score * 100
                subject = result.metadata.get("subject", "")
                predicate = result.metadata.get("predicate", "")
                obj = result.metadata.get("object", "")
                lines.append(f"  {i}. [{score_pct:.1f}%] {subject} {predicate} {obj}")
            lines.append("")

        lines.append(f"ğŸ“Š å…± {len(results)} æ¡ç»“æœï¼ˆæ®µè½: {len(paragraphs)}, å…³ç³»: {len(relations)}ï¼‰")

        return True, "\n".join(lines)

    def _parse_kv_args(self, raw: str) -> Dict[str, str]:
        """
        è§£æ k=v å‚æ•°ï¼Œæ”¯æŒå¼•å·ã€‚
        ç¤ºä¾‹: q="é¡¹ç›®è¿›å±•" from=2025/01/01 to="2025/01/31 12:00"
        """
        pattern = re.compile(r"(\w+)=((?:\"[^\"]*\")|(?:'[^']*')|(?:\S+))")
        parsed: Dict[str, str] = {}
        for match in pattern.finditer(raw):
            key = match.group(1).strip().lower()
            value = match.group(2).strip()
            if len(value) >= 2 and (
                (value[0] == '"' and value[-1] == '"')
                or (value[0] == "'" and value[-1] == "'")
            ):
                value = value[1:-1]
            parsed[key] = value.strip()
        return parsed

    async def _query_time(self, content: str) -> Tuple[bool, str]:
        """
        æ—¶åºæ£€ç´¢: /query time q=... from=... to=... person=... source=... top_k=...
        """
        if not bool(self.get_config("retrieval.temporal.enabled", True)):
            return False, "âŒ æ—¶åºæ£€ç´¢å·²ç¦ç”¨ï¼ˆretrieval.temporal.enabled=falseï¼‰"

        args = self._parse_kv_args(content)
        query = args.get("q") or args.get("query") or ""
        time_from = args.get("from") or args.get("start")
        time_to = args.get("to") or args.get("end")
        person = args.get("person")
        source = args.get("source")

        if not time_from and not time_to:
            return False, "âŒ time æ¨¡å¼è‡³å°‘éœ€è¦ from/start æˆ– to/end å‚æ•°"

        top_k = int(self.get_config("retrieval.temporal.default_top_k", 10))
        if "top_k" in args:
            try:
                top_k = max(1, int(args["top_k"]))
            except ValueError:
                return False, "âŒ top_k å¿…é¡»æ˜¯æ•´æ•°"

        try:
            ts_from, ts_to = parse_query_time_range(time_from, time_to)
        except ValueError as e:
            return False, f"âŒ æ—¶é—´å‚æ•°é”™è¯¯: {e}"

        temporal = TemporalQueryOptions(
            time_from=ts_from,
            time_to=ts_to,
            person=person,
            source=source,
            allow_created_fallback=self.get_config(
                "retrieval.temporal.allow_created_fallback",
                True,
            ),
            candidate_multiplier=int(
                self.get_config("retrieval.temporal.candidate_multiplier", 8)
            ),
            max_scan=int(self.get_config("retrieval.temporal.max_scan", 1000)),
        )

        start_time = time.time()
        results = await self.retriever.retrieve(
            query=query,
            top_k=top_k,
            temporal=temporal,
        )

        # query éç©ºæ—¶å¯ä»¥åº”ç”¨é˜ˆå€¼ï¼›çº¯ time çª—å£æ‰«ææ—¶ä¸åšé˜ˆå€¼è¿‡æ»¤
        if query and self.threshold_filter:
            results = self.threshold_filter.filter(results)

        elapsed = time.time() - start_time
        if not results:
            return True, f"ğŸ•’ æœªæ‰¾åˆ°ç¬¦åˆæ—¶é—´æ¡ä»¶çš„å†…å®¹ï¼ˆè€—æ—¶: {elapsed*1000:.1f}msï¼‰"

        paragraphs = [r for r in results if r.result_type == "paragraph"]
        relations = [r for r in results if r.result_type == "relation"]

        lines = [
            f"ğŸ•’ æ—¶é—´æ£€ç´¢ç»“æœï¼ˆquery='{query or 'N/A'}'ï¼Œè€—æ—¶: {elapsed*1000:.1f}msï¼‰",
            "",
        ]

        if paragraphs:
            lines.append("ğŸ“„ åŒ¹é…æ®µè½ï¼š")
            for i, result in enumerate(paragraphs[:top_k], 1):
                score_pct = result.score * 100
                content_text = result.content[:80] + "..." if len(result.content) > 80 else result.content
                time_meta = result.metadata.get("time_meta", {})
                s_text = time_meta.get("effective_start_text", "N/A")
                e_text = time_meta.get("effective_end_text", "N/A")
                basis = time_meta.get("match_basis", "none")
                lines.append(f"  {i}. [{score_pct:.1f}%] {content_text}")
                lines.append(f"     â±ï¸ {s_text} ~ {e_text} ({basis})")
            lines.append("")

        if relations:
            lines.append("ğŸ”— åŒ¹é…å…³ç³»ï¼š")
            for i, result in enumerate(relations[:top_k], 1):
                score_pct = result.score * 100
                subject = result.metadata.get("subject", "")
                predicate = result.metadata.get("predicate", "")
                obj = result.metadata.get("object", "")
                time_meta = result.metadata.get("time_meta", {})
                s_text = time_meta.get("effective_start_text", "N/A")
                e_text = time_meta.get("effective_end_text", "N/A")
                basis = time_meta.get("match_basis", "none")
                lines.append(f"  {i}. [{score_pct:.1f}%] {subject} {predicate} {obj}")
                lines.append(f"     â±ï¸ {s_text} ~ {e_text} ({basis})")
            lines.append("")

        lines.append(f"ğŸ“Š å…± {len(results)} æ¡ç»“æœï¼ˆæ®µè½: {len(paragraphs)}, å…³ç³»: {len(relations)}ï¼‰")
        return True, "\n".join(lines)

    async def _query_entity(self, entity_name: str) -> Tuple[bool, str]:
        """æŸ¥è¯¢å®ä½“ä¿¡æ¯

        Args:
            entity_name: å®ä½“åç§°

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨
        if not self.graph_store.has_node(entity_name):
            return False, f"âŒ å®ä½“ä¸å­˜åœ¨: {entity_name}"

        # è·å–é‚»å±…èŠ‚ç‚¹
        neighbors = self.graph_store.get_neighbors(entity_name)

        if self.debug_enabled:
            logger.info(f"{self.log_prefix} [DEBUG] å®ä½“ '{entity_name}' é‚»å±…èŠ‚ç‚¹: {neighbors}")

        # è·å–ç›¸å…³æ®µè½
        paragraphs = self.metadata_store.get_paragraphs_by_entity(entity_name)

        # æ„å»ºå“åº”
        lines = [
            f"ğŸ·ï¸ å®ä½“ä¿¡æ¯: {entity_name}",
            "",
            f"ğŸ”— å…³è”å®ä½“ ({len(neighbors)}):",
        ]

        if neighbors:
            for neighbor in neighbors[:10]:
                lines.append(f"  - {neighbor}")
        else:
            lines.append("  (æ— )")

        lines.append("")
        lines.append(f"ğŸ“„ ç›¸å…³æ®µè½ ({len(paragraphs)}):")

        if paragraphs:
            for i, para in enumerate(paragraphs[:5], 1):
                content = para["content"][:80] + "..." if len(para["content"]) > 80 else para["content"]
                lines.append(f"  {i}. {content}")
        else:
            lines.append("  (æ— )")

        return True, "\n".join(lines)

    async def _query_relation(self, relation_spec: str) -> Tuple[bool, str]:
        """æŸ¥è¯¢å…³ç³»ä¿¡æ¯

        Args:
            relation_spec: å…³ç³»è§„æ ¼ (æ ¼å¼: subject|predicate|object æˆ– subject predicate)

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        # è§£æå…³ç³»è§„æ ¼
        if "|" in relation_spec:
            parts = relation_spec.split("|")
            if len(parts) < 2:
                return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject|predicate æˆ– subject|predicate|object"
            subject = parts[0].strip()
            predicate = parts[1].strip()
            obj = parts[2].strip() if len(parts) > 2 else None
        else:
            parts = relation_spec.split(maxsplit=1)
            if len(parts) < 2:
                return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject predicate"
            subject = parts[0].strip()
            predicate = parts[1].strip()
            obj = None

        # æŸ¥è¯¢å…³ç³»
        relations = self.metadata_store.get_relations(
            subject=subject if subject else None,
            predicate=predicate if predicate else None,
            object=obj if obj else None,
        )

        # æ„å»ºå“åº”
        lines = [
            f"ğŸ”— å…³ç³»æŸ¥è¯¢ç»“æœ",
            f"ğŸ“Œ è§„æ ¼: {subject} {predicate} {obj or '*' }",
            f"ğŸ“Š æ‰¾åˆ° {len(relations)} æ¡å…³ç³»",
            "",
        ]

        if relations:
            for i, rel in enumerate(relations[:10], 1):
                s = rel.get("subject", "")
                p = rel.get("predicate", "")
                o = rel.get("object", "")
                conf = rel.get("confidence", 1.0)
                lines.append(f"  {i}. {s} {p} {o} (ç½®ä¿¡åº¦: {conf:.2f})")
        else:
            lines.append("  (æ— åŒ¹é…ç»“æœ)")

        return True, "\n".join(lines)

    def _query_stats(self) -> Tuple[bool, str]:
        """æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»Ÿè®¡ä¿¡æ¯)
        """
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "vector_store": {
                "å‘é‡æ•°é‡": self.vector_store.num_vectors if self.vector_store else 0,
                "ç»´åº¦": self.vector_store.dimension if self.vector_store else 0,
            },
            "graph_store": {
                "èŠ‚ç‚¹æ•°": self.graph_store.num_nodes if self.graph_store else 0,
                "è¾¹æ•°": self.graph_store.num_edges if self.graph_store else 0,
            },
            "metadata_store": {
                "æ®µè½æ•°": self.metadata_store.count_paragraphs() if self.metadata_store else 0,
                "å…³ç³»æ•°": self.metadata_store.count_relations() if self.metadata_store else 0,
                "å®ä½“æ•°": self.metadata_store.count_entities() if self.metadata_store else 0,
            },
            "sparse": self.sparse_index.stats() if self.sparse_index else None,
        }
        
        # è·å–çŸ¥è¯†ç±»å‹åˆ†å¸ƒ
        type_distribution = {}
        if self.metadata_store:
            cursor = self.metadata_store._conn.cursor()
            cursor.execute("""
                SELECT knowledge_type, COUNT(*) as count
                FROM paragraphs
                GROUP BY knowledge_type
            """)
            for row in cursor.fetchall():
                type_name = row[0] if row[0] else "æœªåˆ†ç±»"
                count = row[1]
                type_distribution[type_name] = count

        # æ„å»ºå“åº”
        lines = [
            "ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯",
            "",
            "ğŸ“¦ å‘é‡å­˜å‚¨:",
            f"  - å‘é‡æ•°é‡: {stats['vector_store']['å‘é‡æ•°é‡']}",
            f"  - ç»´åº¦: {stats['vector_store']['ç»´åº¦']}",
            "",
            "ğŸ•¸ï¸ å›¾å­˜å‚¨:",
            f"  - èŠ‚ç‚¹æ•°: {stats['graph_store']['èŠ‚ç‚¹æ•°']}",
            f"  - è¾¹æ•°: {stats['graph_store']['è¾¹æ•°']}",
            "",
            "ğŸ“ å…ƒæ•°æ®å­˜å‚¨:",
            f"  - æ®µè½æ•°: {stats['metadata_store']['æ®µè½æ•°']}",
            f"  - å…³ç³»æ•°: {stats['metadata_store']['å…³ç³»æ•°']}",
            f"  - å®ä½“æ•°: {stats['metadata_store']['å®ä½“æ•°']}",
        ]

        sparse_stats = stats.get("sparse")
        if sparse_stats:
            lines.extend([
                "",
                "ğŸ§© ç¨€ç–æ£€ç´¢:",
                f"  - å¯ç”¨: {'æ˜¯' if sparse_stats.get('enabled') else 'å¦'}",
                f"  - å·²åŠ è½½: {'æ˜¯' if sparse_stats.get('loaded') else 'å¦'}",
                f"  - Tokenizer: {sparse_stats.get('tokenizer_mode', 'N/A')}",
                f"  - FTSæ–‡æ¡£æ•°: {sparse_stats.get('doc_count', 0)}",
            ])
        
        # æ·»åŠ ç±»å‹åˆ†å¸ƒ
        if type_distribution:
            lines.append("")
            lines.append("ğŸ·ï¸ çŸ¥è¯†ç±»å‹åˆ†å¸ƒ:")
            for type_name, count in sorted(type_distribution.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['metadata_store']['æ®µè½æ•°'] * 100) if stats['metadata_store']['æ®µè½æ•°'] > 0 else 0
                lines.append(f"  - {type_name}: {count} ({percentage:.1f}%)")

        return True, "\n".join(lines)

    def _get_help_message(self) -> str:
        """è·å–å¸®åŠ©æ¶ˆæ¯

        Returns:
            å¸®åŠ©æ¶ˆæ¯æ–‡æœ¬
        """
        return """ğŸ“– æŸ¥è¯¢å‘½ä»¤å¸®åŠ©

ç”¨æ³•:
  /query search <æŸ¥è¯¢æ–‡æœ¬>      - æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  /query time <k=vå‚æ•°>         - æ—¶é—´æ£€ç´¢ï¼ˆæ”¯æŒè¯­ä¹‰+æ—¶é—´ï¼‰
  /query entity <å®ä½“åç§°>      - æŸ¥è¯¢å®ä½“ä¿¡æ¯
  /query relation <å…³ç³»è§„æ ¼>    - æŸ¥è¯¢å…³ç³»ä¿¡æ¯
  /query stats                  - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  /query help                   - æ˜¾ç¤ºæ­¤å¸®åŠ©

å¿«æ·æ¨¡å¼:
  /query s <æŸ¥è¯¢æ–‡æœ¬>           - æ£€ç´¢ï¼ˆsearchçš„ç®€å†™ï¼‰
  /query t <k=vå‚æ•°>            - æ—¶é—´æ£€ç´¢ï¼ˆtimeçš„ç®€å†™ï¼‰
  /query e <å®ä½“åç§°>           - å®ä½“æŸ¥è¯¢ï¼ˆentityçš„ç®€å†™ï¼‰
  /query r <å…³ç³»è§„æ ¼>           - å…³ç³»æŸ¥è¯¢ï¼ˆrelationçš„ç®€å†™ï¼‰

ç¤ºä¾‹:
  /query search äººå·¥æ™ºèƒ½çš„åº”ç”¨
  /query time q="é¡¹ç›®è¿›å±•" from=2025/01/01 to="2025/01/31 18:30"
  /query entity Apple
  /query relation Apple|founded|Steve Jobs
  /query relation founded by
  /query stats

è¯´æ˜:
  - æ£€ç´¢æ¨¡å¼ä¼šåŒæ—¶æœç´¢æ®µè½å’Œå…³ç³»
  - time æ¨¡å¼å‚æ•°: q/query, from/start, to/end, person, source, top_k
  - time æ ¼å¼ä»…æ”¯æŒ YYYY/MM/DD æˆ– YYYY/MM/DD HH:mm
  - å®ä½“æŸ¥è¯¢æ˜¾ç¤ºå…³è”å®ä½“å’Œç›¸å…³æ®µè½
  - å…³ç³»æ ¼å¼æ”¯æŒ "|" æˆ–ç©ºæ ¼åˆ†éš”
  - ç»Ÿè®¡æ¨¡å¼æ˜¾ç¤ºçŸ¥è¯†åº“æ¦‚è§ˆ
"""
