"""
å¯¼å…¥çŸ¥è¯†Commandç»„ä»¶

æ”¯æŒä»æ–‡æœ¬ã€æ–‡ä»¶ç­‰æ¥æºå¯¼å…¥çŸ¥è¯†åˆ°çŸ¥è¯†åº“ã€‚
"""

import time
import re
import json
from typing import Tuple, Optional, List, Dict, Any
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.base.base_command import BaseCommand
from src.chat.message_receive.message import MessageRecv

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.plugin_system.apis import llm_api
from src.config.config import model_config as host_model_config
from ...core import (
    VectorStore,
    GraphStore,
    MetadataStore,
    EmbeddingAPIAdapter,
    KnowledgeType,
    detect_knowledge_type,
    should_extract_relations,
    get_type_display_name,
)

logger = get_logger("A_Memorix.ImportCommand")


class ImportCommand(BaseCommand):
    """å¯¼å…¥çŸ¥è¯†Command

    åŠŸèƒ½ï¼š
    - ä»æ–‡æœ¬å¯¼å…¥æ®µè½
    - ä»æ–‡æœ¬æå–å®ä½“å’Œå…³ç³»
    - è‡ªåŠ¨ç”ŸæˆåµŒå…¥å‘é‡
    - æ‰¹é‡å¯¼å…¥æ”¯æŒ
    """

    # CommandåŸºæœ¬ä¿¡æ¯
    command_name = "import"
    command_description = "å¯¼å…¥çŸ¥è¯†åˆ°çŸ¥è¯†åº“ï¼Œæ”¯æŒæ–‡æœ¬ã€æ®µè½ã€å®ä½“å’Œå…³ç³»"
    # CommandåŸºæœ¬ä¿¡æ¯
    command_name = "import"
    command_description = "å¯¼å…¥çŸ¥è¯†åˆ°çŸ¥è¯†åº“ï¼Œæ”¯æŒæ–‡æœ¬ã€æ®µè½ã€å®ä½“å’Œå…³ç³»"
    # ä½¿ç”¨ä¸¥æ ¼çš„æ¨¡å¼åŒ¹é…ï¼Œé¿å…å°†å†…å®¹è¯¯è¯†åˆ«ä¸ºæœªçŸ¥çš„æ¨¡å¼
    command_pattern = r"^\/import(?:\s+(?P<mode>text|paragraph|relation|file|json))?(?:\s+(?P<content>.+))?$"

    def __init__(self, message: MessageRecv, plugin_config: Optional[dict] = None):
        """åˆå§‹åŒ–å¯¼å…¥Command"""
        super().__init__(message, plugin_config)

        # è·å–å­˜å‚¨å®ä¾‹ (ä¼˜å…ˆä»é…ç½®è·å–ï¼Œå…œåº•ä»æ’ä»¶å®ä¾‹è·å–)
        self.vector_store: Optional[VectorStore] = self.plugin_config.get("vector_store")
        self.graph_store: Optional[GraphStore] = self.plugin_config.get("graph_store")
        self.metadata_store: Optional[MetadataStore] = self.plugin_config.get("metadata_store")
        self.embedding_manager: Optional[EmbeddingAPIAdapter] = self.plugin_config.get("embedding_manager")

        # å…œåº•é€»è¾‘ï¼šå¦‚æœé…ç½®ä¸­æ²¡æœ‰å­˜å‚¨å®ä¾‹ï¼Œå°è¯•ç›´æ¥ä»æ’ä»¶ç³»ç»Ÿè·å–
        # ä½¿ç”¨ is not None æ£€æŸ¥ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½å¸ƒå°”å€¼ä¸º False
        if not all([
            self.vector_store is not None,
            self.graph_store is not None,
            self.metadata_store is not None,
            self.embedding_manager is not None
        ]):
            from ...plugin import A_MemorixPlugin
            instances = A_MemorixPlugin.get_storage_instances()
            if instances:
                self.vector_store = self.vector_store or instances.get("vector_store")
                self.graph_store = self.graph_store or instances.get("graph_store")
                self.metadata_store = self.metadata_store or instances.get("metadata_store")
                self.embedding_manager = self.embedding_manager or instances.get("embedding_manager")

        # è®¾ç½®æ—¥å¿—å‰ç¼€
        if self.message and self.message.chat_stream:
            self.log_prefix = f"[ImportCommand-{self.message.chat_stream.stream_id}]"
        else:
            self.log_prefix = "[ImportCommand]"

    @property
    def debug_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è°ƒè¯•æ¨¡å¼"""
        # å°è¯•ä» plugin_config è·å– advanced.debug
        advanced = self.plugin_config.get("advanced", {})
        if isinstance(advanced, dict):
            return advanced.get("debug", False)
        # å…œåº•ï¼šç›´æ¥æ£€æŸ¥ debug å­—æ®µ
        return self.plugin_config.get("debug", False)

    async def execute(self) -> Tuple[bool, Optional[str], int]:
        """æ‰§è¡Œå¯¼å…¥å‘½ä»¤

        Returns:
            Tuple[bool, Optional[str], int]: (æ˜¯å¦æˆåŠŸ, å›å¤æ¶ˆæ¯, æ‹¦æˆªçº§åˆ«)
        """
        # æ£€æŸ¥å­˜å‚¨æ˜¯å¦åˆå§‹åŒ– (ä½¿ç”¨ is not None è€Œéå¸ƒå°”å€¼ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½ä¸º False)
        if not all([
            self.vector_store is not None,
            self.graph_store is not None,
            self.metadata_store is not None,
            self.embedding_manager is not None
        ]):
            error_msg = "âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¯¼å…¥"
            logger.error(f"{self.log_prefix} {error_msg}")
            return False, error_msg, 0

        # è·å–åŒ¹é…çš„å‚æ•°: å¦‚æœ mode æœªæ•è·(None)ï¼Œåˆ™é»˜è®¤ä¸º "text"
        mode = self.matched_groups.get("mode") or "text"
        content = self.matched_groups.get("content", "")

        if not content:
            help_msg = self._get_help_message()
            return True, help_msg, 0

        logger.info(f"{self.log_prefix} æ‰§è¡Œå¯¼å…¥: mode={mode}, content_length={len(content)}")
        if self.debug_enabled:
            logger.info(f"{self.log_prefix} [DEBUG] å¯¼å…¥å†…å®¹é¢„è§ˆ: {content[:200]}...")

        try:
            # æ ¹æ®æ¨¡å¼æ‰§è¡Œå¯¼å…¥
            if mode == "text":
                success, result = await self._import_text(content)
            elif mode == "paragraph":
                success, result = await self._import_paragraph(content)
            elif mode == "relation":
                success, result = await self._import_relation(content)
            elif mode == "file":
                success, result = await self._import_from_file(content)
            elif mode == "json":
                success, result = await self._import_json(content)
            else:
                success, result = False, f"âŒ æœªçŸ¥çš„å¯¼å…¥æ¨¡å¼: {mode}"

            # æŒä¹…åŒ–ä¿å­˜
            if success:
                try:
                    self.vector_store.save()
                    self.graph_store.save()
                    logger.info(f"{self.log_prefix} æ•°æ®å·²æŒä¹…åŒ–ä¿å­˜")
                except Exception as e:
                    logger.error(f"{self.log_prefix} æ•°æ®æŒä¹…åŒ–å¤±è´¥: {e}")

            return success, result, 0

        except Exception as e:
            error_msg = f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}"
            logger.error(f"{self.log_prefix} {error_msg}")
            return False, error_msg, 0

    async def _import_text(self, text: str) -> Tuple[bool, str]:
        """å¯¼å…¥æ–‡æœ¬ï¼ˆè‡ªåŠ¨åˆ†æ®µå’Œæå–ï¼‰

        Args:
            text: å¾…å¯¼å…¥çš„æ–‡æœ¬

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        start_time = time.time()

        # åˆ†æ®µå¤„ç†
        paragraphs = self._split_text(text)

        if not paragraphs:
            return False, "âŒ æœªèƒ½ä»æ–‡æœ¬ä¸­æå–æœ‰æ•ˆæ®µè½"

        logger.info(f"{self.log_prefix} æ–‡æœ¬åˆ†æ®µ: {len(paragraphs)}ä¸ªæ®µè½")

        # å°è¯•é€‰æ‹© LLM æ¨¡å‹
        try:
            model_config_to_use = await self._select_model()
            use_llm = True
        except Exception as e:
            logger.warning(f"{self.log_prefix} æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹æˆ–é€‰æ‹©å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°åŸºç¡€æ¨¡å¼")
            use_llm = False
            model_config_to_use = None

        success_count = 0
        entities_count = 0
        relations_count = 0
        type_counts = {}

        for paragraph in paragraphs:
            # 1. å°è¯• LLM æå–
            llm_result = {}
            if use_llm:
                try:
                    llm_result = await self._llm_extract(paragraph, model_config_to_use)
                except Exception as e:
                    logger.warning(f"{self.log_prefix} LLM æå–å¤±è´¥: {e}")

            # 2. å¯¼å…¥æ®µè½
            try:
                hash_value, detected_type = await self._add_paragraph(paragraph)
                success_count += 1
                
                type_name = detected_type.value
                type_counts[type_name] = type_counts.get(type_name, 0) + 1
            except Exception as e:
                logger.warning(f"{self.log_prefix} æ®µè½å¯¼å…¥å¤±è´¥: {e}")
                continue

            # 3. å¯¼å…¥ LLM æå–çš„å®ä½“
            if llm_result.get("entities"):
                extracted_entities = llm_result["entities"]
                if extracted_entities:
                    for entity in extracted_entities:
                        # ä¼ é€’ source_paragraph ä»¥å»ºç«‹å…³è”
                        await self._add_entity_with_vector(entity, source_paragraph=hash_value)
                    entities_count += len(extracted_entities)
            
            # 4. å¯¼å…¥ LLM æå–çš„å…³ç³»
            if llm_result.get("relations"):
                for rel in llm_result["relations"]:
                    s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
                    if all([s, p, o]):
                        try:
                            await self._add_relation(s, p, o, source_paragraph=hash_value)
                            relations_count += 1
                        except Exception as e:
                            logger.debug(f"{self.log_prefix} å…³ç³»æ·»åŠ å¤±è´¥: {e}")

            # 5. å›é€€é€»è¾‘ï¼šå¦‚æœ LLM ä¸ºç©ºä¸”ç±»å‹é€‚åˆï¼Œå°è¯•æ­£åˆ™
            if not llm_result and should_extract_relations(detected_type):
                e_c, r_c = await self._extract_knowledge_regex([paragraph], source_hash=hash_value)
                entities_count += e_c
                relations_count += r_c


        elapsed = time.time() - start_time

        # æ„å»ºç»“æœæ¶ˆæ¯
        result_lines = [
            "âœ… æ–‡æœ¬å¯¼å…¥å®Œæˆ (æ™ºèƒ½å¢å¼º)",
            f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:",
            f"  - æ®µè½: {success_count}/{len(paragraphs)}",
        ]
        
        if type_counts:
            result_lines.append(f"  - ç±»å‹åˆ†å¸ƒ:")
            for type_name, count in type_counts.items():
                result_lines.append(f"    â€¢ {type_name}: {count}")
        
        result_lines.extend([
            f"  - å®ä½“: {entities_count}",
            f"  - å…³ç³»: {relations_count}",
            f"â±ï¸ è€—æ—¶: {elapsed:.2f}ç§’",
        ])

        return True, "\n".join(result_lines)

    async def _import_paragraph(self, content: str) -> Tuple[bool, str]:
        """å¯¼å…¥å•ä¸ªæ®µè½

        Args:
            content: æ®µè½å†…å®¹

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            hash_value, detected_type = await self._add_paragraph(content)

            result_lines = [
                "âœ… æ®µè½å¯¼å…¥å®Œæˆ",
                f"ğŸ“ Hash: {hash_value[:16]}...",
                f"ğŸ·ï¸ ç±»å‹: {get_type_display_name(detected_type)}",
                f"ğŸ“„ å†…å®¹: {content[:50]}...",
            ]

            return True, "\n".join(result_lines)

        except Exception as e:
            return False, f"âŒ æ®µè½å¯¼å…¥å¤±è´¥: {str(e)}"

    async def _import_relation(self, content: str) -> Tuple[bool, str]:
        """å¯¼å…¥å…³ç³»

        æ ¼å¼: subject|predicate|object æˆ– subject predicate object

        Args:
            content: å…³ç³»å†…å®¹

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            # è§£æå…³ç³»
            if "|" in content:
                parts = content.split("|")
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject|predicate|object"
                subject, predicate, obj = parts
            else:
                # å°è¯•ç©ºæ ¼åˆ†éš”
                parts = content.split(maxsplit=2)
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject|predicate|object"
                subject, predicate, obj = parts

            # å»é™¤ç©ºç™½
            subject = subject.strip()
            predicate = predicate.strip()
            obj = obj.strip()

            if not all([subject, predicate, obj]):
                return False, "âŒ å…³ç³»å­—æ®µä¸èƒ½ä¸ºç©º"

            # æ·»åŠ å…³ç³»
            hash_value = await self._add_relation(subject, predicate, obj)

            result_lines = [
                "âœ… å…³ç³»å¯¼å…¥å®Œæˆ",
                f"ğŸ”— Hash: {hash_value[:16]}...",
                f"ğŸ“Œ {subject} {predicate} {obj}",
            ]

            return True, "\n".join(result_lines)

        except Exception as e:
            return False, f"âŒ å…³ç³»å¯¼å…¥å¤±è´¥: {str(e)}"

    async def _import_json(self, file_path: str) -> Tuple[bool, str]:
        """ä»JSONæ–‡ä»¶å¯¼å…¥çŸ¥è¯†

        JSONæ ¼å¼åº”ä¸º:
        {
            "paragraphs": ["æ®µè½1", "æ®µè½2"],
            "relations": [{"subject": "s", "predicate": "p", "object": "o"}],
            "entities": ["e1", "e2"]
        }

        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            path = Path(file_path)
            if not path.exists():
                return False, f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            p_count = 0
            r_count = 0
            e_count = 0
            
            # å¯¼å…¥æ®µè½
            paragraphs = data.get("paragraphs", [])
            for p in paragraphs:
                if isinstance(p, str):
                    await self._add_paragraph(p)
                    p_count += 1
                elif isinstance(p, dict) and "content" in p:
                    await self._add_paragraph(p["content"])
                    p_count += 1
            
            # å¯¼å…¥å®ä½“
            entities = data.get("entities", [])
            if entities:
                for entity in entities:
                    await self._add_entity_with_vector(entity)
                e_count += len(entities)
            
            # å¯¼å…¥å…³ç³»
            relations = data.get("relations", [])
            for r in relations:
                s = r.get("subject")
                p = r.get("predicate")
                o = r.get("object")
                if all([s, p, o]):
                    await self._add_relation(s, p, o)
                    r_count += 1
            
            result_lines = [
                "âœ… JSONå¯¼å…¥å®Œæˆ",
                f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:",
                f"  - æ®µè½: {p_count}",
                f"  - å®ä½“: {e_count}",
                f"  - å…³ç³»: {r_count}",
            ]
            
            return True, "\n".join(result_lines)
            
        except json.JSONDecodeError:
            return False, "âŒ JSONæ ¼å¼é”™è¯¯"
        except Exception as e:
            return False, f"âŒ JSONå¯¼å…¥å¤±è´¥: {str(e)}"

    async def _import_from_file(self, file_path: str) -> Tuple[bool, str]:
        """ä»æ–‡ä»¶å¯¼å…¥çŸ¥è¯†

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        try:
            path = Path(file_path)

            if not path.exists():
                return False, f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"

            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¯¼å…¥æ–¹å¼
            if path.suffix.lower() in [".txt", ".md"]:
                # è¯»å–æ–‡ä»¶
                with open(path, "r", encoding="utf-8") as f:
                    content = f.read()
                return await self._import_text(content)
            elif path.suffix.lower() == ".json":
                return await self._import_json(str(path))
            else:
                return False, f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {path.suffix}"

        except Exception as e:
            return False, f"âŒ æ–‡ä»¶å¯¼å…¥å¤±è´¥: {str(e)}"

    async def _add_paragraph(
        self,
        content: str,
        knowledge_type: Optional[KnowledgeType] = None,
    ) -> Tuple[str, KnowledgeType]:
        """æ·»åŠ æ®µè½åˆ°çŸ¥è¯†åº“

        Args:
            content: æ®µè½å†…å®¹
            knowledge_type: çŸ¥è¯†ç±»å‹ï¼ˆå¯é€‰ï¼ŒNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰

        Returns:
            å…ƒç»„ï¼š(æ®µè½hashå€¼, æ£€æµ‹åˆ°çš„çŸ¥è¯†ç±»å‹)
        """
        # è‡ªåŠ¨æ£€æµ‹çŸ¥è¯†ç±»å‹
        if knowledge_type is None or knowledge_type == KnowledgeType.AUTO:
            knowledge_type = detect_knowledge_type(content)
        
        if self.debug_enabled:
            logger.info(
                f"{self.log_prefix} [DEBUG] æ®µè½ç±»å‹æ£€æµ‹: {get_type_display_name(knowledge_type)}"
            )
        
        # æ·»åŠ åˆ°metadata store
        hash_value = self.metadata_store.add_paragraph(
            content=content,
            source="import_command",
            knowledge_type=knowledge_type.value,
        )

        # ç”ŸæˆåµŒå…¥å‘é‡ (å¼‚æ­¥è°ƒç”¨)
        embedding = await self.embedding_manager.encode(content)

        # æ·»åŠ åˆ°vector store
        self.vector_store.add(
            vectors=embedding.reshape(1, -1),
            ids=[hash_value],
        )

        logger.debug(f"{self.log_prefix} æ·»åŠ æ®µè½: hash={hash_value[:16]}..., type={knowledge_type.value}")
        if self.debug_enabled:
            logger.info(f"{self.log_prefix} [DEBUG] å‘é‡ç”ŸæˆæˆåŠŸ: shape={embedding.shape}, dtype={embedding.dtype}")
            logger.info(f"{self.log_prefix} [DEBUG] æ®µè½å…ƒæ•°æ®å·²å†™å…¥: hash={hash_value}")

        return hash_value, knowledge_type

    async def _add_relation(
        self,
        subject: str,
        predicate: str,
        obj: str,
        confidence: float = 1.0,
        source_paragraph: str = "",
    ) -> str:
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†åº“

        Args:
            subject: ä¸»ä½“
            predicate: è°“è¯
            obj: å®¢ä½“
            confidence: ç½®ä¿¡åº¦
            source_paragraph: æºæ®µè½

        Returns:
            å…³ç³»hashå€¼
        """
        # æ·»åŠ å®ä½“åˆ°å›¾ (å¹¶å‘é‡åŒ–)
        await self._add_entity_with_vector(subject)
        await self._add_entity_with_vector(obj)

        # æ·»åŠ å…³ç³»åˆ°metadata store
        hash_value = self.metadata_store.add_relation(
            subject=subject,
            predicate=predicate,
            obj=obj,  # å‚æ•°åæ˜¯ obj è€Œä¸æ˜¯ object
            confidence=confidence,
            source_paragraph=source_paragraph, # è¿™é‡Œåº”è¯¥æ˜¯ hash
        )

        # æ·»åŠ å…³ç³»åˆ°å›¾
        self.graph_store.add_edges([(subject, obj)])

        logger.debug(
            f"{self.log_prefix} æ·»åŠ å…³ç³»: {subject} {predicate} {obj}, "
            f"hash={hash_value[:16]}..."
        )

        return hash_value

    def _split_text(self, text: str, max_length: int = 500) -> List[str]:
        """å°†æ–‡æœ¬åˆ†æ®µ

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§æ®µè½é•¿åº¦

        Returns:
            æ®µè½åˆ—è¡¨
        """
        # æŒ‰æ®µè½åˆ†æ®µ
        paragraphs = text.split("\n\n")

        result = []
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # å¦‚æœæ®µè½è¿‡é•¿ï¼ŒæŒ‰å¥å­ç»§ç»­åˆ†æ®µ
            if len(para) > max_length:
                sentences = re.split(r"[ã€‚ï¼ï¼Ÿ.!?]", para)
                current_chunk = ""

                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue

                    if len(current_chunk) + len(sentence) < max_length:
                        current_chunk += sentence + "ã€‚"
                    else:
                        if current_chunk:
                            result.append(current_chunk.strip())
                        current_chunk = sentence + "ã€‚"

                if current_chunk:
                    result.append(current_chunk.strip())
            else:
                result.append(para)

        return result

    async def _extract_knowledge(
        self,
        paragraphs: List[str],
    ) -> Tuple[int, int]:
        """ä»æ®µè½ä¸­æå–å®ä½“å’Œå…³ç³»ï¼ˆç®€åŒ–å®ç°ï¼‰

        Args:
            paragraphs: æ®µè½åˆ—è¡¨

        Returns:
            Tuple[int, int]: (å®ä½“æ•°é‡, å…³ç³»æ•°é‡)
        """
        entities_count = 0
        relations_count = 0

        # è·å–å¯ç”¨æ¨¡å‹
        models = llm_api.get_available_models()
        if not models:
            logger.warning(f"{self.log_prefix} æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œé€€å›åˆ°æ­£åˆ™æå–")
            return await self._extract_knowledge_regex(paragraphs)
        
        # ä¼˜å…ˆé€‰æ‹© balanced æˆ– performance æ¨¡å‹ï¼Œå¦åˆ™é€‰ç¬¬ä¸€ä¸ª
        model_name = "balanced" if "balanced" in models else list(models.keys())[0]
        model_config = models[model_name]

        for para in paragraphs:
            if len(para.strip()) < 10:
                continue

            if self.debug_enabled:
                logger.info(f"{self.log_prefix} [DEBUG] æ­£åœ¨é€šè¿‡ LLM æå–çŸ¥è¯†ï¼Œæ®µè½é•¿åº¦: {len(para)}")

            prompt = f"""è¯·ä»ä»¥ä¸‹æ®µè½ä¸­æå–å®ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚
ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "entities": ["å®ä½“1", "å®ä½“2"],
  "relations": [
    {{"subject": "ä¸»ä½“", "predicate": "å…³ç³»", "object": "å®¢ä½“"}}
  ]
}}

æ®µè½å†…å®¹ï¼š
{para}
"""
            success, response, _, _ = await llm_api.generate_with_model(
                prompt=prompt,
                model_config=model_config,
                request_type="A_Memorix.KnowledgeExtraction"
            )

            if success:
                try:
                    # æå– JSON éƒ¨åˆ†
                    json_match = re.search(r"\{.*\}", response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        if self.debug_enabled:
                            logger.info(f"{self.log_prefix} [DEBUG] LLM æå–ç»“æœ JSON: {json_str}")
                        data = json.loads(json_str)
                        
                        # æ·»åŠ å®ä½“
                        entities = data.get("entities", [])
                        if entities:
                            for entity in entities:
                                # è¿™é‡Œçš„ para æ˜¯ contentï¼Œæˆ‘ä»¬å…¶å®åº”è¯¥ä¼  hashï¼Œä½† _extract_knowledge æ¥å£åªæ¥æ”¶ paragraphs list
                                # ç”±äº _extract_knowledge çš„è®¾è®¡é—®é¢˜ï¼Œå®ƒæ²¡æœ‰å¾ˆå¥½çš„ä¸Šä¸‹æ–‡ hashã€‚
                                # ä½†æ³¨æ„åˆ°è¯¥æ–¹æ³•ä¸»è¦ç”¨äºæµ‹è¯•æˆ–ç®€å•è°ƒç”¨ï¼Œä¸»æµç¨‹æ˜¯ _import_textï¼Œé‚£é‡Œæ˜¯åˆ†å¼€å¤„ç†çš„ã€‚
                                # _import_text è°ƒç”¨çš„æ˜¯ _llm_extract è¿”å›æ•°æ®ï¼Œç„¶åè‡ªå·±åœ¨å¤–é¢å¾ªç¯æ·»åŠ ã€‚
                                # è¿™ä¸ª _extract_knowledge æ–¹æ³•ä¼¼ä¹æ˜¯ç‹¬ç«‹çš„è¾…åŠ©æ–¹æ³•ï¼Ÿ
                                # çœ‹èµ·æ¥ _import_text å¹¶æ²¡æœ‰ç›´æ¥è°ƒç”¨ _extract_knowledgeï¼Œè€Œæ˜¯è°ƒç”¨çš„ _llm_extract å’Œ _add_paragraph åˆ†å¼€å¤„ç†ã€‚
                                # åªæœ‰å½“ ImportCommand è¢«å¤–éƒ¨è°ƒç”¨ç”¨æ¥ "åªæå–ä¸å­˜æ®µè½" æ—¶æ‰ä¼šç”¨åˆ°è¿™ä¸ªï¼Ÿ
                                # æˆ–è€… _extract_knowledge_regex è¢«ç”¨åˆ°äº†ã€‚
                                # ç»è¿‡æ£€æŸ¥ï¼Œ_import_text åœ¨ 228è¡Œè°ƒç”¨äº† _extract_knowledge_regexã€‚
                                # ä½†è¯¥æ–¹æ³•æ²¡æœ‰è¢« _import_text è°ƒç”¨ã€‚
                                # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œè¿˜æ˜¯åŠ ä¸Š source_paragraph=para (è™½ç„¶è¿™å…¶å®æ˜¯ content ä¸æ˜¯ hashï¼Œå¯èƒ½å¯¼è‡´å¤–é”®é”™è¯¯)
                                # ç­‰ç­‰ï¼Œmetadata_store.add_entity çš„ source_paragraph å‚æ•°æœŸæœ›çš„æ˜¯ hashã€‚
                                # å¦‚æœä¼ å…¥ contentï¼Œä¼šè¿åå¤–é”®çº¦æŸ (å¦‚æœæœ‰çš„è¯) æˆ–è€…å­˜å…¥æ— æ•ˆ hashã€‚
                                # é‰´äº _extract_knowledge ä¸åœ¨ä¸»æµç¨‹ _import_text ä¸­ä½¿ç”¨ (å®ƒæ˜¯åˆ†å¼€çš„)ï¼Œ
                                # ä¸”å®ƒç”šè‡³æ²¡æœ‰ parameter hash çš„ä¸Šä¸‹æ–‡ã€‚
                                # æˆ‘ä»¬å…ˆç•™ç©ºï¼Œæˆ–è€…ä¼ å…¥ç©ºå­—ç¬¦ä¸²ã€‚
                                await self._add_entity_with_vector(entity)
                            entities_count += len(entities)

                        # æ·»åŠ å…³ç³»
                        relations = data.get("relations", [])
                        for rel in relations:
                            s, p, o = rel.get("subject"), rel.get("predicate"), rel.get("object")
                            if all([s, p, o]):
                                await self._add_relation(
                                    subject=s,
                                    predicate=p,
                                    obj=o,
                                    source_paragraph=para,
                                )
                                relations_count += 1
                        continue # æˆåŠŸåˆ™è·³è¿‡æ­£åˆ™
                except Exception as e:
                    logger.debug(f"{self.log_prefix} LLM ç»“æœè§£æå¤±è´¥: {e}")

            # å¦‚æœ LLM å¤±è´¥æˆ–æ— æ•ˆï¼Œé€€å›åˆ°æ­£åˆ™
            e_c, r_c = await self._extract_knowledge_regex([para])
            entities_count += e_c
            relations_count += r_c

        return entities_count, relations_count

    async def _select_model(self) -> Any:
        """ç²¾ç¡®é€‰æ‹©æœ€é€‚åˆçŸ¥è¯†æŠ½å–çš„æ¨¡å‹ (ä»…é™æ˜ç¡®é…ç½®å’Œä»»åŠ¡åŒ¹é…)"""
        models = llm_api.get_available_models()
        if not models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹é…ç½®")

        # 1. ä¼˜å…ˆçº§æœ€é«˜ï¼šæ’ä»¶é…ç½®å¼ºåˆ¶æŒ‡å®š
        config_model = self.plugin_config.get("advanced", {}).get("extraction_model", "auto")
        if config_model != "auto" and config_model in models:
            logger.info(f"{self.log_prefix} ä½¿ç”¨æ’ä»¶é…ç½®æŒ‡å®šçš„æ¨¡å‹: {config_model}")
            return models[config_model]

        # 2. ä¼˜å…ˆçº§ç¬¬äºŒï¼šä¸»ç¨‹åºä»»åŠ¡é…ç½®åŒ¹é… (lpmm_entity_extract)
        try:
            task_configs = getattr(host_model_config, "model_task_config", {})
            
            # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸¤ç§ç›¸å…³çš„ä»»åŠ¡é…ç½®
            for task_key in ["lpmm_entity_extract", "lpmm_rdf_build"]:
                if task_key in task_configs:
                    task_models = task_configs[task_key].get("model_list", [])
                    for m in task_models:
                        if m in models:
                            logger.info(f"{self.log_prefix} é€šè¿‡ä¸»ç¨‹åºä»»åŠ¡é…ç½® [{task_key}] åŒ¹é…åˆ°æ¨¡å‹: {m}")
                            return models[m]
        except Exception as e:
            logger.debug(f"{self.log_prefix} è¯»å–ä¸»ç¨‹åºä»»åŠ¡é…ç½®å¤±è´¥: {e}")

        # 3. å…œåº•ç­–ç•¥ï¼šå¦‚æœä»¥ä¸Šå‡æœªåŒ¹é…ï¼Œè¿”å›é¦–ä¸ªå¯ç”¨æ¨¡å‹
        first_model = list(models.keys())[0]
        return models[first_model]

    async def _llm_extract(self, chunk: str, model_config: Any) -> Dict:
        """è°ƒç”¨ LLM æå–çŸ¥è¯†"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„å®ä½“ï¼ˆEntitiesï¼‰å’Œå…³ç³»ï¼ˆRelationsï¼‰ã€‚
ä»…æå–å…³é”®ä¿¡æ¯ã€‚
JSONæ ¼å¼: {{ "entities": ["e1"], "relations": [{{"subject": "s", "predicate": "p", "object": "o"}}] }}
æ–‡æœ¬:
{chunk[:2000]}
"""
        success, response, _, _ = await llm_api.generate_with_model(
            prompt=prompt,
            model_config=model_config,
            request_type="A_Memorix.KnowledgeExtraction"
        )
        if success:
            try:
                # ç®€å•æ¸…ç†
                txt = response.strip()
                if "```" in txt:
                    txt = txt.split("```json")[-1].split("```")[0].strip()
                    if txt.startswith("json"): txt = txt[4:].strip()
                return json.loads(txt)
            except:
                pass
        return {}

    async def _extract_knowledge_regex(self, paragraphs: List[str], source_hash: Optional[str] = None) -> Tuple[int, int]:
        """ä½¿ç”¨æ­£åˆ™æå–çŸ¥è¯†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        entities_count = 0
        relations_count = 0
        for para in paragraphs:
            # ç®€å•æå–: å¤§å†™å•è¯ æˆ– å¼•å·å†…å®¹
            # ä½¿ç”¨éæ•è·ç»„æˆ–åˆ†æ­¥æå–ä»¥é¿å… findall çš„ç©ºå…ƒç»„é—®é¢˜
            entities = re.findall(r"[A-Z][a-z]+", para)
            quoted = re.findall(r"[\"']([^\"']+)[\"']", para)
            entities.extend(quoted)
            
            unique_entities = list(set([e for e in entities if e.strip()]))
            if unique_entities:
                for entity in unique_entities:
                    # ä¼ é€’ source_hash
                    await self._add_entity_with_vector(entity, source_paragraph=source_hash or "")
                entities_count += len(unique_entities)
            relations = re.findall(r"([A-Z][a-z]+)\s+(is|was|are|were)\s+([A-Z][a-z]+)", para)
            for subject, predicate, obj in relations:
                try:
                    await self._add_relation(subject, predicate, obj, source_paragraph=source_hash or "")
                    relations_count += 1
                except:
                    pass
        return entities_count, relations_count

    async def _add_entity_with_vector(
        self,
        name: str,
        source_paragraph: str = "",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """æ·»åŠ å®ä½“å¹¶åœ¨å‘é‡åº“ä¸­ç”Ÿæˆç´¢å¼•
        
        Args:
            name: å®ä½“åç§°
            source_paragraph: æ¥æºæ®µè½å“ˆå¸Œ (å¯é€‰)
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            å®ä½“hashå€¼
        """
        # 1. å­˜å…¥å…ƒæ•°æ®å’Œå›¾å­˜å‚¨
        hash_value = self.metadata_store.add_entity(
            name, 
            source_paragraph=source_paragraph,
            metadata=metadata
        )
        self.graph_store.add_nodes([name])

        # 2. ç”Ÿæˆå‘é‡å¹¶å­˜å…¥å‘é‡åº“
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºå‘é‡åº“ (é€šè¿‡ get æ£€æŸ¥æœ‰æ•ˆæ€§)
            existing_vectors = self.vector_store.get([hash_value])
            if not existing_vectors or existing_vectors[0] is None:
                embedding = await self.embedding_manager.encode(name)
                # å°è¯•æ·»åŠ ã€‚å¦‚æœIDå·²å­˜åœ¨ï¼ˆä¾‹å¦‚è¢«æ ‡è®°åˆ é™¤ï¼‰ï¼Œaddä¼šæŠ›å‡ºValueError
                try:
                    self.vector_store.add(
                        vectors=embedding.reshape(1, -1),
                        ids=[hash_value],
                    )
                    logger.debug(f"{self.log_prefix} Added vector for entity: {name}")
                except ValueError:
                    # IDå­˜åœ¨ä½†addå¤±è´¥ï¼Œå¯èƒ½æ˜¯è¢«è½¯åˆ é™¤äº†ï¼Œæˆ–è€…å¹¶å‘å¯¼è‡´
                    # æš‚æ—¶å¿½ç•¥ï¼Œé¿å…å´©æºƒ
                    logger.warning(f"{self.log_prefix} Entity vector {name} (hash={hash_value}) already exists or conflict.")
        except Exception as e:
            logger.warning(f"{self.log_prefix} Failed to vectorize entity {name}: {e}")

        return hash_value

    def _get_help_message(self) -> str:
        """è·å–å¸®åŠ©æ¶ˆæ¯

        Returns:
            å¸®åŠ©æ¶ˆæ¯æ–‡æœ¬
        """
        return """ğŸ“– å¯¼å…¥å‘½ä»¤å¸®åŠ©

ç”¨æ³•:
  /import text <æ–‡æœ¬å†…å®¹>        - å¯¼å…¥æ–‡æœ¬ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰
  /import paragraph <æ®µè½å†…å®¹>   - å¯¼å…¥å•ä¸ªæ®µè½
  /import relation <å…³ç³»>        - å¯¼å…¥å…³ç³» (æ ¼å¼: subject|predicate|object)
  /import file <æ–‡ä»¶è·¯å¾„>        - ä»æ–‡ä»¶å¯¼å…¥ (.txt, .md, .json)
  /import json <æ–‡ä»¶è·¯å¾„>        - ä»JSONæ–‡ä»¶å¯¼å…¥ (ç»“æ„åŒ–æ•°æ®)

ç¤ºä¾‹:
  /import text äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯...
  /import paragraph æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸ
  /import relation Apple|founded|Steve Jobs
  /import file ./data/knowledge.txt
  /import json ./data/knowledge.json

æç¤º:
  - æ–‡æœ¬æ¨¡å¼ä¼šè‡ªåŠ¨åˆ†æ®µå¹¶æå–å®ä½“å…³ç³»
  - å…³ç³»æ ¼å¼æ”¯æŒ "|" æˆ–ç©ºæ ¼åˆ†éš”
  - æ”¯æŒçš„æ–‡ä»¶ç±»å‹: .txt, .md, .json
"""
