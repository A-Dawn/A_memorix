"""
åˆ é™¤çŸ¥è¯† Command ç»„ä»¶

æä¾›çŸ¥è¯†åº“åˆ é™¤åŠŸèƒ½ï¼Œæ”¯æŒæ®µè½ã€å®ä½“å’Œå…³ç³»çš„åˆ é™¤ã€‚
"""

import re
import time
from typing import Tuple, Optional, List, Dict, Any

from src.common.logger import get_logger
from src.plugin_system.base.base_command import BaseCommand
from src.chat.message_receive.message import MessageRecv

from ...core import VectorStore, GraphStore, MetadataStore
from ...core.utils.hash import compute_hash, normalize_text

logger = get_logger("A_Memorix.DeleteCommand")


class DeleteCommand(BaseCommand):
    """åˆ é™¤çŸ¥è¯† Command"""

    command_name = "delete"
    command_description = "åˆ é™¤çŸ¥è¯†åº“å†…å®¹ï¼Œæ”¯æŒæ®µè½ã€å®ä½“ã€å…³ç³»å’Œæ¸…ç©º"
    command_pattern = r"^\/delete(?:\s+(?P<mode>\w+))?(?:\s+(?P<content>.+))?$"

    def __init__(self, message: MessageRecv, plugin_config: Optional[dict] = None):
        super().__init__(message, plugin_config)

        self.vector_store: Optional[VectorStore] = self.plugin_config.get("vector_store")
        self.graph_store: Optional[GraphStore] = self.plugin_config.get("graph_store")
        self.metadata_store: Optional[MetadataStore] = self.plugin_config.get("metadata_store")

        # å…œåº•ï¼šå½“é…ç½®é‡Œæ²¡æœ‰å®ä¾‹æ—¶ï¼Œä»æ’ä»¶å…¨å±€å®ä¾‹è·å–
        if not all([
            self.vector_store is not None,
            self.graph_store is not None,
            self.metadata_store is not None,
        ]):
            from ...plugin import A_MemorixPlugin

            instances = A_MemorixPlugin.get_storage_instances()
            if instances:
                self.vector_store = self.vector_store or instances.get("vector_store")
                self.graph_store = self.graph_store or instances.get("graph_store")
                self.metadata_store = self.metadata_store or instances.get("metadata_store")

        if self.message and self.message.chat_stream:
            self.log_prefix = f"[DeleteCommand-{self.message.chat_stream.stream_id}]"
        else:
            self.log_prefix = "[DeleteCommand]"

    async def execute(self) -> Tuple[bool, Optional[str], int]:
        """æ‰§è¡Œåˆ é™¤å‘½ä»¤"""
        if not all([
            self.vector_store is not None,
            self.graph_store is not None,
            self.metadata_store is not None,
        ]):
            return False, "âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œåˆ é™¤", 0

        mode = (self.matched_groups.get("mode") or "help").lower()
        content = (self.matched_groups.get("content") or "").strip()

        mode_alias = {
            "p": "paragraph",
            "e": "entity",
            "r": "relation",
            "s": "stats",
            "h": "help",
            "?": "help",
        }
        mode = mode_alias.get(mode, mode)

        try:
            if mode in ["help", ""]:
                return True, self._get_help_message(), 0

            if mode == "stats":
                ok, msg = self._get_deletion_stats()
                return ok, msg, 0

            if mode == "clear":
                ok, msg = await self._clear_knowledge_base()
            elif mode == "paragraph":
                if not content:
                    return False, "âŒ ç”¨æ³•: /delete paragraph <hashæˆ–å†…å®¹>", 0
                ok, msg = await self._delete_paragraph(content)
            elif mode == "entity":
                if not content:
                    return False, "âŒ ç”¨æ³•: /delete entity <å®ä½“åç§°>", 0
                ok, msg = await self._delete_entity(content)
            elif mode == "relation":
                if not content:
                    return False, "âŒ ç”¨æ³•: /delete relation <hashæˆ–subject|predicate|object>", 0
                ok, msg = await self._delete_relation(content)
            else:
                return False, f"âŒ æœªçŸ¥åˆ é™¤æ¨¡å¼: {mode}\n\n{self._get_help_message()}", 0

            if ok:
                try:
                    self.vector_store.save()
                    self.graph_store.save()
                except Exception as save_e:  # noqa: BLE001
                    logger.warning(f"{self.log_prefix} åˆ é™¤åä¿å­˜å¤±è´¥: {save_e}")

            return ok, msg, 0

        except Exception as e:  # noqa: BLE001
            logger.error(f"{self.log_prefix} åˆ é™¤å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {e}")
            return False, f"âŒ åˆ é™¤å¤±è´¥: {e}", 0

    @staticmethod
    def _looks_like_hash(text: str) -> bool:
        return bool(re.fullmatch(r"[0-9a-fA-F]{64}", text.strip()))

    async def _delete_paragraph(self, paragraph_spec: str) -> Tuple[bool, str]:
        """åˆ é™¤æ®µè½ï¼ˆä¼˜å…ˆ hashï¼Œå›é€€æŒ‰å†…å®¹åŒ¹é…ï¼‰"""
        start_time = time.time()

        query = paragraph_spec.strip()
        if not query:
            return False, "âŒ æ®µè½å†…å®¹ä¸èƒ½ä¸ºç©º"

        target: Optional[Dict[str, Any]] = None

        if self._looks_like_hash(query):
            target = self.metadata_store.get_paragraph(query)
            if not target:
                return False, f"âŒ æœªæ‰¾åˆ°æ®µè½: {query[:16]}..."
        else:
            matches = self.metadata_store.search_paragraphs_by_content(query)
            if not matches:
                return False, "âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ®µè½"

            if len(matches) > 1:
                query_norm = normalize_text(query)
                exact = [
                    p for p in matches
                    if normalize_text(str(p.get("content", ""))) == query_norm
                ]
                if len(exact) == 1:
                    target = exact[0]
                else:
                    previews: List[str] = []
                    for p in matches[:5]:
                        content_preview = str(p.get("content", "")).replace("\n", " ")
                        if len(content_preview) > 40:
                            content_preview = content_preview[:40] + "..."
                        previews.append(f"- {p['hash'][:16]}... {content_preview}")
                    return False, "âš ï¸ åŒ¹é…åˆ°å¤šä¸ªæ®µè½ï¼Œè¯·ä½¿ç”¨ hash ç²¾ç¡®åˆ é™¤:\n" + "\n".join(previews)
            else:
                target = matches[0]

        paragraph_hash = str(target["hash"])

        cleanup_plan = self.metadata_store.delete_paragraph_atomic(paragraph_hash)

        relation_prune_ops = cleanup_plan.get("relation_prune_ops", []) or []
        edges_to_remove = cleanup_plan.get("edges_to_remove", []) or []

        # ä¼˜å…ˆæŒ‰ relation hash ç²¾ç¡®ä¿®å‰ªè¾¹æ˜ å°„
        if relation_prune_ops and hasattr(self.graph_store, "prune_relation_hashes"):
            self.graph_store.prune_relation_hashes(relation_prune_ops)
        elif edges_to_remove:
            self.graph_store.delete_edges(edges_to_remove)

        # å‘é‡åˆ é™¤ï¼šæ®µè½å‘é‡ + è¢«å‰ªæ‰çš„å…³ç³»å‘é‡
        vector_ids: List[str] = []
        vector_id_to_remove = cleanup_plan.get("vector_id_to_remove")
        if vector_id_to_remove:
            vector_ids.append(str(vector_id_to_remove))
        for op in relation_prune_ops:
            if len(op) >= 3 and op[2]:
                vector_ids.append(str(op[2]))

        deleted_vectors = 0
        if vector_ids:
            dedup_ids = list(dict.fromkeys(vector_ids))
            deleted_vectors = self.vector_store.delete(dedup_ids)

        elapsed = time.time() - start_time
        result_lines = [
            "âœ… æ®µè½åˆ é™¤å®Œæˆ",
            f"ğŸ“„ Hash: {paragraph_hash[:16]}...",
            f"ğŸ”— æ¸…ç†å…³ç³»: {len(relation_prune_ops)}",
            f"ğŸ§¹ æ¸…ç†å‘é‡: {deleted_vectors}",
            f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
        ]
        return True, "\n".join(result_lines)

    async def _delete_entity(self, entity_name: str) -> Tuple[bool, str]:
        """åˆ é™¤å®ä½“"""
        start_time = time.time()

        entity_name = entity_name.strip()
        if not entity_name:
            return False, "âŒ å®ä½“åç§°ä¸èƒ½ä¸ºç©º"

        canonical_name = entity_name.lower()

        if not self.graph_store.has_node(canonical_name):
            return False, f"âŒ å®ä½“ä¸å­˜åœ¨: {canonical_name}"

        neighbors = self.graph_store.get_neighbors(canonical_name)
        edge_count = len(neighbors)
        related_paragraphs = self.metadata_store.get_paragraphs_by_entity(canonical_name)

        # é¢„å…ˆè®°å½•ç›¸å…³å…³ç³» hashï¼Œä¾¿äºåˆ é™¤å¯¹åº”å‘é‡
        rel_as_subject = self.metadata_store.get_relations(subject=canonical_name)
        rel_as_object = self.metadata_store.get_relations(object=canonical_name)
        relation_hashes = {
            str(r["hash"]) for r in (rel_as_subject + rel_as_object) if r.get("hash")
        }

        deleted_nodes = self.graph_store.delete_nodes([canonical_name])
        meta_deleted = self.metadata_store.delete_entity(canonical_name)

        vector_ids = [compute_hash(canonical_name)] + list(relation_hashes)
        deleted_vectors = 0
        try:
            deleted_vectors = self.vector_store.delete(vector_ids)
        except Exception as e:  # noqa: BLE001
            logger.warning(f"{self.log_prefix} åˆ é™¤å®ä½“å‘é‡å¤±è´¥ {canonical_name}: {e}")

        if deleted_nodes <= 0 and not meta_deleted:
            return False, f"âŒ å®ä½“åˆ é™¤å¤±è´¥: {canonical_name}"

        elapsed = time.time() - start_time
        result_lines = [
            "âœ… å®ä½“åˆ é™¤å®Œæˆ",
            f"ğŸ·ï¸ å®ä½“åç§°: {canonical_name}",
            f"ğŸ”— å…³è”è¾¹æ•°: {edge_count}",
            f"ğŸ“„ ç›¸å…³æ®µè½: {len(related_paragraphs)}",
            f"ğŸ§¹ æ¸…ç†å‘é‡: {deleted_vectors}",
            f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
            "",
            "âš ï¸ æ³¨æ„: ç›¸å…³æ®µè½æœªåˆ é™¤ï¼Œå¦‚éœ€åˆ é™¤è¯·ä½¿ç”¨ /delete paragraph",
        ]

        return True, "\n".join(result_lines)

    async def _delete_relation(self, relation_spec: str) -> Tuple[bool, str]:
        """åˆ é™¤å…³ç³»"""
        start_time = time.time()

        relation_spec = relation_spec.strip()
        if not relation_spec:
            return False, "âŒ å…³ç³»è§„æ ¼ä¸èƒ½ä¸ºç©º"

        relation: Optional[Dict[str, Any]] = None
        hash_value = ""

        if self._looks_like_hash(relation_spec):
            hash_value = relation_spec.lower()
            relation = self.metadata_store.get_relation(hash_value)
            if not relation:
                return False, f"âŒ æœªæ‰¾åˆ°å…³ç³»: {hash_value[:16]}..."
        else:
            if "|" in relation_spec:
                parts = [p.strip() for p in relation_spec.split("|")]
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject|predicate|object"
                subject, predicate, obj = parts
            else:
                parts = relation_spec.split(maxsplit=2)
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject predicate object"
                subject, predicate, obj = parts

            s_canon = subject.strip().lower()
            p_canon = predicate.strip().lower()
            o_canon = obj.strip().lower()

            relation_key = f"{s_canon}|{p_canon}|{o_canon}"
            hash_value = compute_hash(relation_key)

            relation = self.metadata_store.get_relation(hash_value)
            if not relation:
                return False, f"âŒ æœªæ‰¾åˆ°å…³ç³» (hashä¸åŒ¹é…): {subject} {predicate} {obj}"

        success = self.metadata_store.delete_relation(hash_value)
        if not success:
            return False, f"âŒ å…³ç³»åˆ é™¤å¤±è´¥: {hash_value[:16]}..."

        subject = str(relation.get("subject", ""))
        obj = str(relation.get("object", ""))

        if hasattr(self.graph_store, "prune_relation_hashes"):
            self.graph_store.prune_relation_hashes([(subject, obj, hash_value)])
        else:
            self.graph_store.delete_edges([(subject, obj)])

        deleted_vectors = 0
        try:
            deleted_vectors = self.vector_store.delete([hash_value])
        except Exception as e:  # noqa: BLE001
            logger.warning(f"{self.log_prefix} åˆ é™¤å…³ç³»å‘é‡å¤±è´¥ {hash_value[:16]}...: {e}")

        elapsed = time.time() - start_time
        result_lines = [
            "âœ… å…³ç³»åˆ é™¤å®Œæˆ",
            f"ğŸ”— Hash: {hash_value[:16]}...",
            f"ğŸ“Œ {subject} {relation.get('predicate', '')} {obj}",
            f"ğŸ§¹ æ¸…ç†å‘é‡: {deleted_vectors}",
            f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
        ]

        return True, "\n".join(result_lines)

    async def _clear_knowledge_base(self) -> Tuple[bool, str]:
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        start_time = time.time()

        try:
            num_paragraphs = self.metadata_store.count_paragraphs()
            num_relations = self.metadata_store.count_relations()
            num_entities = self.metadata_store.count_entities()
            num_vectors = self.vector_store.num_vectors

            self.vector_store.clear()
            self.graph_store.clear()
            self.metadata_store.clear_all()

            elapsed = time.time() - start_time

            result_lines = [
                "âš ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º",
                "",
                "ğŸ“Š å·²åˆ é™¤å†…å®¹:",
                f"  - æ®µè½: {num_paragraphs}",
                f"  - å…³ç³»: {num_relations}",
                f"  - å®ä½“: {num_entities}",
                f"  - å‘é‡: {num_vectors}",
                "",
                f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
                "",
                "âš ï¸ æ­¤æ“ä½œä¸å¯æ’¤é”€ï¼",
            ]

            return True, "\n".join(result_lines)

        except Exception as e:  # noqa: BLE001
            return False, f"âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"

    def _get_deletion_stats(self) -> Tuple[bool, str]:
        """è·å–åˆ é™¤ç»Ÿè®¡ä¿¡æ¯"""
        deleted_paragraphs = self.metadata_store.count_paragraphs(include_deleted=True, only_deleted=True)
        deleted_relations = self.metadata_store.count_relations(include_deleted=True, only_deleted=True)

        current_paragraphs = self.metadata_store.count_paragraphs()
        current_relations = self.metadata_store.count_relations()
        current_entities = self.metadata_store.count_entities()

        lines = [
            "ğŸ“Š åˆ é™¤ç»Ÿè®¡ä¿¡æ¯",
            "",
            "ğŸ—‘ï¸ å·²åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰:",
            f"  - æ®µè½: {deleted_paragraphs}",
            f"  - å…³ç³»: {deleted_relations}",
            "",
            "ğŸ“¦ å½“å‰å†…å®¹:",
            f"  - æ®µè½: {current_paragraphs}",
            f"  - å…³ç³»: {current_relations}",
            f"  - å®ä½“: {current_entities}",
            "",
            "ğŸ’¡ æç¤º:",
            "  - æ®µè½å’Œå…³ç³»ä½¿ç”¨è½¯åˆ é™¤ï¼Œå¯é€šè¿‡é‡å»ºç´¢å¼•å½»åº•æ¸…é™¤",
            "  - ä½¿ç”¨ /delete clear æ¸…ç©ºæ•´ä¸ªçŸ¥è¯†åº“",
        ]

        return True, "\n".join(lines)

    def _get_help_message(self) -> str:
        """è·å–å¸®åŠ©æ¶ˆæ¯"""
        return """ğŸ“– åˆ é™¤å‘½ä»¤å¸®åŠ©

ç”¨æ³•:
  /delete paragraph <hashæˆ–å†…å®¹>  - åˆ é™¤æ®µè½ï¼ˆè½¯åˆ é™¤ï¼‰
  /delete entity <å®ä½“åç§°>       - åˆ é™¤å®ä½“
  /delete relation <å…³ç³»è§„æ ¼>     - åˆ é™¤å…³ç³»
  /delete clear                  - æ¸…ç©ºçŸ¥è¯†åº“ï¼ˆå±é™©æ“ä½œï¼ï¼‰
  /delete stats                  - æ˜¾ç¤ºåˆ é™¤ç»Ÿè®¡
  /delete help                   - æ˜¾ç¤ºæ­¤å¸®åŠ©

å¿«æ·æ¨¡å¼:
  /delete p <hashæˆ–å†…å®¹>         - åˆ é™¤æ®µè½ï¼ˆparagraphçš„ç®€å†™ï¼‰
  /delete e <å®ä½“åç§°>           - åˆ é™¤å®ä½“ï¼ˆentityçš„ç®€å†™ï¼‰
  /delete r <å…³ç³»è§„æ ¼>           - åˆ é™¤å…³ç³»ï¼ˆrelationçš„ç®€å†™ï¼‰

ç¤ºä¾‹:
  /delete paragraph a1b2c3d4...
  /delete paragraph äººå·¥æ™ºèƒ½çš„å®šä¹‰
  /delete entity Apple
  /delete relation Apple|founded|Steve Jobs
  /delete relation founded by Steve Jobs
  /delete stats

å…³ç³»æ ¼å¼:
  - subject|predicate|objectï¼ˆä½¿ç”¨|åˆ†éš”ï¼‰
  - subject predicate objectï¼ˆä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼‰
  - å®Œæ•´çš„64ä½hashå€¼ï¼ˆç²¾ç¡®åˆ é™¤ï¼‰

æ³¨æ„äº‹é¡¹:
  - æ®µè½åˆ é™¤é‡‡ç”¨è½¯åˆ é™¤ï¼Œä¸ä¼šç«‹å³ç‰©ç†åˆ é™¤
  - åˆ é™¤å®ä½“ä¸ä¼šåˆ é™¤ç›¸å…³æ®µè½ï¼Œä»…åˆ é™¤å®ä½“èŠ‚ç‚¹
  - åˆ é™¤å…³ç³»ä¼šåŒæ—¶åˆ é™¤å›¾ä¸­çš„è¾¹
  - clearæ“ä½œä¸å¯æ’¤é”€ï¼Œè¯·è°¨æ…ä½¿ç”¨
"""
