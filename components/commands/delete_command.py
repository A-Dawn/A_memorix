"""
åˆ é™¤çŸ¥è¯†Commandç»„ä»¶

æä¾›çŸ¥è¯†åº“åˆ é™¤åŠŸèƒ½ï¼Œæ”¯æŒæ®µè½ã€å®ä½“å’Œå…³ç³»çš„åˆ é™¤ã€‚
"""

import time
from typing import Tuple, Optional, List, Dict, Any
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.base.base_command import BaseCommand
from ...core.utils.hash import compute_hash

# ... (existing imports)

class DeleteCommand(BaseCommand):
# ... (existing code)

    async def _delete_entity(self, entity_name: str) -> Tuple[bool, str]:
        """åˆ é™¤å®ä½“
        
        Args:
            entity_name: å®ä½“åç§°

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        start_time = time.time()

        # è§„èŒƒåŒ–å®ä½“åç§°
        entity_name = entity_name.strip().lower()

        # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨
        if not self.graph_store.has_node(entity_name):
            return False, f"âŒ å®ä½“ä¸å­˜åœ¨: {entity_name}"

        # è·å–ç›¸å…³å…³ç³»ç»Ÿè®¡
        neighbors = self.graph_store.get_neighbors(entity_name)
        edge_count = len(neighbors)

        # è·å–ç›¸å…³æ®µè½ (å·²è‡ªåŠ¨å¤„ç† canonical lookup)
        related_paragraphs = self.metadata_store.get_paragraphs_by_entity(entity_name)
        
        # è®¡ç®—hashå¹¶ä»å‘é‡åº“åˆ é™¤ (ç¡®ä¿ä¸€è‡´æ€§)
        try:
            # é€»è¾‘éœ€ä¸ MetadataStore.add_entity ä¿æŒä¸€è‡´
            # entity_name å·²ç»æ˜¯ canonicalized
            entity_hash = compute_hash(entity_name)
            self.vector_store.remove([entity_hash])
        except Exception as e:
            logger.warning(f"{self.log_prefix} åˆ é™¤å®ä½“å‘é‡å¤±è´¥ {entity_name}: {e}")

        # åˆ é™¤å®ä½“
        success = self.graph_store.remove_nodes([entity_name])

        if success:
            # ä»å…ƒæ•°æ®ä¸­åˆ é™¤å®ä½“
            self.metadata_store.delete_entity(entity_name)

            elapsed = time.time() - start_time

            result_lines = [
                "âœ… å®ä½“åˆ é™¤å®Œæˆ",
                f"ğŸ·ï¸ å®ä½“åç§°: {entity_name}",
                f"ğŸ”— å…³è”è¾¹æ•°: {edge_count}",
                f"ğŸ“„ ç›¸å…³æ®µè½: {len(related_paragraphs)}",
                f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
                "",
                "âš ï¸ æ³¨æ„: ç›¸å…³æ®µè½æœªåˆ é™¤ï¼Œå¦‚éœ€åˆ é™¤è¯·ä½¿ç”¨ /delete paragraph",
            ]

            return True, "\n".join(result_lines)
        else:
            return False, f"âŒ å®ä½“åˆ é™¤å¤±è´¥: {entity_name}"

    async def _delete_relation(self, relation_spec: str) -> Tuple[bool, str]:
        """åˆ é™¤å…³ç³»

        Args:
            relation_spec: å…³ç³»è§„æ ¼ (æ ¼å¼: subject|predicate|object æˆ– hash)

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        start_time = time.time()

        # æ£€æŸ¥æ˜¯å¦ä¸ºhash
        if len(relation_spec) == 64:  # SHA256 hashé•¿åº¦
            hash_value = relation_spec
            relation = self.metadata_store.get_relation_by_hash(hash_value)

            if not relation:
                return False, f"âŒ æœªæ‰¾åˆ°å…³ç³»: {hash_value[:16]}..."

            subject = relation.get("subject", "")
            predicate = relation.get("predicate", "")
            obj = relation.get("object", "")
        else:
            # è§£æå…³ç³»è§„æ ¼
            if "|" in relation_spec:
                parts = relation_spec.split("|")
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject|predicate|object"
                subject, predicate, obj = parts
            else:
                parts = relation_spec.split(maxsplit=2)
                if len(parts) != 3:
                    return False, "âŒ å…³ç³»æ ¼å¼é”™è¯¯ï¼Œåº”ä½¿ç”¨: subject predicate object"
                subject, predicate, obj = parts

            # æŸ¥æ‰¾å…³ç³» (æ­¤æ—¶éœ€è¦è§„èŒƒåŒ–å‚æ•°ä»¥åŒ¹é…æ•°æ®åº“ä¸­çš„å­˜å‚¨)
            # æ³¨æ„: MetadataStore.get_relations ç›®å‰æ‰§è¡Œçš„æ˜¯éƒ¨åˆ†åŒ¹é… (LIKE)
            # å¦‚æœæˆ‘ä»¬è¦ç²¾ç¡®åˆ é™¤ï¼Œæœ€å¥½è‡ªå·±ç®— Hash ç„¶å get_relation_by_hash
            # æˆ–è€…ä¿®æ”¹ get_relations æ”¯æŒç²¾ç¡®åŒ¹é…?
            # ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬è®¡ç®— canonical hash ç„¶åç›´æ¥æŸ¥
            
            s_canon = subject.strip().lower()
            p_canon = predicate.strip().lower()
            o_canon = obj.strip().lower()
            
            relation_key = f"{s_canon}|{p_canon}|{o_canon}"
            hash_value = compute_hash(relation_key)
            
            relation = self.metadata_store.get_relation(hash_value)
            
            if not relation:
                 # ä¹Ÿè®¸ç”¨æˆ·åªæ˜¯æƒ³æ¨¡ç³Šåˆ é™¤? ä½† /delete relation åœ¨è¯­ä¹‰ä¸Šåº”è¯¥æ˜¯åˆ é™¤å…·ä½“æŸä¸€ä¸ª
                 return False, f"âŒ æœªæ‰¾åˆ°å…³ç³» (æˆ– Hash ä¸åŒ¹é…): {subject} {predicate} {obj}"

            # å…¼å®¹æ—§é€»è¾‘å˜é‡å
            relations = [relation] 

            if not relations:
                return False, f"âŒ æœªæ‰¾åˆ°å…³ç³»: {subject} {predicate} {obj}"

            if len(relations) > 1:
                return False, f"âš ï¸ æ‰¾åˆ° {len(relations)} ä¸ªåŒ¹é…çš„å…³ç³»ï¼Œè¯·ä½¿ç”¨hashç²¾ç¡®åˆ é™¤"

            relation = relations[0]
            hash_value = relation["hash"]

        # åˆ é™¤å…³ç³»
        success = self.metadata_store.delete_relation(hash_value)

        if success:
            # ä»å›¾ä¸­åˆ é™¤è¾¹
            subject = relation.get("subject", "")
            obj = relation.get("object", "")
            self.graph_store.remove_edges([(subject, obj)])

            elapsed = time.time() - start_time

            result_lines = [
                "âœ… å…³ç³»åˆ é™¤å®Œæˆ",
                f"ğŸ”— Hash: {hash_value[:16]}...",
                f"ğŸ“Œ {subject} {relation.get('predicate', '')} {obj}",
                f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
            ]

            return True, "\n".join(result_lines)
        else:
            return False, f"âŒ å…³ç³»åˆ é™¤å¤±è´¥: {hash_value[:16]}..."

    async def _clear_knowledge_base(self) -> Tuple[bool, str]:
        """æ¸…ç©ºçŸ¥è¯†åº“

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»“æœæ¶ˆæ¯)
        """
        # âš ï¸ å±é™©æ“ä½œï¼Œéœ€è¦é¢å¤–ç¡®è®¤
        # è¿™é‡Œç®€å•å®ç°ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥è¦æ±‚äºŒæ¬¡ç¡®è®¤

        start_time = time.time()

        try:
            # è·å–å½“å‰ç»Ÿè®¡
            num_paragraphs = self.metadata_store.count_paragraphs()
            num_relations = self.metadata_store.count_relations()
            num_entities = self.metadata_store.count_entities()
            num_vectors = self.vector_store.num_vectors

            # æ¸…ç©ºå‘é‡å­˜å‚¨
            self.vector_store.clear()

            # æ¸…ç©ºå›¾å­˜å‚¨
            self.graph_store.clear()

            # æ¸…ç©ºå…ƒæ•°æ®å­˜å‚¨
            self.metadata_store.clear_all()

            elapsed = time.time() - start_time

            result_lines = [
                "âš ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º",
                "",
                "ğŸ“Š å·²åˆ é™¤å†…å®¹:",
                f"  - æ®µè½: {num_paragraphs}",
                f"  - å…³ç³»: {num_relations}",
                f"  - å®ä½“: {num_entities}",
                f"  - å‘é‡: {num_vectors}",
                "",
                f"â±ï¸ è€—æ—¶: {elapsed*1000:.1f}ms",
                "",
                "âš ï¸ æ­¤æ“ä½œä¸å¯æ’¤é”€ï¼",
            ]

            return True, "\n".join(result_lines)

        except Exception as e:
            return False, f"âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"

    def _get_deletion_stats(self) -> Tuple[bool, str]:
        """è·å–åˆ é™¤ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, ç»Ÿè®¡ä¿¡æ¯)
        """
        # è·å–è½¯åˆ é™¤ç»Ÿè®¡
        deleted_paragraphs = self.metadata_store.count_paragraphs(include_deleted=True, only_deleted=True)
        deleted_relations = self.metadata_store.count_relations(include_deleted=True, only_deleted=True)

        # è·å–å½“å‰ç»Ÿè®¡
        current_paragraphs = self.metadata_store.count_paragraphs()
        current_relations = self.metadata_store.count_relations()
        current_entities = self.metadata_store.count_entities()

        # æ„å»ºå“åº”
        lines = [
            "ğŸ“Š åˆ é™¤ç»Ÿè®¡ä¿¡æ¯",
            "",
            "ğŸ—‘ï¸ å·²åˆ é™¤ï¼ˆè½¯åˆ é™¤ï¼‰:",
            f"  - æ®µè½: {deleted_paragraphs}",
            f"  - å…³ç³»: {deleted_relations}",
            "",
            "ğŸ“¦ å½“å‰å†…å®¹:",
            f"  - æ®µè½: {current_paragraphs}",
            f"  - å…³ç³»: {current_relations}",
            f"  - å®ä½“: {current_entities}",
            "",
            "ğŸ’¡ æç¤º:",
            "  - æ®µè½å’Œå…³ç³»ä½¿ç”¨è½¯åˆ é™¤ï¼Œå¯é€šè¿‡é‡å»ºç´¢å¼•å½»åº•æ¸…é™¤",
            "  - ä½¿ç”¨ /delete clear æ¸…ç©ºæ•´ä¸ªçŸ¥è¯†åº“",
        ]

        return True, "\n".join(lines)

    def _get_help_message(self) -> str:
        """è·å–å¸®åŠ©æ¶ˆæ¯

        Returns:
            å¸®åŠ©æ¶ˆæ¯æ–‡æœ¬
        """
        return """ğŸ“– åˆ é™¤å‘½ä»¤å¸®åŠ©

ç”¨æ³•:
  /delete paragraph <hashæˆ–å†…å®¹>  - åˆ é™¤æ®µè½ï¼ˆè½¯åˆ é™¤ï¼‰
  /delete entity <å®ä½“åç§°>       - åˆ é™¤å®ä½“
  /delete relation <å…³ç³»è§„æ ¼>     - åˆ é™¤å…³ç³»
  /delete clear                  - æ¸…ç©ºçŸ¥è¯†åº“ï¼ˆå±é™©æ“ä½œï¼ï¼‰
  /delete stats                  - æ˜¾ç¤ºåˆ é™¤ç»Ÿè®¡
  /delete help                   - æ˜¾ç¤ºæ­¤å¸®åŠ©

å¿«æ·æ¨¡å¼:
  /delete p <hashæˆ–å†…å®¹>         - åˆ é™¤æ®µè½ï¼ˆparagraphçš„ç®€å†™ï¼‰
  /delete e <å®ä½“åç§°>           - åˆ é™¤å®ä½“ï¼ˆentityçš„ç®€å†™ï¼‰
  /delete r <å…³ç³»è§„æ ¼>           - åˆ é™¤å…³ç³»ï¼ˆrelationçš„ç®€å†™ï¼‰

ç¤ºä¾‹:
  /delete paragraph a1b2c3d4...
  /delete paragraph äººå·¥æ™ºèƒ½çš„å®šä¹‰
  /delete entity Apple
  /delete relation Apple|founded|Steve Jobs
  /delete relation founded by Steve Jobs
  /delete stats

å…³ç³»æ ¼å¼:
  - subject|predicate|objectï¼ˆä½¿ç”¨|åˆ†éš”ï¼‰
  - subject predicate objectï¼ˆä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼‰
  - å®Œæ•´çš„64ä½hashå€¼ï¼ˆç²¾ç¡®åˆ é™¤ï¼‰

æ³¨æ„äº‹é¡¹:
  - æ®µè½åˆ é™¤é‡‡ç”¨è½¯åˆ é™¤ï¼Œä¸ä¼šç«‹å³ç‰©ç†åˆ é™¤
  - åˆ é™¤å®ä½“ä¸ä¼šåˆ é™¤ç›¸å…³æ®µè½ï¼Œä»…åˆ é™¤å®ä½“èŠ‚ç‚¹
  - åˆ é™¤å…³ç³»ä¼šåŒæ—¶åˆ é™¤å›¾ä¸­çš„è¾¹
  - clearæ“ä½œä¸å¯æ’¤é”€ï¼Œè¯·è°¨æ…ä½¿ç”¨
"""
