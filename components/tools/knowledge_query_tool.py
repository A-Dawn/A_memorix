"""
çŸ¥è¯†æŸ¥è¯¢Toolç»„ä»¶

æä¾›LLMå¯è°ƒç”¨çš„çŸ¥è¯†æŸ¥è¯¢å·¥å…·ã€‚
"""

import time
from typing import Any, List, Tuple, Optional, Dict
from pathlib import Path

from src.common.logger import get_logger
from src.plugin_system.apis import person_api
from src.plugin_system.base.base_tool import BaseTool
from src.plugin_system.base.component_types import ToolParamType
from src.chat.message_receive.chat_stream import ChatStream

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from ...core import (
    DualPathRetriever,
    RetrievalStrategy,
    DualPathRetrieverConfig,
    TemporalQueryOptions,
    DynamicThresholdFilter,
    ThresholdMethod,
    ThresholdConfig,
    SparseBM25Config,
    FusionConfig,
)
from ...core.utils.time_parser import parse_query_time_range
from ...core.utils.person_profile_service import PersonProfileService

logger = get_logger("A_Memorix.KnowledgeQueryTool")


class KnowledgeQueryTool(BaseTool):
    """çŸ¥è¯†æŸ¥è¯¢Tool

    åŠŸèƒ½ï¼š
    - åŒè·¯æ£€ç´¢æŸ¥è¯¢
    - å®ä½“æŸ¥è¯¢
    - å…³ç³»æŸ¥è¯¢
    - ç»Ÿè®¡ä¿¡æ¯
    - LLMå¯ç›´æ¥è°ƒç”¨
    """

    # ToolåŸºæœ¬ä¿¡æ¯
    name = "knowledge_query"
    description = "æŸ¥è¯¢A_MemorixçŸ¥è¯†åº“ï¼Œæ”¯æŒæ£€ç´¢ã€å®ä½“æŸ¥è¯¢ã€å…³ç³»æŸ¥è¯¢å’Œç»Ÿè®¡ä¿¡æ¯"

    # Toolå‚æ•°å®šä¹‰
    parameters: List[Tuple[str, ToolParamType, str, bool, List[str] | None]] = [
        (
            "query_type",
            ToolParamType.STRING,
            "æŸ¥è¯¢ç±»å‹ï¼šsearch(æ£€ç´¢)ã€time(æ—¶åºæ£€ç´¢)ã€entity(å®ä½“)ã€relation(å…³ç³»)ã€person(äººç‰©ç”»åƒ)ã€stats(ç»Ÿè®¡)",
            True,
            ["search", "time", "entity", "relation", "person", "stats"],
        ),
        (
            "query",
            ToolParamType.STRING,
            "æŸ¥è¯¢å†…å®¹ï¼ˆæ£€ç´¢æ–‡æœ¬/å®ä½“åç§°/å…³ç³»è§„æ ¼ï¼‰ï¼Œstatsæ¨¡å¼ä¸éœ€è¦",
            False,
            None,
        ),
        (
            "person_id",
            ToolParamType.STRING,
            "äººç‰©IDï¼ˆpersonæ¨¡å¼å¯é€‰ï¼›ä¸ºç©ºæ—¶ä¼šå°è¯•é€šè¿‡queryæˆ–ä¼šè¯ä¸Šä¸‹æ–‡è§£æï¼‰",
            False,
            None,
        ),
        (
            "top_k",
            ToolParamType.INTEGER,
            "è¿”å›ç»“æœæ•°é‡ï¼ˆsearch/timeæ¨¡å¼ï¼‰",
            False,
            None,
        ),
        (
            "use_threshold",
            ToolParamType.BOOLEAN,
            "æ˜¯å¦ä½¿ç”¨åŠ¨æ€é˜ˆå€¼è¿‡æ»¤ï¼ˆä»…searchæ¨¡å¼ï¼‰",
            False,
            None,
        ),
        (
            "time_from",
            ToolParamType.STRING,
            "å¼€å§‹æ—¶é—´ï¼ˆtimeæ¨¡å¼ï¼Œä»…æ”¯æŒ YYYY/MM/DD æˆ– YYYY/MM/DD HH:mmï¼›æ—¥æœŸæŒ‰ 00:00 å±•å¼€ï¼‰",
            False,
            None,
        ),
        (
            "time_to",
            ToolParamType.STRING,
            "ç»“æŸæ—¶é—´ï¼ˆtimeæ¨¡å¼ï¼Œä»…æ”¯æŒ YYYY/MM/DD æˆ– YYYY/MM/DD HH:mmï¼›æ—¥æœŸæŒ‰ 23:59 å±•å¼€ï¼‰",
            False,
            None,
        ),
        (
            "person",
            ToolParamType.STRING,
            "äººç‰©è¿‡æ»¤ï¼ˆtimeæ¨¡å¼å¯é€‰ï¼‰",
            False,
            None,
        ),
        (
            "source",
            ToolParamType.STRING,
            "æ¥æºè¿‡æ»¤ï¼ˆtimeæ¨¡å¼å¯é€‰ï¼‰",
            False,
            None,
        ),
    ]

    # LLMå¯ç”¨
    available_for_llm = True

    def __init__(self, plugin_config: Optional[dict] = None, chat_stream: Optional["ChatStream"] = None):
        """åˆå§‹åŒ–çŸ¥è¯†æŸ¥è¯¢Tool"""
        super().__init__(plugin_config, chat_stream)

        # è·å–å­˜å‚¨å®ä¾‹
        self.vector_store = self.plugin_config.get("vector_store")
        self.graph_store = self.plugin_config.get("graph_store")
        self.metadata_store = self.plugin_config.get("metadata_store")
        self.embedding_manager = self.plugin_config.get("embedding_manager")
        self.sparse_index = self.plugin_config.get("sparse_index")

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.retriever: Optional[DualPathRetriever] = None
        self.threshold_filter: Optional[DynamicThresholdFilter] = None

        # è®¾ç½®æ—¥å¿—å‰ç¼€
        chat_id = self.chat_id if self.chat_id else "unknown"
        self.log_prefix = f"[KnowledgeQueryTool-{chat_id}]"

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

    @property
    def debug_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è°ƒè¯•æ¨¡å¼"""
        advanced = self.plugin_config.get("advanced", {})
        if isinstance(advanced, dict):
            return advanced.get("debug", False)
        return self.plugin_config.get("debug", False)

    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æ£€ç´¢å’Œè¿‡æ»¤ç»„ä»¶"""
        try:
            # æ£€æŸ¥å­˜å‚¨æ˜¯å¦å¯ç”¨ (ä¼˜å…ˆä»é…ç½®è·å–ï¼Œå…œåº•ä»æ’ä»¶å®ä¾‹è·å–)
            vector_store = self.vector_store
            graph_store = self.graph_store
            metadata_store = self.metadata_store
            embedding_manager = self.embedding_manager
            sparse_index = self.sparse_index

            # å…œåº•é€»è¾‘ï¼šå¦‚æœé…ç½®ä¸­æ²¡æœ‰å­˜å‚¨å®ä¾‹ï¼Œå°è¯•ç›´æ¥ä»æ’ä»¶ç³»ç»Ÿè·å–
            # ä½¿ç”¨ is not None æ£€æŸ¥ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½å¸ƒå°”å€¼ä¸º False
            if not all([
                vector_store is not None,
                graph_store is not None,
                metadata_store is not None,
                embedding_manager is not None
            ]):
                from ...plugin import A_MemorixPlugin
                instances = A_MemorixPlugin.get_storage_instances()
                if instances:
                    vector_store = vector_store or instances.get("vector_store")
                    graph_store = graph_store or instances.get("graph_store")
                    metadata_store = metadata_store or instances.get("metadata_store")
                    embedding_manager = embedding_manager or instances.get("embedding_manager")
                    sparse_index = sparse_index or instances.get("sparse_index")
                    
                    # åŒæ­¥å›å®ä¾‹å±æ€§
                    self.vector_store = vector_store
                    self.graph_store = graph_store
                    self.metadata_store = metadata_store
                    self.embedding_manager = embedding_manager
                    self.sparse_index = sparse_index


            # æœ€ç»ˆæ£€æŸ¥ (ä½¿ç”¨ is not None è€Œéå¸ƒå°”å€¼ï¼Œå› ä¸ºç©ºå¯¹è±¡å¯èƒ½ä¸º False)
            if not all([
                vector_store is not None,
                graph_store is not None,
                metadata_store is not None,
                embedding_manager is not None
            ]):
                logger.warning(f"{self.log_prefix} å­˜å‚¨ç»„ä»¶æœªå®Œå…¨åˆå§‹åŒ–")
                return

            # åˆ›å»ºæ£€ç´¢å™¨é…ç½®
            sparse_cfg_raw = self.get_config("retrieval.sparse", {}) or {}
            if not isinstance(sparse_cfg_raw, dict):
                sparse_cfg_raw = {}
            fusion_cfg_raw = self.get_config("retrieval.fusion", {}) or {}
            if not isinstance(fusion_cfg_raw, dict):
                fusion_cfg_raw = {}
            try:
                sparse_cfg = SparseBM25Config(**sparse_cfg_raw)
            except Exception as e:
                logger.warning(f"{self.log_prefix} sparse é…ç½®éæ³•ï¼Œå›é€€é»˜è®¤: {e}")
                sparse_cfg = SparseBM25Config()
            try:
                fusion_cfg = FusionConfig(**fusion_cfg_raw)
            except Exception as e:
                logger.warning(f"{self.log_prefix} fusion é…ç½®éæ³•ï¼Œå›é€€é»˜è®¤: {e}")
                fusion_cfg = FusionConfig()
            config = DualPathRetrieverConfig(
                top_k_paragraphs=self.get_config("retrieval.top_k_paragraphs", 20),
                top_k_relations=self.get_config("retrieval.top_k_relations", 10),
                top_k_final=self.get_config("retrieval.top_k_final", 10),
                alpha=self.get_config("retrieval.alpha", 0.5),
                enable_ppr=self.get_config("retrieval.enable_ppr", True),
                ppr_alpha=self.get_config("retrieval.ppr_alpha", 0.85),
                ppr_concurrency_limit=self.get_config("retrieval.ppr_concurrency_limit", 4),
                enable_parallel=self.get_config("retrieval.enable_parallel", True),
                retrieval_strategy=RetrievalStrategy.DUAL_PATH,
                debug=self.debug_enabled,
                sparse=sparse_cfg,
                fusion=fusion_cfg,
            )

            # åˆ›å»ºæ£€ç´¢å™¨
            self.retriever = DualPathRetriever(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                metadata_store=self.metadata_store,
                embedding_manager=self.embedding_manager,
                sparse_index=self.sparse_index,
                config=config,
            )

            # åˆ›å»ºé˜ˆå€¼è¿‡æ»¤å™¨
            threshold_config = ThresholdConfig(
                method=ThresholdMethod.ADAPTIVE,
                min_threshold=self.get_config("threshold.min_threshold", 0.3),
                max_threshold=self.get_config("threshold.max_threshold", 0.95),
                percentile=self.get_config("threshold.percentile", 75.0),
                std_multiplier=self.get_config("threshold.std_multiplier", 1.5),
                min_results=self.get_config("threshold.min_results", 3),
                enable_auto_adjust=self.get_config("threshold.enable_auto_adjust", True),
            )

            self.threshold_filter = DynamicThresholdFilter(threshold_config)

            logger.info(f"{self.log_prefix} çŸ¥è¯†æŸ¥è¯¢Toolåˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"{self.log_prefix} ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

    async def execute(self, function_args: dict[str, Any]) -> dict[str, Any]:
        """æ‰§è¡Œå·¥å…·å‡½æ•°ï¼ˆä¾›LLMè°ƒç”¨ï¼‰

        Args:
            function_args: å·¥å…·è°ƒç”¨å‚æ•°
                - query_type: æŸ¥è¯¢ç±»å‹
                - query: æŸ¥è¯¢å†…å®¹
                - top_k: è¿”å›ç»“æœæ•°é‡
                - use_threshold: æ˜¯å¦ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤

        Returns:
            dict: å·¥å…·æ‰§è¡Œç»“æœ
        """
        # æ£€æŸ¥ç»„ä»¶æ˜¯å¦åˆå§‹åŒ–
        if not self.retriever:
            return {
                "success": False,
                "error": "çŸ¥è¯†æŸ¥è¯¢Toolæœªåˆå§‹åŒ–",
                "content": "âŒ çŸ¥è¯†æŸ¥è¯¢Toolæœªåˆå§‹åŒ–",
                "results": [],
            }

        # è§£æå‚æ•°
        query_type = str(function_args.get("query_type", "search") or "search").strip().lower()
        query = function_args.get("query", "")
        top_k_raw = function_args.get("top_k")
        default_top_k = (
            int(self.get_config("retrieval.temporal.default_top_k", 10))
            if query_type == "time"
            else 10
        )
        if top_k_raw is None:
            top_k = default_top_k
        else:
            try:
                top_k = max(1, int(top_k_raw))
            except (TypeError, ValueError):
                return {
                    "success": False,
                    "error": "top_k å¿…é¡»æ˜¯æ•´æ•°",
                    "content": "âŒ top_k å¿…é¡»æ˜¯æ•´æ•°",
                    "results": [],
                }
        use_threshold = function_args.get("use_threshold", True)
        time_from = function_args.get("time_from")
        time_to = function_args.get("time_to")
        person = function_args.get("person")
        source = function_args.get("source")
        person_id = function_args.get("person_id")
        for_injection = bool(function_args.get("for_injection", False))
        force_refresh = bool(function_args.get("force_refresh", False))
        stream_id = function_args.get("stream_id")
        user_id = function_args.get("user_id")

        logger.info(
            f"{self.log_prefix} LLMè°ƒç”¨: query_type={query_type}, "
            f"query='{query}', top_k={top_k}, time_from={time_from}, time_to={time_to}"
        )

        if self.debug_enabled:
            logger.info(f"{self.log_prefix} [DEBUG] å·¥å…·å®Œæ•´å‚æ•°: {function_args}")

        try:
            # æ ¹æ®æŸ¥è¯¢ç±»å‹æ‰§è¡Œ
            if query_type == "search":
                result = await self._search(query, top_k, use_threshold)
            elif query_type == "time":
                result = await self._query_time(
                    query=query,
                    top_k=top_k,
                    time_from=time_from,
                    time_to=time_to,
                    person=person,
                    source=source,
                    use_threshold=use_threshold,
                )
            elif query_type == "entity":
                result = await self._query_entity(query)
            elif query_type == "relation":
                result = await self._query_relation(query)
            elif query_type == "person":
                result = await self._query_person(
                    query=query,
                    person_id=person_id,
                    top_k=top_k,
                    for_injection=for_injection,
                    force_refresh=force_refresh,
                    stream_id=stream_id,
                    user_id=user_id,
                )
            elif query_type == "stats":
                result = self._get_stats()
            else:
                result = {
                    "success": False,
                    "error": f"æœªçŸ¥çš„æŸ¥è¯¢ç±»å‹: {query_type}",
                    "content": f"âŒ æœªçŸ¥çš„æŸ¥è¯¢ç±»å‹: {query_type}",
                    "results": [],
                }

            return result

        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
            logger.error(f"{self.log_prefix} {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "content": f"âŒ æŸ¥è¯¢å‘ç”Ÿé”™è¯¯: {error_msg}",
                "results": [],
            }

    async def direct_execute(
        self,
        query_type: str = "search",
        query: str = "",
        top_k: int = 10,
        use_threshold: bool = True,
        time_from: Optional[str] = None,
        time_to: Optional[str] = None,
        person: Optional[str] = None,
        source: Optional[str] = None,
        person_id: Optional[str] = None,
        for_injection: bool = False,
        force_refresh: bool = False,
        stream_id: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ç›´æ¥æ‰§è¡Œå·¥å…·å‡½æ•°ï¼ˆä¾›æ’ä»¶è°ƒç”¨ï¼‰

        Args:
            query_type: æŸ¥è¯¢ç±»å‹
            query: æŸ¥è¯¢å†…å®¹
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤

        Returns:
            Dict: æ‰§è¡Œç»“æœ
        """
        function_args = {
            "query_type": query_type,
            "query": query,
            "top_k": top_k,
            "use_threshold": use_threshold,
            "time_from": time_from,
            "time_to": time_to,
            "person": person,
            "source": source,
            "person_id": person_id,
            "for_injection": for_injection,
            "force_refresh": force_refresh,
            "stream_id": stream_id,
            "user_id": user_id,
        }

        return await self.execute(function_args)

    def _is_person_profile_injection_enabled(self, stream_id: Optional[str], user_id: Optional[str]) -> bool:
        if not bool(self.get_config("person_profile.enabled", True)):
            return False

        opt_in_required = bool(self.get_config("person_profile.opt_in_required", True))
        default_enabled = bool(self.get_config("person_profile.default_injection_enabled", False))

        if not opt_in_required:
            return default_enabled

        s_id = str(stream_id or "").strip()
        u_id = str(user_id or "").strip()
        if not s_id or not u_id or self.metadata_store is None:
            return False
        return bool(self.metadata_store.get_person_profile_switch(s_id, u_id, default=default_enabled))

    async def _query_person(
        self,
        query: str,
        person_id: Optional[str],
        top_k: int,
        for_injection: bool = False,
        force_refresh: bool = False,
        stream_id: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢äººç‰©ç”»åƒã€‚"""
        if not bool(self.get_config("person_profile.enabled", True)):
            if for_injection:
                return {
                    "success": True,
                    "query_type": "person",
                    "content": "",
                    "results": [],
                    "disabled_reason": "person_profile_module_disabled",
                }
            return {
                "success": False,
                "query_type": "person",
                "error": "äººç‰©ç”»åƒåŠŸèƒ½æœªå¯ç”¨ï¼ˆperson_profile.enabled=falseï¼‰",
                "content": "âŒ äººç‰©ç”»åƒåŠŸèƒ½æœªå¯ç”¨ï¼ˆperson_profile.enabled=falseï¼‰",
                "results": [],
            }

        resolved_stream_id = str(stream_id or self.chat_id or "").strip()
        resolved_user_id = str(user_id or "").strip()
        if not resolved_user_id and self.chat_stream and getattr(self.chat_stream, "user_info", None):
            resolved_user_id = str(getattr(self.chat_stream.user_info, "user_id", "") or "").strip()

        if for_injection and not self._is_person_profile_injection_enabled(resolved_stream_id, resolved_user_id):
            return {
                "success": True,
                "query_type": "person",
                "content": "",
                "results": [],
                "disabled_reason": "person_profile_not_opted_in",
            }

        service = PersonProfileService(
            metadata_store=self.metadata_store,
            graph_store=self.graph_store,
            vector_store=self.vector_store,
            embedding_manager=self.embedding_manager,
            sparse_index=self.sparse_index,
            plugin_config=self.plugin_config,
            retriever=self.retriever,
        )

        pid = str(person_id or "").strip()
        if not pid and resolved_user_id and self.platform:
            try:
                pid = person_api.get_person_id(self.platform, resolved_user_id)
            except Exception:
                pid = ""
        if not pid and query:
            pid = service.resolve_person_id(str(query))

        if not pid:
            if for_injection:
                return {
                    "success": True,
                    "query_type": "person",
                    "content": "",
                    "results": [],
                    "disabled_reason": "person_id_unresolved",
                }
            return {
                "success": False,
                "query_type": "person",
                "error": "æœªèƒ½è§£æ person_idï¼Œè¯·æä¾› person_id æˆ–æœ‰æ•ˆçš„äººå/åˆ«å",
                "content": "âŒ æœªèƒ½è§£æ person_idï¼Œè¯·æä¾› person_id æˆ–æœ‰æ•ˆçš„äººå/åˆ«å",
                "results": [],
            }

        ttl_minutes = float(self.get_config("person_profile.profile_ttl_minutes", 360))
        ttl_seconds = max(60.0, ttl_minutes * 60.0)

        profile = await service.query_person_profile(
            person_id=pid,
            person_keyword=str(query or "").strip(),
            top_k=max(4, top_k),
            ttl_seconds=ttl_seconds,
            force_refresh=bool(force_refresh),
            source_note="knowledge_query:person",
        )

        if not profile.get("success", False):
            if for_injection:
                return {
                    "success": True,
                    "query_type": "person",
                    "content": "",
                    "results": [],
                    "error": profile.get("error", "unknown"),
                }
            return {
                "success": False,
                "query_type": "person",
                "error": profile.get("error", "unknown"),
                "content": "âŒ äººç‰©ç”»åƒæŸ¥è¯¢å¤±è´¥",
                "results": [],
            }

        if resolved_stream_id and resolved_user_id and self.metadata_store is not None:
            try:
                self.metadata_store.mark_person_profile_active(resolved_stream_id, resolved_user_id, pid)
            except Exception as e:
                logger.warning(f"{self.log_prefix} è®°å½•æ´»è·ƒäººç‰©å¤±è´¥: {e}")

        persona_block = PersonProfileService.format_persona_profile_block(profile)
        if not persona_block and not for_injection:
            persona_block = "æš‚æ— è¶³å¤Ÿè¯æ®å½¢æˆè¯¥äººç‰©ç”»åƒã€‚"

        return {
            "success": True,
            "query_type": "person",
            "person_id": pid,
            "person_name": profile.get("person_name", ""),
            "profile_version": profile.get("profile_version"),
            "updated_at": profile.get("updated_at"),
            "expires_at": profile.get("expires_at"),
            "evidence_ids": profile.get("evidence_ids", []),
            "aliases": profile.get("aliases", []),
            "relation_edges": profile.get("relation_edges", []),
            "vector_evidence": profile.get("vector_evidence", []),
            "profile_source": profile.get("profile_source", "auto_snapshot"),
            "has_manual_override": bool(profile.get("has_manual_override", False)),
            "manual_override_text": profile.get("manual_override_text", ""),
            "auto_profile_text": profile.get("auto_profile_text", profile.get("profile_text", "")),
            "override_updated_at": profile.get("override_updated_at"),
            "override_updated_by": profile.get("override_updated_by", ""),
            "profile_text": profile.get("profile_text", ""),
            "content": persona_block,
            "results": [],
        }

    async def _search(
        self,
        query: str,
        top_k: int,
        use_threshold: bool,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ£€ç´¢æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤

        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        if not query:
            return {
                "success": False,
                "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "content": "âš ï¸ æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "results": [],
            }

        start_time = time.time()

        # æ‰§è¡Œæ£€ç´¢ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
        results = await self.retriever.retrieve(query, top_k=top_k)

        # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        if use_threshold and self.threshold_filter:
            results = self.threshold_filter.filter(results)
            if self.debug_enabled:
                logger.info(f"{self.log_prefix} [DEBUG] è¿‡æ»¤åç»“æœæ•°é‡ (Tool): {len(results)}")

        elapsed = time.time() - start_time

        # 3. Smart Fallback if results are weak
        # å¦‚æœæœ€é«˜åˆ† < 0.6ï¼Œå°è¯•æå–å®ä½“å¹¶è¿›è¡Œ Path Search
        max_score = 0.0
        if results:
            max_score = results[0].score

        fallback_triggered = False
        path_results = []
        
        # å¯é…ç½®é˜ˆå€¼ (TODO: ç§»è‡³ Config)
        SMART_FALLBACK_THRESHOLD = 0.6
        
        if max_score < SMART_FALLBACK_THRESHOLD:
            # å°è¯•æå–å®ä½“
            entities = self._extract_entities_from_query(query)
            if len(entities) == 2:
                if self.debug_enabled:
                    logger.info(f"{self.log_prefix} [Smart Fallback] Triggering Path Search for {entities}")
                
                path_data = self._path_search(query)
                if path_data and path_data.get("results"):
                    # è½¬æ¢ path results ä¸º search result æ ¼å¼
                    for p in path_data["results"]:
                        # æ„é€ ä¸€ä¸ªä¼ª RetrievalResult ç±»ä¼¼çš„ç»“æ„
                        path_results.append({
                            "type": "relation_path",
                            "score": 0.95, # ç»™èµ‹äºˆè¾ƒé«˜ç½®ä¿¡åº¦ï¼Œå› ä¸ºå®ƒåŸºäºå›¾
                            "content": f"[Indirect Relation] {p['description']}",
                            "metadata": {"source": "graph_path", "nodes": p['nodes']}
                        })
                    fallback_triggered = True

        # 4. åˆå¹¶ç»“æœ (Path Results ä¼˜å…ˆ)
        # Convert original results to dict format first
        formatted_results = []
        try:
            for result in results:
                formatted_results.append({
                    "type": result.result_type,
                    "score": float(result.score),
                    "content": result.content,
                    "metadata": result.metadata,
                })
        except Exception as e:
            logger.error(f"{self.log_prefix} Error formatting results: {e}")

        # å¦‚æœè§¦å‘äº† Fallbackï¼Œå°† Path ç»“æœåŠ åˆ°å‰é¢
        if fallback_triggered:
            formatted_results = path_results + formatted_results

        # 5. Deduplication (Safe Mode)
        # å»é‡ï¼Œä½†ä¿ç•™è‡³å°‘ 1 æ¡ (å¦‚æœåŸç»“æœä¸ä¸ºç©º)
        original_count = len(formatted_results)
        formatted_results = self._deduplicate_results(formatted_results)
        
        if self.debug_enabled:
            logger.info(f"{self.log_prefix} Deduplication: {original_count} -> {len(formatted_results)}")

        # 6. ç”Ÿæˆ content æ‘˜è¦ (Clean Output)
        if formatted_results:
            summary_lines = [f"æ‰¾åˆ° {len(formatted_results)} æ¡ç›¸å…³ä¿¡æ¯ï¼š"]
            for i, res in enumerate(formatted_results[:5]): # Top 5 for context
                type_icon = "ğŸ“„" if res['type'] == 'paragraph' else "ğŸ”—"
                if res['type'] == 'relation_path': type_icon = "ğŸ›¤ï¸"
                
                content_text = res.get('content', 'N/A')
                # Remove score from LLM output to avoid bias
                # But keep it in logs/debug
                summary_lines.append(f"{i+1}. {type_icon} {content_text}")
                
            content = "\n".join(summary_lines)
            logger.info(f"{self.log_prefix} Returning {len(formatted_results)} results to LLM context")
        else:
            content = "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"

        return {
            "success": True,
            "query_type": "search",
            "query": query,
            "results": formatted_results, # åŒ…å«åˆ†æ•°çš„å®Œæ•´æ•°æ®è¿”å›ç»™ç¨‹åº
            "count": len(formatted_results),
            "elapsed_ms": elapsed * 1000,
            "content": content, # ç»™ LLM çœ‹çš„æ‘˜è¦ (æ— åˆ†æ•°)
        }

    async def _query_time(
        self,
        query: str,
        top_k: int,
        time_from: Optional[str],
        time_to: Optional[str],
        person: Optional[str],
        source: Optional[str],
        use_threshold: bool = True,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ—¶åºæ£€ç´¢ï¼ˆå¯é€‰è¯­ä¹‰queryï¼‰ã€‚"""
        if not bool(self.get_config("retrieval.temporal.enabled", True)):
            return {
                "success": False,
                "error": "æ—¶åºæ£€ç´¢å·²ç¦ç”¨ï¼ˆretrieval.temporal.enabled=falseï¼‰",
                "content": "âŒ æ—¶åºæ£€ç´¢å·²ç¦ç”¨ï¼ˆretrieval.temporal.enabled=falseï¼‰",
                "results": [],
            }

        if not time_from and not time_to:
            return {
                "success": False,
                "error": "timeæ¨¡å¼è‡³å°‘éœ€è¦time_fromæˆ–time_to",
                "content": "âŒ timeæ¨¡å¼è‡³å°‘éœ€è¦time_fromæˆ–time_to",
                "results": [],
            }

        try:
            ts_from, ts_to = parse_query_time_range(
                str(time_from) if time_from is not None else None,
                str(time_to) if time_to is not None else None,
            )
        except ValueError as e:
            return {
                "success": False,
                "error": f"æ—¶é—´å‚æ•°é”™è¯¯: {e}",
                "content": f"âŒ æ—¶é—´å‚æ•°é”™è¯¯: {e}",
                "results": [],
            }

        temporal = TemporalQueryOptions(
            time_from=ts_from,
            time_to=ts_to,
            person=str(person).strip() if person else None,
            source=str(source).strip() if source else None,
            allow_created_fallback=self.get_config(
                "retrieval.temporal.allow_created_fallback",
                True,
            ),
            candidate_multiplier=int(
                self.get_config("retrieval.temporal.candidate_multiplier", 8)
            ),
            max_scan=int(self.get_config("retrieval.temporal.max_scan", 1000)),
        )

        start_time = time.time()
        results = await self.retriever.retrieve(
            query=query,
            top_k=top_k,
            temporal=temporal,
        )
        if query and use_threshold and self.threshold_filter:
            results = self.threshold_filter.filter(results)
        elapsed = time.time() - start_time

        formatted_results = []
        for result in results:
            metadata = dict(result.metadata or {})
            if "time_meta" not in metadata:
                metadata["time_meta"] = {}
            formatted_results.append(
                {
                    "hash": result.hash_value,
                    "type": result.result_type,
                    "score": float(result.score),
                    "content": result.content,
                    "metadata": metadata,
                }
            )

        if formatted_results:
            lines = [f"æ‰¾åˆ° {len(formatted_results)} æ¡æ—¶é—´ç›¸å…³ä¿¡æ¯ï¼š"]
            for i, item in enumerate(formatted_results[:5], 1):
                time_meta = item["metadata"].get("time_meta", {})
                s_text = time_meta.get("effective_start_text", "N/A")
                e_text = time_meta.get("effective_end_text", "N/A")
                basis = time_meta.get("match_basis", "none")
                lines.append(f"{i}. {item['content']}")
                lines.append(f"   æ—¶é—´: {s_text} ~ {e_text} ({basis})")
            content = "\n".join(lines)
        else:
            content = "æœªæ‰¾åˆ°ç¬¦åˆæ—¶é—´æ¡ä»¶çš„ç»“æœã€‚"

        return {
            "success": True,
            "query_type": "time",
            "query": query,
            "time_from": time_from,
            "time_to": time_to,
            "person": person,
            "source": source,
            "results": formatted_results,
            "count": len(formatted_results),
            "elapsed_ms": elapsed * 1000,
            "content": content,
        }

    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹ç»“æœè¿›è¡Œå»é‡
        - åŸºäºå†…å®¹å“ˆå¸Œ/ç›¸ä¼¼åº¦
        - ä¿ç•™åˆ†æ•°æœ€é«˜çš„
        - å®‰å…¨ä¿®æ­£: ä¿è¯ä¸å› ä¸ºå»é‡å¯¼è‡´ç»“æœä¸ºç©º
        """
        if not results:
            return []
            
        unique_results = []
        seen_hashes = set()
        seen_contents = set() # ä¸ºäº†å¤„ç†ä¸åŒ Hash ä½†å†…å®¹æç›¸ä¼¼çš„æƒ…å†µ (ç®€å•å‰ç¼€/åŒ…å«æ£€æŸ¥)
        
        # 1. Path Results æ€»æ˜¯ä¿ç•™ (åªè¦ä¸å®Œå…¨é‡å¤)
        # 2. Others based on content
        
        for res in results:
            # Simple content normalization
            content = res.get("content", "").strip()
            if not content:
                continue
                
            # Check exact hash (if available) or content hash
            res_md = res.get("metadata", {})
            h = res_md.get("hash") or str(hash(content))
            
            if h in seen_hashes:
                continue
                
            # Soft dedup: check if content is substring of already seen (or vice versa)
            # This is O(N^2) but N is small (TopK=10-20)
            is_dup = False
            for seen in seen_contents:
                # å¦‚æœæå…¶ç›¸ä¼¼ (æ¯”å¦‚åªæ˜¯å‰ç¼€ä¸åŒ)ï¼Œè§†ä¸ºé‡å¤
                # è¿™é‡Œç®€å•ä»ä¸¥ï¼šå¦‚æœ A åŒ…å« Bï¼Œä¿ç•™ A (é€šå¸¸ A ä¿¡æ¯é‡å¤§) ?
                # æˆ–è€…ä¿ç•™åˆ†æ•°é«˜çš„ã€‚
                # ç®€å•ç­–ç•¥: å¦‚æœ content å‡ ä¹ä¸€æ · (Levenshtein costly)ï¼Œè¿™é‡Œç”¨åŒ…å«å…³ç³»
                if content in seen or seen in content:
                    # å¦‚æœå½“å‰åˆ†æ•°æ˜¾è‘—æ›´é«˜ (>0.1 diff)ï¼Œåˆ™ä¿ç•™å½“å‰(è¿™å¾ˆéš¾ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯åœ¨ append)
                    # å‡è®¾ results å·²ç»æŒ‰ score æ’åº
                    is_dup = True
                    break
            
            if is_dup:
                continue
                
            seen_hashes.add(h)
            seen_contents.add(content)
            unique_results.append(res)
            
        # å®‰å…¨ä¿®æ­£: å¦‚æœå»é‡åç©ºäº† (æä¸å¯èƒ½ï¼Œå› ä¸ºç¬¬ä¸€æ¡è‚¯å®šè¿›)ï¼Œæˆ–è€…å»å¾—å¤ªç‹ 
        # è¿™é‡Œåªè¦ original results æœ‰ä¸œè¥¿ï¼Œunique_results è‡³å°‘ä¼šæœ‰ 1 æ¡
        if not unique_results and results:
             unique_results.append(results[0])
             
        return unique_results

    async def _query_entity(self, entity_name: str) -> Dict[str, Any]:
        """æŸ¥è¯¢å®ä½“ä¿¡æ¯

        Args:
            entity_name: å®ä½“åç§°

        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        if not entity_name:

            return {
                "success": False,
                "error": "å®ä½“åç§°ä¸èƒ½ä¸ºç©º",
                "content": "âš ï¸ å®ä½“åç§°ä¸èƒ½ä¸ºç©º",
                "results": [],
            }

        # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨
        if not self.graph_store.has_node(entity_name):

            return {
                "success": False,
                "error": f"å®ä½“ä¸å­˜åœ¨: {entity_name}",
                "content": f"âŒ å®ä½“ '{entity_name}' ä¸å­˜åœ¨",
                "results": [],
            }

        # è·å–é‚»å±…èŠ‚ç‚¹
        neighbors = self.graph_store.get_neighbors(entity_name)

        # è·å–ç›¸å…³æ®µè½
        paragraphs = self.metadata_store.get_paragraphs_by_entity(entity_name)

        # æ ¼å¼åŒ–æ®µè½
        formatted_paragraphs = [
            {
                "hash": para["hash"],
                "content": para["content"],
                "created_at": para.get("created_at"),
            }
            for para in paragraphs
        ]


        # ç”Ÿæˆ content æ‘˜è¦
        content_lines = [f"å®ä½“ '{entity_name}' ä¿¡æ¯ï¼š"]
        content_lines.append(f"- é‚»å±…èŠ‚ç‚¹ ({len(neighbors)}): {', '.join(neighbors[:10])}{'...' if len(neighbors)>10 else ''}")
        content_lines.append(f"- ç›¸å…³æ®µè½ ({len(paragraphs)}):")
        for i, para in enumerate(formatted_paragraphs[:3]):
             content_lines.append(f"  {i+1}. {para['content'][:50]}...")
        
        content = "\n".join(content_lines)

        return {
            "success": True,
            "query_type": "entity",
            "entity": entity_name,
            "neighbors": neighbors,
            "related_paragraphs": formatted_paragraphs,
            "neighbor_count": len(neighbors),
            "paragraph_count": len(paragraphs),
            "content": content,
        }

    async def _query_relation(self, relation_spec: str) -> Dict[str, Any]:
        """æŸ¥è¯¢å…³ç³»ä¿¡æ¯

        Args:
            relation_spec: å…³ç³»è§„æ ¼

        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        # è·å–é…ç½®
        enable_fallback = self.get_config("retrieval.relation_semantic_fallback", True)
        fallback_min_score = self.get_config("retrieval.relation_fallback_min_score", 0.3)
        
        # Path Search é…ç½®
        enable_path_search = self.get_config("retrieval.relation_enable_path_search", True)
        path_trigger_threshold = self.get_config("retrieval.relation_path_trigger_threshold", 0.4)

        # 1. ç»“æ„åŒ–æ£€æµ‹
        # å¦‚æœåŒ…å«æ˜ç¡®çš„åˆ†éš”ç¬¦ï¼Œè§†ä¸ºç»“æ„åŒ–æŸ¥è¯¢
        is_structured = "|" in relation_spec or "->" in relation_spec

        # 2. è‡ªç„¶è¯­è¨€ä¼˜å…ˆå¤„ç†
        # å¦‚æœä¸æ˜¯æ˜ç¡®çš„ç»“æ„åŒ–æŸ¥è¯¢ï¼Œä¸”å¯ç”¨äº†å›é€€ï¼ˆæ„å‘³ç€æ”¯æŒè¯­ä¹‰æ¨¡å¼ï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨è¯­ä¹‰æ£€ç´¢
        if not is_structured and enable_fallback:
            return await self._semantic_search_relation(relation_spec, fallback_min_score)

        # 3. ç»“æ„åŒ–æŸ¥è¯¢å¤„ç† (ç²¾ç¡®åŒ¹é…)
        subject, predicate, obj = None, None, None

        if "|" in relation_spec:
            parts = relation_spec.split("|")
            if len(parts) >= 2:
                subject = parts[0].strip()
                predicate = parts[1].strip()
                obj = parts[2].strip() if len(parts) > 2 else None
        elif "->" in relation_spec:
             parts = relation_spec.split("->")
             if len(parts) >= 2:
                subject = parts[0].strip()
                predicate = parts[1].strip() # ç®€åŒ–å¤„ç†ï¼Œå‡è®¾ -> å°±æ˜¯è°“è¯­çš„ä¸€éƒ¨åˆ†æˆ–è€…åˆ†éš”
                obj = parts[1].strip() # è¿™é‡Œ split åªæœ‰ä¸¤éƒ¨åˆ†ï¼Œä¸­é—´ä½œä¸ºè°“è¯­å¤„ç†æœ‰ç‚¹æ¨¡ç³Šï¼Œæš‚ä¸”ç»´æŒåŸé€»è¾‘æˆ–ä½œä¸º binary
                # å®é™…ä¸ŠåŸé€»è¾‘æ²¡å¤„ç† ->, è¿™é‡Œä»…åšç®€å•å…¼å®¹ï¼Œæˆ–è€…é€€å›åˆ° split()
                # è€ƒè™‘åˆ°å…¼å®¹æ€§ï¼Œè¿™é‡Œä»…ä»¥æ­¤ä½œä¸º"ç»“æ„åŒ–"æ ‡å¿—ï¼Œè§£æè¿˜æ˜¯å°è¯•ç©ºæ ¼
                pass

        if not subject: # å°è¯•ç©ºæ ¼è§£æ (Legacy)
            parts = relation_spec.split(maxsplit=1)
            if len(parts) >= 2:
                subject = parts[0].strip()
                predicate = parts[1].strip()
                obj = None
            else:
                 # æ— æ³•è§£æä¸ºç»“æ„åŒ–ï¼Œä¸”æ²¡èµ° NL è·¯å¾„ (è¯´æ˜ enable_fallback=False)
                 return {
                    "success": False,
                    "error": "å…³ç³»æ ¼å¼é”™è¯¯ (è¯·ä½¿ç”¨ S|P|O æˆ–å¼€å¯è¯­ä¹‰å›é€€)",
                    "content": "âŒ å…³ç³»æ ¼å¼é”™è¯¯: è¯·ä½¿ç”¨ 'Subject|Predicate|Object' æ ¼å¼",
                    "results": [],
                }

        # æ‰§è¡Œç²¾ç¡®æŸ¥è¯¢
        relations = self.metadata_store.get_relations(
            subject=subject if subject else None,
            predicate=predicate if predicate else None,
            object=obj if obj else None,
        )

        # 4. ç»“æ„åŒ–æŸ¥è¯¢å¤±è´¥çš„å›é€€
        # å¦‚æœç²¾ç¡®åŒ¹é…æ— ç»“æœï¼Œä¸”å¯ç”¨äº†å›é€€ï¼Œå°è¯•è¯­ä¹‰æ£€ç´¢
        if not relations and enable_fallback:
             # ä½¿ç”¨åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²è¿›è¡Œè¯­ä¹‰æ£€ç´¢
             semantic_result = await self._semantic_search_relation(relation_spec, fallback_min_score)
             
             # æ£€æŸ¥æ˜¯å¦è§¦å‘ Path Search
             # è§¦å‘æ¡ä»¶: å¯ç”¨ä¸” (æ— ç»“æœ æˆ– æœ€é«˜åˆ†ä½äºé˜ˆå€¼)
             hits_count = semantic_result.get("count", 0)
             max_score = 0.0
             if hits_count > 0 and semantic_result.get("results"):
                 max_score = semantic_result["results"][0].get("similarity", 0.0)
                 
             if enable_path_search and (hits_count == 0 or max_score < path_trigger_threshold):
                 if self.debug_enabled:
                     logger.info(f"{self.log_prefix} è§¦å‘è·¯å¾„æœç´¢ (Hits={hits_count}, MaxScore={max_score:.2f})")
                     
                 path_result = self._path_search(relation_spec)
                 if path_result:
                     return path_result
             
             return semantic_result

        # æ ¼å¼åŒ–ç²¾ç¡®åŒ¹é…ç»“æœ
        formatted_relations = []
        for rel in relations:
            formatted_relations.append({
                "hash": rel["hash"],
                "subject": rel["subject"],
                "predicate": rel["predicate"],
                "object": rel["object"],
                "confidence": rel.get("confidence", 1.0),
                "is_semantic": False,
            })

        # ç”Ÿæˆ content æ‘˜è¦
        if formatted_relations:
            lines = [f"æ‰¾åˆ° {len(formatted_relations)} æ¡ç²¾ç¡®åŒ¹é…å…³ç³»ï¼š"]
            for i, rel in enumerate(formatted_relations[:10]):
                lines.append(f"{i+1}. {rel['subject']} {rel['predicate']} {rel['object']}")
            content = "\n".join(lines)
        else:
            content = "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å…³ç³»ã€‚"

        return {
            "success": True,
            "query_type": "relation",
            "spec": {"subject": subject, "predicate": predicate, "object": obj},
            "results": formatted_relations,
            "count": len(formatted_relations),
            "content": content,
        }

    async def _semantic_search_relation(
        self,
        query: str,
        min_score: float,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œè¯­ä¹‰å…³ç³»æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            min_score: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if not self.retriever:
             return {
                "success": False,
                "error": "æ£€ç´¢å™¨æœªåˆå§‹åŒ–",
                "content": "âŒ æ£€ç´¢å™¨æœªåˆå§‹åŒ–",
                "results": [],
            }

        # æ‰§è¡Œæ£€ç´¢ (ç­–ç•¥: REL_ONLY, TopK: 5)
        # æŠ¤æ  B: TopK å°ä¸€ç‚¹
        results = await self.retriever.retrieve(
            query,
            top_k=5,
            strategy=RetrievalStrategy.REL_ONLY
        )

        formatted_results = []
        seen_relations = set()

        for res in results:
            # æŠ¤æ  B: é˜ˆå€¼è¿‡æ»¤
            if res.score < min_score:
                continue
            
            # æŠ¤æ  D: ç±»å‹è¿‡æ»¤ (retrieve REL_ONLY åº”è¯¥åªè¿”å› relationï¼Œä½†é˜²å¾¡æ€§æ£€æŸ¥)
            if res.result_type != "relation":
                continue

            # è·å–å…ƒæ•°æ®
            meta = res.metadata
            subj = meta.get("subject", "?")
            pred = meta.get("predicate", "?")
            obj = meta.get("object", "?")
            
            # æŠ¤æ  D: å»é‡
            rel_key = (subj, pred, obj)
            if rel_key in seen_relations:
                continue
            seen_relations.add(rel_key)

            formatted_results.append({
                "hash": res.hash_value,
                "subject": subj,
                "predicate": pred,
                "object": obj,
                "confidence": meta.get("confidence", 1.0),
                "similarity": res.score,
                "is_semantic": True, # æ ‡è®°ä¸ºè¯­ä¹‰ç»“æœ
            })

        # æŠ¤æ  C: æ˜ç¡®æ ‡æ³¨
        if formatted_results:
            lines = [f"æ‰¾åˆ° {len(formatted_results)} æ¡ [è¯­ä¹‰å€™é€‰] å…³ç³»ï¼š"]
            for i, rel in enumerate(formatted_results):
                lines.append(
                    f"{i+1}. {rel['subject']} {rel['predicate']} {rel['object']} "
                    f"(ç›¸ä¼¼åº¦: {rel['similarity']:.2f})"
                )
            
            lines.append("")
            lines.append("ğŸ’¡ è‹¥éœ€ç²¾ç¡®è¿‡æ»¤ï¼Œè¯·ä½¿ç”¨ 'Subject|Predicate|Object' æ ¼å¼")
            content = "\n".join(lines)
        else:
            content = (
                f"æœªæ‰¾åˆ°ç›¸å…³çš„å…³ç³» (è¯­ä¹‰ç›¸ä¼¼åº¦å‡ä½äº {min_score})ã€‚\n"
                "ğŸ’¡ è¯·å°è¯•æ›´å…·ä½“çš„å…³ç³»æè¿°ï¼Œæˆ–ä½¿ç”¨ 'S|P|O' æ ¼å¼è¿›è¡Œç²¾ç¡®æŸ¥è¯¢ã€‚"
            )

        return {
            "success": True,
            "query_type": "relation",
            "search_mode": "semantic",
            "query": query,
            "results": formatted_results,
            "count": len(formatted_results),
            "content": content,
        }

    def _path_search(self, query: str) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œè·¯å¾„æœç´¢ (å¤šè·³å…³ç³»)"""
        # 1. æå–å®ä½“
        entities = self._extract_entities_from_query(query)
        if len(entities) != 2:
            if self.debug_enabled:
                logger.debug(f"{self.log_prefix} PathSearch Abort: Requires exactly 2 entities, found {len(entities)}: {entities}")
            return None
            
        start_node, end_node = entities[0], entities[1]
        
        # 2. æŸ¥æ‰¾è·¯å¾„
        paths = self.graph_store.find_paths(
            start_node, 
            end_node, 
            max_depth=3, # Configurable?
            max_paths=5
        )
        
        if not paths:
            return None
            
        # 3. ä¸°å¯Œè·¯å¾„ä¿¡æ¯ (æŸ¥æ‰¾è¾¹ä¸Šçš„å…³ç³»è°“è¯­)
        formatted_paths = []
        edge_cache = {} # (u, v) -> relation_str
        
        for path_nodes in paths:
            path_desc = []
            valid_path = True
            
            for i in range(len(path_nodes) - 1):
                u, v = path_nodes[i], path_nodes[i+1]
                
                # Check cache
                cache_key = tuple(sorted((u, v))) # Undirected cache key
                if cache_key in edge_cache:
                    rel_info = edge_cache[cache_key]
                else:
                    # Query metadata for relation u->v or v->u
                    # ä¼˜å…ˆæ‰¾ u->v
                    rels = self.metadata_store.get_relations(subject=u, object=v)
                    direction = "->"
                    if not rels:
                        # å°è¯• v->u
                        rels = self.metadata_store.get_relations(subject=v, object=u)
                        direction = "<-"
                    
                    if rels:
                        # Pick best confidence or first
                        best_rel = max(rels, key=lambda x: x.get("confidence", 1.0))
                        pred = best_rel.get("predicate", "related")
                        rel_info = (pred, direction, u, v) if direction == "->" else (pred, direction, v, u)
                    else:
                        rel_info = ("?", "->", u, v) # Should not happen if graph consistent
                        
                    edge_cache[cache_key] = rel_info
                
                pred, direction, src, tgt = rel_info
                if direction == "->":
                    step_str = f"-[{pred}]->"
                else:
                    step_str = f"<-[{pred}]-"
                path_desc.append(step_str)
            
            # Reconstruct full string: Node1 -[pred]-> Node2 ...
            full_path_str = path_nodes[0]
            for i, step in enumerate(path_desc):
                full_path_str += f" {step} {path_nodes[i+1]}"
            
            formatted_paths.append({
                "nodes": path_nodes,
                "description": full_path_str
            })

        # Generate content
        lines = [f"Found {len(formatted_paths)} indirect connection paths between '{start_node}' and '{end_node}':"]
        for i, p in enumerate(formatted_paths):
            lines.append(f"{i+1}. {p['description']}")
            
        content = "\n".join(lines)
        
        return {
            "success": True,
            "query_type": "relation",
            "search_mode": "path",
            "query": query,
            "results": formatted_paths,
            "count": len(formatted_paths),
            "content": content
        }

    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å·²çŸ¥çš„å›¾èŠ‚ç‚¹å®ä½“ (ç®€æ˜“å¯å‘å¼)"""
        if not self.graph_store:
            return []
            
        # 1. ç®€å•çš„ N-gram åŒ¹é… (N=1..4)
        tokens = query.replace("?", " ").replace("!", " ").replace(".", " ").split()
        found_entities = set()
        
        # ä¼˜åŒ–: ä»…æ£€æŸ¥ query ä¸­çš„ potential matches
        # ç”±äºæ— æ³•éå†æ‰€æœ‰ nodeï¼Œæˆ‘ä»¬ç”Ÿæˆ query çš„æ‰€æœ‰å­ä¸² check existence
        # O(L^2) where L is query length (small)
        
        n = len(tokens)
        # Max n-gram size: 4 or length of query
        max_n = min(4, n)
        
        # Greedy search: prioritize longer matches
        skip_indices = set()
        
        for size in range(max_n, 0, -1):
            for i in range(n - size + 1):
                # Check if this span is already covered
                if any(idx in skip_indices for idx in range(i, i+size)):
                    continue
                    
                span = " ".join(tokens[i : i+size])
                # Check original case first, then exact match only (kv store usually case sensitive-ish)
                # But user query might be lower/upper.
                # Use ignore_case=True to support "system" matches "System"
                matched_node = self.graph_store.find_node(span, ignore_case=True)
                if matched_node:
                    found_entities.add(matched_node)
                    # Mark indices as covered
                    for idx in range(i, i+size):
                        skip_indices.add(idx)
                else:
                    pass
                    
        return list(found_entities)

    def _get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "vector_store": {
                "num_vectors": self.vector_store.num_vectors if self.vector_store else 0,
                "dimension": self.vector_store.dimension if self.vector_store else 0,
            },
            "graph_store": {
                "num_nodes": self.graph_store.num_nodes if self.graph_store else 0,
                "num_edges": self.graph_store.num_edges if self.graph_store else 0,
            },
            "metadata_store": {
                "num_paragraphs": self.metadata_store.count_paragraphs() if self.metadata_store else 0,
                "num_relations": self.metadata_store.count_relations() if self.metadata_store else 0,
                "num_entities": self.metadata_store.count_entities() if self.metadata_store else 0,
            },
            "sparse": self.sparse_index.stats() if self.sparse_index else None,
        }

        # Format a human-readable summary
        content = (
            f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\n\n"
            f"ğŸ“¦ å‘é‡å­˜å‚¨:\n"
            f"  - å‘é‡æ•°é‡: {stats['vector_store']['num_vectors']}\n"
            f"  - ç»´åº¦: {stats['vector_store']['dimension']}\n\n"
            f"ğŸ•¸ï¸ å›¾å­˜å‚¨:\n"
            f"  - èŠ‚ç‚¹æ•°: {stats['graph_store']['num_nodes']}\n"
            f"  - è¾¹æ•°: {stats['graph_store']['num_edges']}\n\n"
            f"ğŸ“ å…ƒæ•°æ®å­˜å‚¨:\n"
            f"  - æ®µè½æ•°: {stats['metadata_store']['num_paragraphs']}\n"
            f"  - å…³ç³»æ•°: {stats['metadata_store']['num_relations']}\n"
            f"  - å®ä½“æ•°: {stats['metadata_store']['num_entities']}"
        )
        sparse_stats = stats.get("sparse")
        if sparse_stats:
            content += (
                f"\n\nğŸ§© ç¨€ç–æ£€ç´¢:\n"
                f"  - å¯ç”¨: {'æ˜¯' if sparse_stats.get('enabled') else 'å¦'}\n"
                f"  - å·²åŠ è½½: {'æ˜¯' if sparse_stats.get('loaded') else 'å¦'}\n"
                f"  - Tokenizer: {sparse_stats.get('tokenizer_mode', 'N/A')}\n"
                f"  - FTSæ–‡æ¡£æ•°: {sparse_stats.get('doc_count', 0)}"
            )

        return {
            "success": True,
            "query_type": "stats",
            "content": content,
            "statistics": stats,
        }

    def get_tool_info_summary(self) -> str:
        """è·å–å·¥å…·ä¿¡æ¯æ‘˜è¦

        Returns:
            å·¥å…·ä¿¡æ¯æ‘˜è¦æ–‡æœ¬
        """
        if not self.retriever:
            return "âŒ çŸ¥è¯†æŸ¥è¯¢Toolæœªåˆå§‹åŒ–"

        lines = [
            "ğŸ”§ çŸ¥è¯†æŸ¥è¯¢Toolä¿¡æ¯",
            "",
            "ğŸ“‹ åŸºæœ¬ä¿¡æ¯:",
            f"  - åç§°: {self.name}",
            f"  - æè¿°: {self.description}",
            f"  - LLMå¯ç”¨: {'æ˜¯' if self.available_for_llm else 'å¦'}",
            "",
            "âš™ï¸ æ£€ç´¢é…ç½®:",
            f"  - Top-Kæ®µè½: {self.retriever.config.top_k_paragraphs}",
            f"  - Top-Kå…³ç³»: {self.retriever.config.top_k_relations}",
            f"  - èåˆç³»æ•°(alpha): {self.retriever.config.alpha}",
            f"  - èåˆæ–¹æ³•: {self.retriever.config.fusion.method}",
            f"  - PPRå¯ç”¨: {'æ˜¯' if self.retriever.config.enable_ppr else 'å¦'}",
            f"  - å¹¶è¡Œæ£€ç´¢: {'æ˜¯' if self.retriever.config.enable_parallel else 'å¦'}",
            "",
            "ğŸ“Š å­˜å‚¨ç»Ÿè®¡:",
            f"  - å‘é‡æ•°é‡: {self.vector_store.num_vectors if self.vector_store else 0}",
            f"  - èŠ‚ç‚¹æ•°é‡: {self.graph_store.num_nodes if self.graph_store else 0}",
            f"  - è¾¹æ•°é‡: {self.graph_store.num_edges if self.graph_store else 0}",
            f"  - æ®µè½æ•°é‡: {self.metadata_store.count_paragraphs() if self.metadata_store else 0}",
        ]
        if self.sparse_index:
            sparse_stats = self.sparse_index.stats()
            lines.extend([
                "",
                "ğŸ§© ç¨€ç–æ£€ç´¢:",
                f"  - å¯ç”¨: {'æ˜¯' if sparse_stats.get('enabled') else 'å¦'}",
                f"  - å·²åŠ è½½: {'æ˜¯' if sparse_stats.get('loaded') else 'å¦'}",
                f"  - Tokenizer: {sparse_stats.get('tokenizer_mode', 'N/A')}",
            ])

        return "\n".join(lines)
